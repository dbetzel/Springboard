{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GOTtextGeneration.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoHzNDs9ovhe"
      },
      "source": [
        "**Dawn Betzel**\n",
        "\n",
        "**Capstone 1**\n",
        "\n",
        "**Text Generation**\n",
        "\n",
        "For my first Capstone, my goal was to generate text.  I want to train a model using text based stories and once the model is trained generate text similar to that style.\n",
        "\n",
        "In researching and learning for this Capstone I took several different attempts.  I first started with an old program I had worked on in another class that was actually my inspiration for this idea.  This first program used a neural network and LSTMs.  I found that many of the libraries that I had used in this program were deprecated, and I did not want to work in old libraries that no longer were being updated.  Then I tried several tutorials, finally finding something on transformers using HuggingFace.  In my desire to pursue more knowledge of HuggingFace, I purchased a book titled *Transformers for Natural Language Processing* by Denis Rothman.  I took the material in the book and tweaked it for my own purposes for this Capstone.\n",
        "\n",
        "\n",
        "**Code References**\n",
        "\n",
        "Reference: OpenAI Repository The repository was cloned and adapted to N Shepperd's repository.\n",
        "\n",
        "Reference: N Shepperd Repository The repository was not cloned. N Shepperd's training programs were inserted into the OpenAI Repository. The list of N Shepperd's programs are cited in the 'N Shepperd' section of the notebook. Some programs were modified for educational purposes only to work with this notebook.\n",
        "\n",
        "**Model Reference Paper**\n",
        "\n",
        "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,2019,'Language Models are Unsupervised Multitask Learners'\n",
        "\n",
        "**Step 1: Pre-requisites:**\n",
        "\n",
        "a) activate GPU in the notebook settings runTime menu\n",
        "b) Upload the following program files and dset.txt(dataset) with the file manager: train.py,load_dataset.py,encode.py,accumulate,memory_saving_gradients.py,dset.txt\n",
        "\n",
        "dset.txt contains the story or stories you are using to train your model.  I have used five Game Of Thrones books that I found on kaggle, https://www.kaggle.com/khulasasndh/game-of-thrones-books.  I combined the five books from this kaggle dataset into one text file and named it dset.txt to use to train my model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n-woQDwtUYk"
      },
      "source": [
        "**Step 1  Activate the GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2uuYurJrW8k",
        "outputId": "84655359-7914-495c-9804-0a32defa2487"
      },
      "source": [
        "#Cloning the OpenAI GPT-2 Repository \n",
        "!git clone https://github.com/openai/gpt-2.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 233, done.\u001b[K\n",
            "remote: Total 233 (delta 0), reused 0 (delta 0), pack-reused 233\u001b[K\n",
            "Receiving objects: 100% (233/233), 4.38 MiB | 19.93 MiB/s, done.\n",
            "Resolving deltas: 100% (124/124), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpsh1NTxsPZp",
        "outputId": "95f716fb-5135-4b0a-de5d-38bfe0d0069f"
      },
      "source": [
        "#Installing the requirements\n",
        "#when the VM restarts import os necessary\n",
        "import os                     \n",
        "os.chdir(\"/content/gpt-2\")    \n",
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fire>=0.1.3\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting regex==2017.4.5\n",
            "  Downloading regex-2017.04.05.tar.gz (601 kB)\n",
            "\u001b[K     |████████████████████████████████| 601 kB 31.5 MB/s \n",
            "\u001b[?25hCollecting requests==2.21.0\n",
            "  Downloading requests-2.21.0-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 6.6 MB/s \n",
            "\u001b[?25hCollecting tqdm==4.31.1\n",
            "  Downloading tqdm-4.31.1-py2.py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 6.0 MB/s \n",
            "\u001b[?25hCollecting idna<2.9,>=2.5\n",
            "  Downloading idna-2.8-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2021.5.30)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
            "Building wheels for collected packages: regex, fire\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.4.5-cp37-cp37m-linux_x86_64.whl size=534454 sha256=5cbeace4fe69e564f609080c452eab7dac909190fa6c29d8c4c78b7490a63a8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/e8/a5/d4894e7ef29935f75c6074409ce8ca80a0271f0ce2a30da5d3\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115943 sha256=04de0f9a2738c987ebec69960ebf9dbf14e4da0336ef794d2d0b272eb6484841\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "Successfully built regex fire\n",
            "Installing collected packages: idna, tqdm, requests, regex, fire\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 2.2.4 requires tqdm<5.0.0,>=4.38.0, but you have tqdm 4.31.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.21.0 which is incompatible.\n",
            "fbprophet 0.7.1 requires tqdm>=4.36.1, but you have tqdm 4.31.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed fire-0.4.0 idna-2.8 regex-2017.4.5 requests-2.21.0 tqdm-4.31.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhPjx8sbsmgw",
        "outputId": "ad388f5d-cf9d-49e8-fabd-f8bab86e98e9"
      },
      "source": [
        "#Implements a topological sort algorithm\n",
        "!pip install toposort"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting toposort\n",
            "  Downloading toposort-1.6-py2.py3-none-any.whl (9.5 kB)\n",
            "Installing collected packages: toposort\n",
            "Successfully installed toposort-1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvFHAJTNviBe",
        "outputId": "437efcd8-4a5b-4bc7-a7e3-655769a1443f"
      },
      "source": [
        "#Checking TensorFlow version \n",
        "#Colab has tf 1.x and tf 2.x installed\n",
        "#Restart runtime using 'Runtime' -> 'Restart runtime...'\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSkV0R0pvzgn",
        "outputId": "f812a780-88c1-4e9d-a6dd-36cf8cf6ea87"
      },
      "source": [
        "#Downloading 117M parameter GPT-2 Model\n",
        "#run code and send argument\n",
        "#import os # after runtime is restarted\n",
        "os.chdir(\"/content/gpt-2\")\n",
        "#creates model directory\n",
        "!python3 download_model.py '117M' "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.00kit [00:00, 960kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:00, 3.16Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 970kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:25, 19.5Mit/s]                                  \n",
            "Fetching model.ckpt.index: 6.00kit [00:00, 5.28Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 472kit [00:00, 1.72Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 1.79Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7vMFTGDwizG"
      },
      "source": [
        "#Copying the Project Resources to scr\n",
        "!cp /content/dset.txt /content/gpt-2/src/\n",
        "!cp -r /content/gpt-2/models/ /content/gpt-2/src/"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlsTyvBY1AWm"
      },
      "source": [
        "#Copying the N Shepperd Training Files\n",
        "#Referfence GitHub repository: https://github.com/nshepperd/gpt-2\n",
        "#import os # import after runtime is restarted\n",
        "!cp /content/train.py /content/gpt-2/src/\n",
        "!cp /content/load_dataset.py /content/gpt-2/src/\n",
        "!cp /content/encode.py /content/gpt-2/src/\n",
        "!cp /content/accumulate.py /content/gpt-2/src/\n",
        "!cp /content/memory_saving_gradients.py /content/gpt-2/src/"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPqRx5401Z8d",
        "outputId": "87b146bc-29e9-46c0-8c67-ab86ad0e4ff6"
      },
      "source": [
        "#Encoding dataset\n",
        "#import os # import after runtime is restarted\n",
        "os.chdir(\"/content/gpt-2/src/\")\n",
        "model_name=\"117M\"\n",
        "!python /content/gpt-2/src/encode.py dset.txt out.npz "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading files\n",
            "100% 1/1 [00:12<00:00, 12.06s/it]\n",
            "Writing out.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nro2VtTM2ON7",
        "outputId": "2d6ddc40-6561-4c81-eb2e-e0414ed61dea"
      },
      "source": [
        "#Training the Model\n",
        "#Model saved after 1000 steps\n",
        "#import os # import after runtime is restarted\n",
        "os.chdir(\"/content/gpt-2/src/\")\n",
        "!python train.py --dataset out.npz"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "[5639 | 2811.90] loss=2.63 avg=2.72\n",
            "[5640 | 2812.38] loss=2.48 avg=2.72\n",
            "[5641 | 2812.87] loss=2.64 avg=2.72\n",
            "[5642 | 2813.36] loss=3.03 avg=2.72\n",
            "[5643 | 2813.84] loss=2.43 avg=2.72\n",
            "[5644 | 2814.33] loss=2.62 avg=2.72\n",
            "[5645 | 2814.82] loss=2.61 avg=2.72\n",
            "[5646 | 2815.31] loss=2.67 avg=2.72\n",
            "[5647 | 2815.79] loss=2.69 avg=2.72\n",
            "[5648 | 2816.28] loss=2.40 avg=2.71\n",
            "[5649 | 2816.77] loss=2.54 avg=2.71\n",
            "[5650 | 2817.26] loss=2.92 avg=2.71\n",
            "[5651 | 2817.74] loss=2.64 avg=2.71\n",
            "[5652 | 2818.23] loss=2.61 avg=2.71\n",
            "[5653 | 2818.72] loss=2.88 avg=2.71\n",
            "[5654 | 2819.20] loss=2.80 avg=2.71\n",
            "[5655 | 2819.69] loss=2.58 avg=2.71\n",
            "[5656 | 2820.18] loss=2.80 avg=2.71\n",
            "[5657 | 2820.67] loss=2.54 avg=2.71\n",
            "[5658 | 2821.15] loss=2.87 avg=2.71\n",
            "[5659 | 2821.64] loss=2.70 avg=2.71\n",
            "[5660 | 2822.13] loss=2.88 avg=2.72\n",
            "[5661 | 2822.61] loss=2.83 avg=2.72\n",
            "[5662 | 2823.10] loss=2.50 avg=2.71\n",
            "[5663 | 2823.58] loss=2.76 avg=2.71\n",
            "[5664 | 2824.07] loss=2.51 avg=2.71\n",
            "[5665 | 2824.56] loss=2.66 avg=2.71\n",
            "[5666 | 2825.05] loss=2.81 avg=2.71\n",
            "[5667 | 2825.53] loss=2.83 avg=2.71\n",
            "[5668 | 2826.02] loss=2.91 avg=2.72\n",
            "[5669 | 2826.51] loss=2.88 avg=2.72\n",
            "[5670 | 2827.00] loss=2.73 avg=2.72\n",
            "[5671 | 2827.48] loss=2.68 avg=2.72\n",
            "[5672 | 2827.97] loss=2.45 avg=2.71\n",
            "[5673 | 2828.46] loss=3.12 avg=2.72\n",
            "[5674 | 2828.94] loss=3.12 avg=2.72\n",
            "[5675 | 2829.43] loss=2.72 avg=2.72\n",
            "[5676 | 2829.92] loss=2.69 avg=2.72\n",
            "[5677 | 2830.41] loss=2.20 avg=2.72\n",
            "[5678 | 2830.89] loss=2.42 avg=2.71\n",
            "[5679 | 2831.38] loss=2.47 avg=2.71\n",
            "[5680 | 2831.87] loss=2.75 avg=2.71\n",
            "[5681 | 2832.36] loss=2.81 avg=2.71\n",
            "[5682 | 2832.84] loss=2.46 avg=2.71\n",
            "[5683 | 2833.33] loss=2.79 avg=2.71\n",
            "[5684 | 2833.82] loss=2.61 avg=2.71\n",
            "[5685 | 2834.31] loss=2.82 avg=2.71\n",
            "[5686 | 2834.79] loss=2.41 avg=2.71\n",
            "[5687 | 2835.28] loss=2.98 avg=2.71\n",
            "[5688 | 2835.77] loss=1.76 avg=2.70\n",
            "[5689 | 2836.25] loss=2.57 avg=2.70\n",
            "[5690 | 2836.74] loss=2.62 avg=2.70\n",
            "[5691 | 2837.22] loss=2.89 avg=2.70\n",
            "[5692 | 2837.71] loss=2.94 avg=2.70\n",
            "[5693 | 2838.19] loss=2.79 avg=2.70\n",
            "[5694 | 2838.67] loss=2.42 avg=2.70\n",
            "[5695 | 2839.16] loss=2.87 avg=2.70\n",
            "[5696 | 2839.65] loss=2.78 avg=2.70\n",
            "[5697 | 2840.13] loss=2.54 avg=2.70\n",
            "[5698 | 2840.62] loss=2.52 avg=2.70\n",
            "[5699 | 2841.11] loss=2.99 avg=2.70\n",
            "[5700 | 2841.60] loss=2.92 avg=2.71\n",
            "[5701 | 2842.08] loss=3.15 avg=2.71\n",
            "[5702 | 2842.57] loss=3.03 avg=2.71\n",
            "[5703 | 2843.06] loss=3.05 avg=2.72\n",
            "[5704 | 2843.55] loss=1.92 avg=2.71\n",
            "[5705 | 2844.03] loss=2.45 avg=2.71\n",
            "[5706 | 2844.52] loss=2.89 avg=2.71\n",
            "[5707 | 2845.01] loss=2.72 avg=2.71\n",
            "[5708 | 2845.49] loss=2.59 avg=2.71\n",
            "[5709 | 2845.98] loss=2.55 avg=2.71\n",
            "[5710 | 2846.47] loss=2.70 avg=2.71\n",
            "[5711 | 2846.95] loss=2.58 avg=2.70\n",
            "[5712 | 2847.44] loss=2.62 avg=2.70\n",
            "[5713 | 2847.93] loss=2.84 avg=2.70\n",
            "[5714 | 2848.42] loss=2.71 avg=2.70\n",
            "[5715 | 2848.91] loss=3.12 avg=2.71\n",
            "[5716 | 2849.39] loss=2.75 avg=2.71\n",
            "[5717 | 2849.88] loss=2.73 avg=2.71\n",
            "[5718 | 2850.37] loss=2.65 avg=2.71\n",
            "[5719 | 2850.86] loss=2.44 avg=2.71\n",
            "[5720 | 2851.34] loss=2.88 avg=2.71\n",
            "[5721 | 2851.84] loss=2.79 avg=2.71\n",
            "[5722 | 2852.32] loss=2.74 avg=2.71\n",
            "[5723 | 2852.81] loss=2.73 avg=2.71\n",
            "[5724 | 2853.29] loss=2.67 avg=2.71\n",
            "[5725 | 2853.78] loss=2.95 avg=2.71\n",
            "[5726 | 2854.27] loss=2.96 avg=2.71\n",
            "[5727 | 2854.76] loss=2.48 avg=2.71\n",
            "[5728 | 2855.25] loss=3.03 avg=2.71\n",
            "[5729 | 2855.73] loss=2.82 avg=2.72\n",
            "[5730 | 2856.22] loss=2.51 avg=2.71\n",
            "[5731 | 2856.71] loss=3.03 avg=2.72\n",
            "[5732 | 2857.20] loss=2.65 avg=2.72\n",
            "[5733 | 2857.68] loss=2.52 avg=2.71\n",
            "[5734 | 2858.17] loss=2.95 avg=2.72\n",
            "[5735 | 2858.66] loss=2.66 avg=2.72\n",
            "[5736 | 2859.15] loss=2.55 avg=2.71\n",
            "[5737 | 2859.64] loss=2.51 avg=2.71\n",
            "[5738 | 2860.12] loss=2.99 avg=2.71\n",
            "[5739 | 2860.61] loss=2.94 avg=2.72\n",
            "[5740 | 2861.10] loss=2.56 avg=2.72\n",
            "[5741 | 2861.59] loss=2.94 avg=2.72\n",
            "[5742 | 2862.08] loss=2.70 avg=2.72\n",
            "[5743 | 2862.56] loss=2.73 avg=2.72\n",
            "[5744 | 2863.05] loss=2.94 avg=2.72\n",
            "[5745 | 2863.54] loss=2.86 avg=2.72\n",
            "[5746 | 2864.03] loss=2.36 avg=2.72\n",
            "[5747 | 2864.51] loss=2.77 avg=2.72\n",
            "[5748 | 2865.00] loss=2.86 avg=2.72\n",
            "[5749 | 2865.49] loss=2.61 avg=2.72\n",
            "[5750 | 2865.98] loss=2.37 avg=2.72\n",
            "[5751 | 2866.47] loss=2.62 avg=2.71\n",
            "[5752 | 2866.96] loss=2.60 avg=2.71\n",
            "[5753 | 2867.44] loss=2.79 avg=2.71\n",
            "[5754 | 2867.93] loss=2.74 avg=2.71\n",
            "[5755 | 2868.42] loss=2.91 avg=2.72\n",
            "[5756 | 2868.90] loss=2.82 avg=2.72\n",
            "[5757 | 2869.39] loss=2.65 avg=2.72\n",
            "[5758 | 2869.88] loss=2.53 avg=2.71\n",
            "[5759 | 2870.37] loss=2.63 avg=2.71\n",
            "[5760 | 2870.85] loss=2.86 avg=2.72\n",
            "[5761 | 2871.34] loss=2.53 avg=2.71\n",
            "[5762 | 2871.83] loss=3.01 avg=2.72\n",
            "[5763 | 2872.32] loss=2.27 avg=2.71\n",
            "[5764 | 2872.80] loss=2.71 avg=2.71\n",
            "[5765 | 2873.29] loss=2.67 avg=2.71\n",
            "[5766 | 2873.78] loss=2.54 avg=2.71\n",
            "[5767 | 2874.27] loss=2.37 avg=2.71\n",
            "[5768 | 2874.75] loss=2.45 avg=2.70\n",
            "[5769 | 2875.24] loss=2.62 avg=2.70\n",
            "[5770 | 2875.73] loss=2.78 avg=2.70\n",
            "[5771 | 2876.22] loss=2.71 avg=2.70\n",
            "[5772 | 2876.70] loss=2.82 avg=2.70\n",
            "[5773 | 2877.19] loss=2.40 avg=2.70\n",
            "[5774 | 2877.68] loss=2.47 avg=2.70\n",
            "[5775 | 2878.16] loss=2.42 avg=2.70\n",
            "[5776 | 2878.65] loss=2.56 avg=2.70\n",
            "[5777 | 2879.14] loss=2.69 avg=2.70\n",
            "[5778 | 2879.62] loss=2.82 avg=2.70\n",
            "[5779 | 2880.11] loss=2.58 avg=2.70\n",
            "[5780 | 2880.60] loss=2.84 avg=2.70\n",
            "[5781 | 2881.09] loss=2.77 avg=2.70\n",
            "[5782 | 2881.57] loss=2.79 avg=2.70\n",
            "[5783 | 2882.06] loss=2.87 avg=2.70\n",
            "[5784 | 2882.55] loss=2.89 avg=2.70\n",
            "[5785 | 2883.04] loss=2.60 avg=2.70\n",
            "[5786 | 2883.53] loss=2.61 avg=2.70\n",
            "[5787 | 2884.01] loss=2.73 avg=2.70\n",
            "[5788 | 2884.50] loss=2.93 avg=2.70\n",
            "[5789 | 2884.99] loss=2.38 avg=2.70\n",
            "[5790 | 2885.47] loss=3.02 avg=2.70\n",
            "[5791 | 2885.96] loss=2.70 avg=2.70\n",
            "[5792 | 2886.44] loss=2.61 avg=2.70\n",
            "[5793 | 2886.93] loss=2.42 avg=2.70\n",
            "[5794 | 2887.42] loss=2.46 avg=2.70\n",
            "[5795 | 2887.91] loss=3.02 avg=2.70\n",
            "[5796 | 2888.40] loss=2.69 avg=2.70\n",
            "[5797 | 2888.89] loss=2.82 avg=2.70\n",
            "[5798 | 2889.37] loss=2.81 avg=2.70\n",
            "[5799 | 2889.86] loss=2.75 avg=2.70\n",
            "[5800 | 2890.35] loss=2.55 avg=2.70\n",
            "[5801 | 2890.83] loss=2.39 avg=2.70\n",
            "[5802 | 2891.32] loss=2.70 avg=2.70\n",
            "[5803 | 2891.81] loss=2.63 avg=2.70\n",
            "[5804 | 2892.30] loss=2.91 avg=2.70\n",
            "[5805 | 2892.78] loss=2.83 avg=2.70\n",
            "[5806 | 2893.27] loss=3.22 avg=2.71\n",
            "[5807 | 2893.76] loss=2.72 avg=2.71\n",
            "[5808 | 2894.25] loss=1.74 avg=2.70\n",
            "[5809 | 2894.73] loss=2.61 avg=2.70\n",
            "[5810 | 2895.22] loss=2.47 avg=2.69\n",
            "[5811 | 2895.71] loss=2.32 avg=2.69\n",
            "[5812 | 2896.20] loss=2.83 avg=2.69\n",
            "[5813 | 2896.68] loss=2.67 avg=2.69\n",
            "[5814 | 2897.17] loss=2.13 avg=2.69\n",
            "[5815 | 2897.65] loss=2.74 avg=2.69\n",
            "[5816 | 2898.14] loss=2.89 avg=2.69\n",
            "[5817 | 2898.63] loss=2.83 avg=2.69\n",
            "[5818 | 2899.12] loss=3.11 avg=2.69\n",
            "[5819 | 2899.60] loss=2.75 avg=2.69\n",
            "[5820 | 2900.09] loss=2.38 avg=2.69\n",
            "[5821 | 2900.58] loss=2.34 avg=2.69\n",
            "[5822 | 2901.06] loss=1.94 avg=2.68\n",
            "[5823 | 2901.55] loss=2.65 avg=2.68\n",
            "[5824 | 2902.03] loss=2.65 avg=2.68\n",
            "[5825 | 2902.53] loss=1.81 avg=2.67\n",
            "[5826 | 2903.01] loss=2.62 avg=2.67\n",
            "[5827 | 2903.50] loss=2.59 avg=2.67\n",
            "[5828 | 2903.98] loss=2.44 avg=2.67\n",
            "[5829 | 2904.48] loss=2.42 avg=2.66\n",
            "[5830 | 2904.96] loss=2.27 avg=2.66\n",
            "[5831 | 2905.45] loss=2.99 avg=2.66\n",
            "[5832 | 2905.94] loss=2.53 avg=2.66\n",
            "[5833 | 2906.42] loss=2.64 avg=2.66\n",
            "[5834 | 2906.91] loss=2.58 avg=2.66\n",
            "[5835 | 2907.40] loss=2.96 avg=2.66\n",
            "[5836 | 2907.88] loss=2.80 avg=2.67\n",
            "[5837 | 2908.37] loss=3.00 avg=2.67\n",
            "[5838 | 2908.86] loss=2.48 avg=2.67\n",
            "[5839 | 2909.34] loss=2.66 avg=2.67\n",
            "[5840 | 2909.83] loss=3.11 avg=2.67\n",
            "[5841 | 2910.32] loss=2.64 avg=2.67\n",
            "[5842 | 2910.81] loss=3.00 avg=2.67\n",
            "[5843 | 2911.29] loss=2.67 avg=2.67\n",
            "[5844 | 2911.78] loss=2.42 avg=2.67\n",
            "[5845 | 2912.27] loss=2.87 avg=2.67\n",
            "[5846 | 2912.76] loss=2.70 avg=2.67\n",
            "[5847 | 2913.24] loss=2.78 avg=2.68\n",
            "[5848 | 2913.73] loss=3.00 avg=2.68\n",
            "[5849 | 2914.22] loss=2.90 avg=2.68\n",
            "[5850 | 2914.70] loss=2.44 avg=2.68\n",
            "[5851 | 2915.19] loss=2.95 avg=2.68\n",
            "[5852 | 2915.68] loss=2.71 avg=2.68\n",
            "[5853 | 2916.17] loss=2.41 avg=2.68\n",
            "[5854 | 2916.65] loss=2.77 avg=2.68\n",
            "[5855 | 2917.14] loss=2.28 avg=2.68\n",
            "[5856 | 2917.63] loss=2.69 avg=2.68\n",
            "[5857 | 2918.12] loss=2.74 avg=2.68\n",
            "[5858 | 2918.61] loss=2.69 avg=2.68\n",
            "[5859 | 2919.09] loss=2.94 avg=2.68\n",
            "[5860 | 2919.58] loss=2.27 avg=2.68\n",
            "[5861 | 2920.07] loss=2.60 avg=2.67\n",
            "[5862 | 2920.56] loss=1.70 avg=2.66\n",
            "[5863 | 2921.05] loss=2.86 avg=2.67\n",
            "[5864 | 2921.53] loss=2.60 avg=2.67\n",
            "[5865 | 2922.02] loss=2.57 avg=2.67\n",
            "[5866 | 2922.51] loss=2.75 avg=2.67\n",
            "[5867 | 2923.00] loss=2.84 avg=2.67\n",
            "[5868 | 2923.49] loss=2.44 avg=2.67\n",
            "[5869 | 2923.98] loss=2.67 avg=2.67\n",
            "[5870 | 2924.46] loss=2.43 avg=2.66\n",
            "[5871 | 2924.95] loss=2.43 avg=2.66\n",
            "[5872 | 2925.44] loss=2.91 avg=2.66\n",
            "[5873 | 2925.93] loss=2.54 avg=2.66\n",
            "[5874 | 2926.42] loss=2.69 avg=2.66\n",
            "[5875 | 2926.91] loss=2.67 avg=2.66\n",
            "[5876 | 2927.40] loss=2.18 avg=2.66\n",
            "[5877 | 2927.89] loss=3.19 avg=2.66\n",
            "[5878 | 2928.38] loss=2.62 avg=2.66\n",
            "[5879 | 2928.86] loss=2.59 avg=2.66\n",
            "[5880 | 2929.35] loss=2.78 avg=2.66\n",
            "[5881 | 2929.84] loss=2.93 avg=2.67\n",
            "[5882 | 2930.33] loss=2.65 avg=2.67\n",
            "[5883 | 2930.82] loss=2.71 avg=2.67\n",
            "[5884 | 2931.30] loss=2.67 avg=2.67\n",
            "[5885 | 2931.79] loss=2.91 avg=2.67\n",
            "[5886 | 2932.28] loss=2.78 avg=2.67\n",
            "[5887 | 2932.77] loss=2.63 avg=2.67\n",
            "[5888 | 2933.26] loss=2.95 avg=2.67\n",
            "[5889 | 2933.74] loss=2.95 avg=2.67\n",
            "[5890 | 2934.23] loss=3.03 avg=2.68\n",
            "[5891 | 2934.72] loss=2.63 avg=2.68\n",
            "[5892 | 2935.21] loss=2.43 avg=2.68\n",
            "[5893 | 2935.69] loss=2.68 avg=2.68\n",
            "[5894 | 2936.18] loss=3.01 avg=2.68\n",
            "[5895 | 2936.67] loss=2.84 avg=2.68\n",
            "[5896 | 2937.15] loss=2.92 avg=2.68\n",
            "[5897 | 2937.64] loss=2.58 avg=2.68\n",
            "[5898 | 2938.13] loss=2.79 avg=2.68\n",
            "[5899 | 2938.61] loss=2.80 avg=2.68\n",
            "[5900 | 2939.10] loss=2.63 avg=2.68\n",
            "[5901 | 2939.59] loss=2.89 avg=2.69\n",
            "[5902 | 2940.08] loss=2.89 avg=2.69\n",
            "[5903 | 2940.57] loss=2.87 avg=2.69\n",
            "[5904 | 2941.05] loss=2.80 avg=2.69\n",
            "[5905 | 2941.54] loss=1.72 avg=2.68\n",
            "[5906 | 2942.02] loss=2.69 avg=2.68\n",
            "[5907 | 2942.51] loss=2.78 avg=2.68\n",
            "[5908 | 2943.00] loss=2.63 avg=2.68\n",
            "[5909 | 2943.49] loss=3.03 avg=2.68\n",
            "[5910 | 2943.97] loss=3.10 avg=2.69\n",
            "[5911 | 2944.46] loss=2.69 avg=2.69\n",
            "[5912 | 2944.95] loss=2.38 avg=2.69\n",
            "[5913 | 2945.43] loss=2.89 avg=2.69\n",
            "[5914 | 2945.92] loss=2.58 avg=2.69\n",
            "[5915 | 2946.41] loss=2.99 avg=2.69\n",
            "[5916 | 2946.90] loss=2.38 avg=2.69\n",
            "[5917 | 2947.38] loss=2.71 avg=2.69\n",
            "[5918 | 2947.87] loss=2.99 avg=2.69\n",
            "[5919 | 2948.36] loss=2.56 avg=2.69\n",
            "[5920 | 2948.85] loss=3.01 avg=2.69\n",
            "[5921 | 2949.33] loss=2.88 avg=2.69\n",
            "[5922 | 2949.82] loss=2.68 avg=2.69\n",
            "[5923 | 2950.30] loss=2.88 avg=2.70\n",
            "[5924 | 2950.79] loss=2.84 avg=2.70\n",
            "[5925 | 2951.28] loss=3.14 avg=2.70\n",
            "[5926 | 2951.77] loss=1.62 avg=2.69\n",
            "[5927 | 2952.25] loss=2.73 avg=2.69\n",
            "[5928 | 2952.74] loss=2.74 avg=2.69\n",
            "[5929 | 2953.23] loss=2.81 avg=2.69\n",
            "[5930 | 2953.71] loss=2.88 avg=2.69\n",
            "[5931 | 2954.20] loss=2.92 avg=2.70\n",
            "[5932 | 2954.69] loss=2.79 avg=2.70\n",
            "[5933 | 2955.17] loss=2.83 avg=2.70\n",
            "[5934 | 2955.66] loss=2.74 avg=2.70\n",
            "[5935 | 2956.15] loss=2.41 avg=2.70\n",
            "[5936 | 2956.64] loss=2.68 avg=2.70\n",
            "[5937 | 2957.12] loss=2.89 avg=2.70\n",
            "[5938 | 2957.61] loss=2.80 avg=2.70\n",
            "[5939 | 2958.10] loss=2.95 avg=2.70\n",
            "[5940 | 2958.58] loss=2.35 avg=2.70\n",
            "[5941 | 2959.07] loss=2.75 avg=2.70\n",
            "[5942 | 2959.56] loss=2.53 avg=2.70\n",
            "[5943 | 2960.05] loss=2.78 avg=2.70\n",
            "[5944 | 2960.53] loss=2.76 avg=2.70\n",
            "[5945 | 2961.02] loss=2.16 avg=2.69\n",
            "[5946 | 2961.51] loss=3.04 avg=2.70\n",
            "[5947 | 2962.00] loss=2.56 avg=2.70\n",
            "[5948 | 2962.49] loss=2.86 avg=2.70\n",
            "[5949 | 2962.97] loss=2.91 avg=2.70\n",
            "[5950 | 2963.46] loss=2.66 avg=2.70\n",
            "[5951 | 2963.95] loss=2.39 avg=2.70\n",
            "[5952 | 2964.43] loss=2.76 avg=2.70\n",
            "[5953 | 2964.92] loss=2.69 avg=2.70\n",
            "[5954 | 2965.41] loss=2.62 avg=2.70\n",
            "[5955 | 2965.89] loss=2.45 avg=2.69\n",
            "[5956 | 2966.38] loss=2.45 avg=2.69\n",
            "[5957 | 2966.87] loss=2.82 avg=2.69\n",
            "[5958 | 2967.36] loss=2.85 avg=2.69\n",
            "[5959 | 2967.84] loss=2.32 avg=2.69\n",
            "[5960 | 2968.33] loss=2.94 avg=2.69\n",
            "[5961 | 2968.82] loss=2.74 avg=2.69\n",
            "[5962 | 2969.30] loss=2.75 avg=2.69\n",
            "[5963 | 2969.79] loss=2.82 avg=2.69\n",
            "[5964 | 2970.28] loss=2.69 avg=2.69\n",
            "[5965 | 2970.77] loss=2.58 avg=2.69\n",
            "[5966 | 2971.25] loss=3.03 avg=2.70\n",
            "[5967 | 2971.74] loss=2.84 avg=2.70\n",
            "[5968 | 2972.23] loss=2.75 avg=2.70\n",
            "[5969 | 2972.72] loss=2.92 avg=2.70\n",
            "[5970 | 2973.20] loss=2.84 avg=2.70\n",
            "[5971 | 2973.69] loss=2.90 avg=2.70\n",
            "[5972 | 2974.18] loss=2.90 avg=2.71\n",
            "[5973 | 2974.66] loss=2.70 avg=2.71\n",
            "[5974 | 2975.15] loss=2.38 avg=2.70\n",
            "[5975 | 2975.64] loss=2.81 avg=2.70\n",
            "[5976 | 2976.13] loss=2.69 avg=2.70\n",
            "[5977 | 2976.61] loss=2.99 avg=2.71\n",
            "[5978 | 2977.10] loss=2.64 avg=2.71\n",
            "[5979 | 2977.59] loss=2.99 avg=2.71\n",
            "[5980 | 2978.07] loss=2.71 avg=2.71\n",
            "[5981 | 2978.56] loss=2.44 avg=2.71\n",
            "[5982 | 2979.05] loss=2.73 avg=2.71\n",
            "[5983 | 2979.53] loss=2.82 avg=2.71\n",
            "[5984 | 2980.02] loss=2.83 avg=2.71\n",
            "[5985 | 2980.51] loss=2.92 avg=2.71\n",
            "[5986 | 2981.00] loss=2.73 avg=2.71\n",
            "[5987 | 2981.49] loss=2.92 avg=2.71\n",
            "[5988 | 2981.97] loss=2.58 avg=2.71\n",
            "[5989 | 2982.46] loss=2.41 avg=2.71\n",
            "[5990 | 2982.95] loss=2.75 avg=2.71\n",
            "[5991 | 2983.44] loss=2.65 avg=2.71\n",
            "[5992 | 2983.92] loss=2.51 avg=2.71\n",
            "[5993 | 2984.41] loss=2.64 avg=2.71\n",
            "[5994 | 2984.90] loss=2.72 avg=2.71\n",
            "[5995 | 2985.39] loss=2.62 avg=2.71\n",
            "[5996 | 2985.87] loss=2.74 avg=2.71\n",
            "[5997 | 2986.36] loss=2.94 avg=2.71\n",
            "[5998 | 2986.85] loss=3.33 avg=2.71\n",
            "[5999 | 2987.34] loss=2.50 avg=2.71\n",
            "Saving checkpoint/run1/model-6000\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " did not know what to do. \n",
            "\"I am not a man to be taken for a fool,\" he said. \"I am a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"I am a man to be trusted,\" said Ser Kevan. \"I am a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"I am a man to be trusted,\" said Ser Kevan. \"I am a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"I am a man to be trusted,\" said Ser Kevan. \"I am a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan. \"You are a man to be trusted.\" \n",
            "\"You are a man to be trusted,\" said Ser Kevan\n",
            "\n",
            "[6000 | 3000.36] loss=2.93 avg=2.71\n",
            "[6001 | 3000.85] loss=2.89 avg=2.72\n",
            "[6002 | 3001.34] loss=2.41 avg=2.71\n",
            "[6003 | 3001.83] loss=2.41 avg=2.71\n",
            "[6004 | 3002.31] loss=2.44 avg=2.71\n",
            "[6005 | 3002.80] loss=2.44 avg=2.70\n",
            "[6006 | 3003.29] loss=2.76 avg=2.71\n",
            "[6007 | 3003.78] loss=2.30 avg=2.70\n",
            "[6008 | 3004.26] loss=2.64 avg=2.70\n",
            "[6009 | 3004.75] loss=2.73 avg=2.70\n",
            "[6010 | 3005.23] loss=2.63 avg=2.70\n",
            "[6011 | 3005.72] loss=2.92 avg=2.70\n",
            "[6012 | 3006.21] loss=2.44 avg=2.70\n",
            "[6013 | 3006.70] loss=2.71 avg=2.70\n",
            "[6014 | 3007.18] loss=2.73 avg=2.70\n",
            "[6015 | 3007.67] loss=2.47 avg=2.70\n",
            "[6016 | 3008.16] loss=2.74 avg=2.70\n",
            "[6017 | 3008.64] loss=3.00 avg=2.70\n",
            "[6018 | 3009.13] loss=2.55 avg=2.70\n",
            "[6019 | 3009.62] loss=2.72 avg=2.70\n",
            "[6020 | 3010.11] loss=2.83 avg=2.70\n",
            "[6021 | 3010.59] loss=2.87 avg=2.70\n",
            "[6022 | 3011.08] loss=1.72 avg=2.69\n",
            "[6023 | 3011.57] loss=2.79 avg=2.69\n",
            "[6024 | 3012.06] loss=2.67 avg=2.69\n",
            "[6025 | 3012.54] loss=2.51 avg=2.69\n",
            "[6026 | 3013.03] loss=2.67 avg=2.69\n",
            "[6027 | 3013.52] loss=2.71 avg=2.69\n",
            "[6028 | 3014.01] loss=2.63 avg=2.69\n",
            "[6029 | 3014.49] loss=3.04 avg=2.69\n",
            "[6030 | 3014.98] loss=2.48 avg=2.69\n",
            "[6031 | 3015.47] loss=3.15 avg=2.70\n",
            "[6032 | 3015.96] loss=3.00 avg=2.70\n",
            "[6033 | 3016.44] loss=2.63 avg=2.70\n",
            "[6034 | 3016.93] loss=2.64 avg=2.70\n",
            "[6035 | 3017.42] loss=2.78 avg=2.70\n",
            "[6036 | 3017.90] loss=2.84 avg=2.70\n",
            "[6037 | 3018.39] loss=2.93 avg=2.70\n",
            "[6038 | 3018.88] loss=2.82 avg=2.70\n",
            "[6039 | 3019.37] loss=2.77 avg=2.71\n",
            "[6040 | 3019.85] loss=2.73 avg=2.71\n",
            "[6041 | 3020.34] loss=2.58 avg=2.70\n",
            "[6042 | 3020.83] loss=1.98 avg=2.70\n",
            "[6043 | 3021.31] loss=2.69 avg=2.70\n",
            "[6044 | 3021.80] loss=2.96 avg=2.70\n",
            "[6045 | 3022.29] loss=2.74 avg=2.70\n",
            "[6046 | 3022.78] loss=2.71 avg=2.70\n",
            "[6047 | 3023.26] loss=2.84 avg=2.70\n",
            "[6048 | 3023.75] loss=2.82 avg=2.70\n",
            "[6049 | 3024.24] loss=2.44 avg=2.70\n",
            "[6050 | 3024.73] loss=2.72 avg=2.70\n",
            "[6051 | 3025.22] loss=2.68 avg=2.70\n",
            "[6052 | 3025.70] loss=2.84 avg=2.70\n",
            "[6053 | 3026.20] loss=2.80 avg=2.70\n",
            "[6054 | 3026.69] loss=2.82 avg=2.70\n",
            "[6055 | 3027.18] loss=2.61 avg=2.70\n",
            "[6056 | 3027.67] loss=2.80 avg=2.70\n",
            "[6057 | 3028.16] loss=3.04 avg=2.71\n",
            "[6058 | 3028.64] loss=2.60 avg=2.71\n",
            "[6059 | 3029.13] loss=2.97 avg=2.71\n",
            "[6060 | 3029.62] loss=2.84 avg=2.71\n",
            "[6061 | 3030.10] loss=2.59 avg=2.71\n",
            "[6062 | 3030.59] loss=2.81 avg=2.71\n",
            "[6063 | 3031.08] loss=2.85 avg=2.71\n",
            "[6064 | 3031.56] loss=2.96 avg=2.71\n",
            "[6065 | 3032.05] loss=2.75 avg=2.71\n",
            "[6066 | 3032.54] loss=2.90 avg=2.72\n",
            "[6067 | 3033.03] loss=2.31 avg=2.71\n",
            "[6068 | 3033.51] loss=2.53 avg=2.71\n",
            "[6069 | 3034.00] loss=2.76 avg=2.71\n",
            "[6070 | 3034.49] loss=2.93 avg=2.71\n",
            "[6071 | 3034.98] loss=2.92 avg=2.71\n",
            "[6072 | 3035.46] loss=2.42 avg=2.71\n",
            "[6073 | 3035.95] loss=2.67 avg=2.71\n",
            "[6074 | 3036.44] loss=2.85 avg=2.71\n",
            "[6075 | 3036.93] loss=2.73 avg=2.71\n",
            "[6076 | 3037.41] loss=2.60 avg=2.71\n",
            "[6077 | 3037.90] loss=2.82 avg=2.71\n",
            "[6078 | 3038.39] loss=2.50 avg=2.71\n",
            "[6079 | 3038.88] loss=2.67 avg=2.71\n",
            "[6080 | 3039.37] loss=2.36 avg=2.71\n",
            "[6081 | 3039.86] loss=2.96 avg=2.71\n",
            "[6082 | 3040.36] loss=2.67 avg=2.71\n",
            "[6083 | 3040.84] loss=2.28 avg=2.70\n",
            "[6084 | 3041.33] loss=2.61 avg=2.70\n",
            "[6085 | 3041.82] loss=2.83 avg=2.70\n",
            "[6086 | 3042.30] loss=2.86 avg=2.71\n",
            "[6087 | 3042.79] loss=2.81 avg=2.71\n",
            "[6088 | 3043.28] loss=2.46 avg=2.71\n",
            "[6089 | 3043.76] loss=2.80 avg=2.71\n",
            "[6090 | 3044.25] loss=2.66 avg=2.71\n",
            "[6091 | 3044.74] loss=2.82 avg=2.71\n",
            "[6092 | 3045.23] loss=2.52 avg=2.70\n",
            "[6093 | 3045.71] loss=2.48 avg=2.70\n",
            "[6094 | 3046.20] loss=2.35 avg=2.70\n",
            "[6095 | 3046.69] loss=2.71 avg=2.70\n",
            "[6096 | 3047.18] loss=3.09 avg=2.70\n",
            "[6097 | 3047.67] loss=2.90 avg=2.71\n",
            "[6098 | 3048.16] loss=2.75 avg=2.71\n",
            "[6099 | 3048.66] loss=2.58 avg=2.70\n",
            "[6100 | 3049.14] loss=2.35 avg=2.70\n",
            "[6101 | 3049.63] loss=3.06 avg=2.70\n",
            "[6102 | 3050.12] loss=3.04 avg=2.71\n",
            "[6103 | 3050.61] loss=2.35 avg=2.70\n",
            "[6104 | 3051.10] loss=2.68 avg=2.70\n",
            "[6105 | 3051.58] loss=2.53 avg=2.70\n",
            "[6106 | 3052.07] loss=2.28 avg=2.70\n",
            "[6107 | 3052.56] loss=2.65 avg=2.70\n",
            "[6108 | 3053.04] loss=2.78 avg=2.70\n",
            "[6109 | 3053.53] loss=2.87 avg=2.70\n",
            "[6110 | 3054.02] loss=2.71 avg=2.70\n",
            "[6111 | 3054.50] loss=2.61 avg=2.70\n",
            "[6112 | 3054.99] loss=2.80 avg=2.70\n",
            "[6113 | 3055.48] loss=2.79 avg=2.70\n",
            "[6114 | 3055.97] loss=2.81 avg=2.70\n",
            "[6115 | 3056.46] loss=2.80 avg=2.70\n",
            "[6116 | 3056.94] loss=2.67 avg=2.70\n",
            "[6117 | 3057.43] loss=3.14 avg=2.71\n",
            "[6118 | 3057.92] loss=2.65 avg=2.71\n",
            "[6119 | 3058.41] loss=2.52 avg=2.70\n",
            "[6120 | 3058.90] loss=2.61 avg=2.70\n",
            "[6121 | 3059.39] loss=2.78 avg=2.70\n",
            "[6122 | 3059.87] loss=2.48 avg=2.70\n",
            "[6123 | 3060.37] loss=2.93 avg=2.70\n",
            "[6124 | 3060.85] loss=2.70 avg=2.70\n",
            "[6125 | 3061.34] loss=3.04 avg=2.71\n",
            "[6126 | 3061.83] loss=3.02 avg=2.71\n",
            "[6127 | 3062.32] loss=2.97 avg=2.71\n",
            "[6128 | 3062.80] loss=2.79 avg=2.71\n",
            "[6129 | 3063.29] loss=3.07 avg=2.72\n",
            "[6130 | 3063.78] loss=3.17 avg=2.72\n",
            "[6131 | 3064.27] loss=2.82 avg=2.72\n",
            "[6132 | 3064.75] loss=2.38 avg=2.72\n",
            "[6133 | 3065.24] loss=2.38 avg=2.72\n",
            "[6134 | 3065.73] loss=2.67 avg=2.72\n",
            "[6135 | 3066.22] loss=2.60 avg=2.72\n",
            "[6136 | 3066.71] loss=2.92 avg=2.72\n",
            "[6137 | 3067.19] loss=2.67 avg=2.72\n",
            "[6138 | 3067.69] loss=2.76 avg=2.72\n",
            "[6139 | 3068.17] loss=2.84 avg=2.72\n",
            "[6140 | 3068.66] loss=2.57 avg=2.72\n",
            "[6141 | 3069.15] loss=2.67 avg=2.72\n",
            "[6142 | 3069.63] loss=2.42 avg=2.71\n",
            "[6143 | 3070.12] loss=2.62 avg=2.71\n",
            "[6144 | 3070.61] loss=2.92 avg=2.71\n",
            "[6145 | 3071.09] loss=2.91 avg=2.72\n",
            "[6146 | 3071.58] loss=2.61 avg=2.72\n",
            "[6147 | 3072.07] loss=2.66 avg=2.71\n",
            "[6148 | 3072.56] loss=2.90 avg=2.72\n",
            "[6149 | 3073.04] loss=2.75 avg=2.72\n",
            "[6150 | 3073.53] loss=2.77 avg=2.72\n",
            "[6151 | 3074.02] loss=2.74 avg=2.72\n",
            "[6152 | 3074.51] loss=2.95 avg=2.72\n",
            "[6153 | 3074.99] loss=2.68 avg=2.72\n",
            "[6154 | 3075.48] loss=2.62 avg=2.72\n",
            "[6155 | 3075.97] loss=2.67 avg=2.72\n",
            "[6156 | 3076.45] loss=2.62 avg=2.72\n",
            "[6157 | 3076.94] loss=2.71 avg=2.72\n",
            "[6158 | 3077.43] loss=1.87 avg=2.71\n",
            "[6159 | 3077.92] loss=2.71 avg=2.71\n",
            "[6160 | 3078.40] loss=2.45 avg=2.71\n",
            "[6161 | 3078.89] loss=2.09 avg=2.70\n",
            "[6162 | 3079.38] loss=2.55 avg=2.70\n",
            "[6163 | 3079.86] loss=2.75 avg=2.70\n",
            "[6164 | 3080.35] loss=2.62 avg=2.70\n",
            "[6165 | 3080.84] loss=2.60 avg=2.70\n",
            "[6166 | 3081.33] loss=2.70 avg=2.70\n",
            "[6167 | 3081.81] loss=2.71 avg=2.70\n",
            "[6168 | 3082.30] loss=2.71 avg=2.70\n",
            "[6169 | 3082.79] loss=2.87 avg=2.70\n",
            "[6170 | 3083.27] loss=2.82 avg=2.70\n",
            "[6171 | 3083.76] loss=2.36 avg=2.70\n",
            "[6172 | 3084.25] loss=2.68 avg=2.70\n",
            "[6173 | 3084.73] loss=2.80 avg=2.70\n",
            "[6174 | 3085.22] loss=2.85 avg=2.70\n",
            "[6175 | 3085.71] loss=2.58 avg=2.70\n",
            "[6176 | 3086.19] loss=2.37 avg=2.69\n",
            "[6177 | 3086.68] loss=1.98 avg=2.69\n",
            "[6178 | 3087.17] loss=2.70 avg=2.69\n",
            "[6179 | 3087.65] loss=2.70 avg=2.69\n",
            "[6180 | 3088.14] loss=2.65 avg=2.69\n",
            "[6181 | 3088.63] loss=2.88 avg=2.69\n",
            "[6182 | 3089.12] loss=2.81 avg=2.69\n",
            "[6183 | 3089.60] loss=2.41 avg=2.69\n",
            "[6184 | 3090.09] loss=2.92 avg=2.69\n",
            "[6185 | 3090.58] loss=2.98 avg=2.69\n",
            "[6186 | 3091.07] loss=2.93 avg=2.70\n",
            "[6187 | 3091.55] loss=2.85 avg=2.70\n",
            "[6188 | 3092.04] loss=2.65 avg=2.70\n",
            "[6189 | 3092.53] loss=2.46 avg=2.69\n",
            "[6190 | 3093.02] loss=2.31 avg=2.69\n",
            "[6191 | 3093.50] loss=2.83 avg=2.69\n",
            "[6192 | 3093.99] loss=2.50 avg=2.69\n",
            "[6193 | 3094.48] loss=2.75 avg=2.69\n",
            "[6194 | 3094.96] loss=2.60 avg=2.69\n",
            "[6195 | 3095.45] loss=2.67 avg=2.69\n",
            "[6196 | 3095.94] loss=2.77 avg=2.69\n",
            "[6197 | 3096.43] loss=2.90 avg=2.69\n",
            "[6198 | 3096.91] loss=2.60 avg=2.69\n",
            "[6199 | 3097.40] loss=2.86 avg=2.69\n",
            "[6200 | 3097.89] loss=3.07 avg=2.70\n",
            "[6201 | 3098.38] loss=2.40 avg=2.69\n",
            "[6202 | 3098.86] loss=2.35 avg=2.69\n",
            "[6203 | 3099.35] loss=2.64 avg=2.69\n",
            "[6204 | 3099.84] loss=2.69 avg=2.69\n",
            "[6205 | 3100.32] loss=3.09 avg=2.69\n",
            "[6206 | 3100.81] loss=2.46 avg=2.69\n",
            "[6207 | 3101.30] loss=2.52 avg=2.69\n",
            "[6208 | 3101.79] loss=2.71 avg=2.69\n",
            "[6209 | 3102.28] loss=2.09 avg=2.68\n",
            "[6210 | 3102.76] loss=2.95 avg=2.69\n",
            "[6211 | 3103.25] loss=2.66 avg=2.69\n",
            "[6212 | 3103.74] loss=2.78 avg=2.69\n",
            "[6213 | 3104.22] loss=2.87 avg=2.69\n",
            "[6214 | 3104.71] loss=2.76 avg=2.69\n",
            "[6215 | 3105.20] loss=2.72 avg=2.69\n",
            "[6216 | 3105.68] loss=2.86 avg=2.69\n",
            "[6217 | 3106.17] loss=2.50 avg=2.69\n",
            "[6218 | 3106.66] loss=2.62 avg=2.69\n",
            "[6219 | 3107.15] loss=2.64 avg=2.69\n",
            "[6220 | 3107.63] loss=2.87 avg=2.69\n",
            "[6221 | 3108.12] loss=2.29 avg=2.69\n",
            "[6222 | 3108.61] loss=2.59 avg=2.69\n",
            "[6223 | 3109.10] loss=2.55 avg=2.68\n",
            "[6224 | 3109.58] loss=2.64 avg=2.68\n",
            "[6225 | 3110.07] loss=2.79 avg=2.68\n",
            "[6226 | 3110.56] loss=2.72 avg=2.69\n",
            "[6227 | 3111.04] loss=2.75 avg=2.69\n",
            "[6228 | 3111.53] loss=2.74 avg=2.69\n",
            "[6229 | 3112.02] loss=2.64 avg=2.69\n",
            "[6230 | 3112.51] loss=1.29 avg=2.67\n",
            "[6231 | 3112.99] loss=2.81 avg=2.67\n",
            "[6232 | 3113.48] loss=2.60 avg=2.67\n",
            "[6233 | 3113.97] loss=2.63 avg=2.67\n",
            "[6234 | 3114.45] loss=2.42 avg=2.67\n",
            "[6235 | 3114.94] loss=3.19 avg=2.67\n",
            "[6236 | 3115.43] loss=2.74 avg=2.68\n",
            "[6237 | 3115.92] loss=3.22 avg=2.68\n",
            "[6238 | 3116.40] loss=2.70 avg=2.68\n",
            "[6239 | 3116.89] loss=2.98 avg=2.68\n",
            "[6240 | 3117.38] loss=3.06 avg=2.69\n",
            "[6241 | 3117.87] loss=2.72 avg=2.69\n",
            "[6242 | 3118.35] loss=2.79 avg=2.69\n",
            "[6243 | 3118.84] loss=2.64 avg=2.69\n",
            "[6244 | 3119.33] loss=2.78 avg=2.69\n",
            "[6245 | 3119.82] loss=2.72 avg=2.69\n",
            "[6246 | 3120.30] loss=2.84 avg=2.69\n",
            "[6247 | 3120.79] loss=2.65 avg=2.69\n",
            "[6248 | 3121.28] loss=2.62 avg=2.69\n",
            "[6249 | 3121.77] loss=2.88 avg=2.69\n",
            "[6250 | 3122.25] loss=2.61 avg=2.69\n",
            "[6251 | 3122.74] loss=2.70 avg=2.69\n",
            "[6252 | 3123.23] loss=2.76 avg=2.69\n",
            "[6253 | 3123.72] loss=2.42 avg=2.69\n",
            "[6254 | 3124.20] loss=2.70 avg=2.69\n",
            "[6255 | 3124.69] loss=2.41 avg=2.69\n",
            "[6256 | 3125.18] loss=2.81 avg=2.69\n",
            "[6257 | 3125.66] loss=2.47 avg=2.69\n",
            "[6258 | 3126.16] loss=2.78 avg=2.69\n",
            "[6259 | 3126.64] loss=2.82 avg=2.69\n",
            "[6260 | 3127.13] loss=2.64 avg=2.69\n",
            "[6261 | 3127.62] loss=2.56 avg=2.69\n",
            "[6262 | 3128.11] loss=2.84 avg=2.69\n",
            "[6263 | 3128.59] loss=2.71 avg=2.69\n",
            "[6264 | 3129.08] loss=2.86 avg=2.69\n",
            "[6265 | 3129.57] loss=2.78 avg=2.69\n",
            "[6266 | 3130.06] loss=2.48 avg=2.69\n",
            "[6267 | 3130.55] loss=2.70 avg=2.69\n",
            "[6268 | 3131.03] loss=2.79 avg=2.69\n",
            "[6269 | 3131.52] loss=2.69 avg=2.69\n",
            "[6270 | 3132.01] loss=2.56 avg=2.69\n",
            "[6271 | 3132.50] loss=2.73 avg=2.69\n",
            "[6272 | 3132.99] loss=2.66 avg=2.69\n",
            "[6273 | 3133.47] loss=3.00 avg=2.69\n",
            "[6274 | 3133.96] loss=2.78 avg=2.69\n",
            "[6275 | 3134.45] loss=2.51 avg=2.69\n",
            "[6276 | 3134.94] loss=2.99 avg=2.69\n",
            "[6277 | 3135.42] loss=2.44 avg=2.69\n",
            "[6278 | 3135.91] loss=2.79 avg=2.69\n",
            "[6279 | 3136.40] loss=2.66 avg=2.69\n",
            "[6280 | 3136.89] loss=2.90 avg=2.69\n",
            "[6281 | 3137.37] loss=2.72 avg=2.69\n",
            "[6282 | 3137.86] loss=2.86 avg=2.70\n",
            "[6283 | 3138.35] loss=2.76 avg=2.70\n",
            "[6284 | 3138.83] loss=2.27 avg=2.69\n",
            "[6285 | 3139.32] loss=2.74 avg=2.69\n",
            "[6286 | 3139.81] loss=2.76 avg=2.69\n",
            "[6287 | 3140.29] loss=2.46 avg=2.69\n",
            "[6288 | 3140.78] loss=2.64 avg=2.69\n",
            "[6289 | 3141.27] loss=2.54 avg=2.69\n",
            "[6290 | 3141.76] loss=2.87 avg=2.69\n",
            "[6291 | 3142.24] loss=2.84 avg=2.69\n",
            "[6292 | 3142.73] loss=2.48 avg=2.69\n",
            "[6293 | 3143.22] loss=2.88 avg=2.69\n",
            "[6294 | 3143.70] loss=2.58 avg=2.69\n",
            "[6295 | 3144.19] loss=2.52 avg=2.69\n",
            "[6296 | 3144.68] loss=2.77 avg=2.69\n",
            "[6297 | 3145.16] loss=2.74 avg=2.69\n",
            "[6298 | 3145.65] loss=2.92 avg=2.69\n",
            "[6299 | 3146.14] loss=2.63 avg=2.69\n",
            "[6300 | 3146.63] loss=2.53 avg=2.69\n",
            "[6301 | 3147.12] loss=2.54 avg=2.69\n",
            "[6302 | 3147.60] loss=2.71 avg=2.69\n",
            "[6303 | 3148.09] loss=2.89 avg=2.69\n",
            "[6304 | 3148.58] loss=2.77 avg=2.69\n",
            "[6305 | 3149.07] loss=2.53 avg=2.69\n",
            "[6306 | 3149.55] loss=2.50 avg=2.69\n",
            "[6307 | 3150.04] loss=2.12 avg=2.68\n",
            "[6308 | 3150.52] loss=2.75 avg=2.68\n",
            "[6309 | 3151.01] loss=2.40 avg=2.68\n",
            "[6310 | 3151.50] loss=2.61 avg=2.68\n",
            "[6311 | 3151.99] loss=2.62 avg=2.68\n",
            "[6312 | 3152.47] loss=2.69 avg=2.68\n",
            "[6313 | 3152.96] loss=2.83 avg=2.68\n",
            "[6314 | 3153.45] loss=2.79 avg=2.68\n",
            "[6315 | 3153.94] loss=2.51 avg=2.68\n",
            "[6316 | 3154.42] loss=2.69 avg=2.68\n",
            "[6317 | 3154.91] loss=2.99 avg=2.68\n",
            "[6318 | 3155.40] loss=2.49 avg=2.68\n",
            "[6319 | 3155.88] loss=3.15 avg=2.69\n",
            "[6320 | 3156.37] loss=2.67 avg=2.69\n",
            "[6321 | 3156.86] loss=2.88 avg=2.69\n",
            "[6322 | 3157.35] loss=2.34 avg=2.68\n",
            "[6323 | 3157.84] loss=2.95 avg=2.69\n",
            "[6324 | 3158.33] loss=2.29 avg=2.68\n",
            "[6325 | 3158.81] loss=2.95 avg=2.69\n",
            "[6326 | 3159.30] loss=2.42 avg=2.68\n",
            "[6327 | 3159.79] loss=2.59 avg=2.68\n",
            "[6328 | 3160.27] loss=2.68 avg=2.68\n",
            "[6329 | 3160.76] loss=2.72 avg=2.68\n",
            "[6330 | 3161.24] loss=3.04 avg=2.69\n",
            "[6331 | 3161.73] loss=2.85 avg=2.69\n",
            "[6332 | 3162.22] loss=2.41 avg=2.68\n",
            "[6333 | 3162.71] loss=2.92 avg=2.69\n",
            "[6334 | 3163.19] loss=2.86 avg=2.69\n",
            "[6335 | 3163.68] loss=2.62 avg=2.69\n",
            "[6336 | 3164.17] loss=2.46 avg=2.69\n",
            "[6337 | 3164.65] loss=2.84 avg=2.69\n",
            "[6338 | 3165.14] loss=2.67 avg=2.69\n",
            "[6339 | 3165.63] loss=3.12 avg=2.69\n",
            "[6340 | 3166.12] loss=2.65 avg=2.69\n",
            "[6341 | 3166.60] loss=2.16 avg=2.69\n",
            "[6342 | 3167.09] loss=2.67 avg=2.69\n",
            "[6343 | 3167.58] loss=2.85 avg=2.69\n",
            "[6344 | 3168.06] loss=2.94 avg=2.69\n",
            "[6345 | 3168.55] loss=2.83 avg=2.69\n",
            "[6346 | 3169.04] loss=3.04 avg=2.69\n",
            "[6347 | 3169.53] loss=2.66 avg=2.69\n",
            "[6348 | 3170.01] loss=2.76 avg=2.70\n",
            "[6349 | 3170.50] loss=2.60 avg=2.69\n",
            "[6350 | 3170.99] loss=2.87 avg=2.70\n",
            "[6351 | 3171.47] loss=2.74 avg=2.70\n",
            "[6352 | 3171.96] loss=2.32 avg=2.69\n",
            "[6353 | 3172.45] loss=2.88 avg=2.69\n",
            "[6354 | 3172.94] loss=2.69 avg=2.69\n",
            "[6355 | 3173.43] loss=3.09 avg=2.70\n",
            "[6356 | 3173.91] loss=2.95 avg=2.70\n",
            "[6357 | 3174.40] loss=2.81 avg=2.70\n",
            "[6358 | 3174.88] loss=2.62 avg=2.70\n",
            "[6359 | 3175.37] loss=2.78 avg=2.70\n",
            "[6360 | 3175.85] loss=2.83 avg=2.70\n",
            "[6361 | 3176.35] loss=2.84 avg=2.70\n",
            "[6362 | 3176.83] loss=2.63 avg=2.70\n",
            "[6363 | 3177.32] loss=1.59 avg=2.69\n",
            "[6364 | 3177.81] loss=2.86 avg=2.69\n",
            "[6365 | 3178.30] loss=2.59 avg=2.69\n",
            "[6366 | 3178.78] loss=2.72 avg=2.69\n",
            "[6367 | 3179.27] loss=2.99 avg=2.70\n",
            "[6368 | 3179.76] loss=2.56 avg=2.70\n",
            "[6369 | 3180.24] loss=2.42 avg=2.69\n",
            "[6370 | 3180.73] loss=2.58 avg=2.69\n",
            "[6371 | 3181.22] loss=2.88 avg=2.69\n",
            "[6372 | 3181.70] loss=2.77 avg=2.69\n",
            "[6373 | 3182.19] loss=2.10 avg=2.69\n",
            "[6374 | 3182.68] loss=2.89 avg=2.69\n",
            "[6375 | 3183.17] loss=2.50 avg=2.69\n",
            "[6376 | 3183.65] loss=2.81 avg=2.69\n",
            "[6377 | 3184.14] loss=2.77 avg=2.69\n",
            "[6378 | 3184.62] loss=2.66 avg=2.69\n",
            "[6379 | 3185.11] loss=2.64 avg=2.69\n",
            "[6380 | 3185.60] loss=2.56 avg=2.69\n",
            "[6381 | 3186.09] loss=2.85 avg=2.69\n",
            "[6382 | 3186.57] loss=2.61 avg=2.69\n",
            "[6383 | 3187.06] loss=2.76 avg=2.69\n",
            "[6384 | 3187.55] loss=2.78 avg=2.69\n",
            "[6385 | 3188.04] loss=2.75 avg=2.69\n",
            "[6386 | 3188.53] loss=2.48 avg=2.69\n",
            "[6387 | 3189.01] loss=2.77 avg=2.69\n",
            "[6388 | 3189.50] loss=2.78 avg=2.69\n",
            "[6389 | 3189.99] loss=3.12 avg=2.70\n",
            "[6390 | 3190.48] loss=2.68 avg=2.69\n",
            "[6391 | 3190.96] loss=1.94 avg=2.69\n",
            "[6392 | 3191.45] loss=2.78 avg=2.69\n",
            "[6393 | 3191.94] loss=2.73 avg=2.69\n",
            "[6394 | 3192.43] loss=2.65 avg=2.69\n",
            "[6395 | 3192.91] loss=2.59 avg=2.69\n",
            "[6396 | 3193.40] loss=2.46 avg=2.69\n",
            "[6397 | 3193.89] loss=2.71 avg=2.69\n",
            "[6398 | 3194.38] loss=2.63 avg=2.68\n",
            "[6399 | 3194.87] loss=2.63 avg=2.68\n",
            "[6400 | 3195.36] loss=2.71 avg=2.68\n",
            "[6401 | 3195.85] loss=2.44 avg=2.68\n",
            "[6402 | 3196.33] loss=2.74 avg=2.68\n",
            "[6403 | 3196.82] loss=2.73 avg=2.68\n",
            "[6404 | 3197.31] loss=2.31 avg=2.68\n",
            "[6405 | 3197.80] loss=2.50 avg=2.68\n",
            "[6406 | 3198.29] loss=3.05 avg=2.68\n",
            "[6407 | 3198.78] loss=2.64 avg=2.68\n",
            "[6408 | 3199.27] loss=2.33 avg=2.68\n",
            "[6409 | 3199.75] loss=2.84 avg=2.68\n",
            "[6410 | 3200.24] loss=3.00 avg=2.68\n",
            "[6411 | 3200.73] loss=2.76 avg=2.68\n",
            "[6412 | 3201.22] loss=2.64 avg=2.68\n",
            "[6413 | 3201.71] loss=2.83 avg=2.68\n",
            "[6414 | 3202.19] loss=2.78 avg=2.68\n",
            "[6415 | 3202.68] loss=2.62 avg=2.68\n",
            "[6416 | 3203.17] loss=2.56 avg=2.68\n",
            "[6417 | 3203.65] loss=2.62 avg=2.68\n",
            "[6418 | 3204.14] loss=2.90 avg=2.68\n",
            "[6419 | 3204.63] loss=2.62 avg=2.68\n",
            "[6420 | 3205.11] loss=2.74 avg=2.68\n",
            "[6421 | 3205.60] loss=2.79 avg=2.69\n",
            "[6422 | 3206.09] loss=2.52 avg=2.68\n",
            "[6423 | 3206.58] loss=3.04 avg=2.69\n",
            "[6424 | 3207.07] loss=2.68 avg=2.69\n",
            "[6425 | 3207.55] loss=2.66 avg=2.69\n",
            "[6426 | 3208.04] loss=2.69 avg=2.69\n",
            "[6427 | 3208.52] loss=2.96 avg=2.69\n",
            "[6428 | 3209.01] loss=2.73 avg=2.69\n",
            "[6429 | 3209.50] loss=2.74 avg=2.69\n",
            "[6430 | 3209.99] loss=2.70 avg=2.69\n",
            "[6431 | 3210.47] loss=2.54 avg=2.69\n",
            "[6432 | 3210.96] loss=2.38 avg=2.69\n",
            "[6433 | 3211.45] loss=2.82 avg=2.69\n",
            "[6434 | 3211.93] loss=2.65 avg=2.69\n",
            "[6435 | 3212.42] loss=2.46 avg=2.68\n",
            "[6436 | 3212.91] loss=2.63 avg=2.68\n",
            "[6437 | 3213.40] loss=2.56 avg=2.68\n",
            "[6438 | 3213.88] loss=2.61 avg=2.68\n",
            "[6439 | 3214.37] loss=2.76 avg=2.68\n",
            "[6440 | 3214.86] loss=2.45 avg=2.68\n",
            "[6441 | 3215.35] loss=2.63 avg=2.68\n",
            "[6442 | 3215.83] loss=2.34 avg=2.68\n",
            "[6443 | 3216.32] loss=2.89 avg=2.68\n",
            "[6444 | 3216.81] loss=2.72 avg=2.68\n",
            "[6445 | 3217.29] loss=2.75 avg=2.68\n",
            "[6446 | 3217.78] loss=2.72 avg=2.68\n",
            "[6447 | 3218.27] loss=2.99 avg=2.68\n",
            "[6448 | 3218.76] loss=2.90 avg=2.69\n",
            "[6449 | 3219.25] loss=2.66 avg=2.69\n",
            "[6450 | 3219.73] loss=2.83 avg=2.69\n",
            "[6451 | 3220.22] loss=2.73 avg=2.69\n",
            "[6452 | 3220.70] loss=2.91 avg=2.69\n",
            "[6453 | 3221.19] loss=2.62 avg=2.69\n",
            "[6454 | 3221.68] loss=2.89 avg=2.69\n",
            "[6455 | 3222.17] loss=2.65 avg=2.69\n",
            "[6456 | 3222.65] loss=2.77 avg=2.69\n",
            "[6457 | 3223.14] loss=2.84 avg=2.69\n",
            "[6458 | 3223.63] loss=2.65 avg=2.69\n",
            "[6459 | 3224.12] loss=3.10 avg=2.70\n",
            "[6460 | 3224.60] loss=2.70 avg=2.70\n",
            "[6461 | 3225.09] loss=3.13 avg=2.70\n",
            "[6462 | 3225.58] loss=2.66 avg=2.70\n",
            "[6463 | 3226.06] loss=2.93 avg=2.70\n",
            "[6464 | 3226.55] loss=2.63 avg=2.70\n",
            "[6465 | 3227.04] loss=2.44 avg=2.70\n",
            "[6466 | 3227.53] loss=3.02 avg=2.70\n",
            "[6467 | 3228.01] loss=2.83 avg=2.70\n",
            "[6468 | 3228.50] loss=1.63 avg=2.69\n",
            "[6469 | 3228.99] loss=2.81 avg=2.69\n",
            "[6470 | 3229.48] loss=2.55 avg=2.69\n",
            "[6471 | 3229.96] loss=2.72 avg=2.69\n",
            "[6472 | 3230.45] loss=2.53 avg=2.69\n",
            "[6473 | 3230.94] loss=2.72 avg=2.69\n",
            "[6474 | 3231.42] loss=1.77 avg=2.68\n",
            "[6475 | 3231.91] loss=2.96 avg=2.69\n",
            "[6476 | 3232.40] loss=2.62 avg=2.68\n",
            "[6477 | 3232.88] loss=2.57 avg=2.68\n",
            "[6478 | 3233.37] loss=2.58 avg=2.68\n",
            "[6479 | 3233.86] loss=2.69 avg=2.68\n",
            "[6480 | 3234.34] loss=3.10 avg=2.69\n",
            "[6481 | 3234.83] loss=2.63 avg=2.69\n",
            "[6482 | 3235.32] loss=2.73 avg=2.69\n",
            "[6483 | 3235.81] loss=2.80 avg=2.69\n",
            "[6484 | 3236.30] loss=3.14 avg=2.69\n",
            "[6485 | 3236.78] loss=2.50 avg=2.69\n",
            "[6486 | 3237.27] loss=2.40 avg=2.69\n",
            "[6487 | 3237.76] loss=2.80 avg=2.69\n",
            "[6488 | 3238.24] loss=2.64 avg=2.69\n",
            "[6489 | 3238.73] loss=2.89 avg=2.69\n",
            "[6490 | 3239.22] loss=2.80 avg=2.69\n",
            "[6491 | 3239.71] loss=2.55 avg=2.69\n",
            "[6492 | 3240.19] loss=2.70 avg=2.69\n",
            "[6493 | 3240.68] loss=2.36 avg=2.69\n",
            "[6494 | 3241.16] loss=2.73 avg=2.69\n",
            "[6495 | 3241.65] loss=2.74 avg=2.69\n",
            "[6496 | 3242.14] loss=2.65 avg=2.69\n",
            "[6497 | 3242.63] loss=2.72 avg=2.69\n",
            "[6498 | 3243.12] loss=2.98 avg=2.69\n",
            "[6499 | 3243.60] loss=2.71 avg=2.69\n",
            "[6500 | 3244.09] loss=2.65 avg=2.69\n",
            "[6501 | 3244.58] loss=2.72 avg=2.69\n",
            "[6502 | 3245.07] loss=3.18 avg=2.70\n",
            "[6503 | 3245.55] loss=2.93 avg=2.70\n",
            "[6504 | 3246.04] loss=2.74 avg=2.70\n",
            "[6505 | 3246.52] loss=2.51 avg=2.70\n",
            "[6506 | 3247.01] loss=3.02 avg=2.70\n",
            "[6507 | 3247.50] loss=2.49 avg=2.70\n",
            "[6508 | 3247.99] loss=2.67 avg=2.70\n",
            "[6509 | 3248.47] loss=2.84 avg=2.70\n",
            "[6510 | 3248.97] loss=2.76 avg=2.70\n",
            "[6511 | 3249.45] loss=2.74 avg=2.70\n",
            "[6512 | 3249.94] loss=2.34 avg=2.70\n",
            "[6513 | 3250.42] loss=2.38 avg=2.69\n",
            "[6514 | 3250.91] loss=2.21 avg=2.69\n",
            "[6515 | 3251.40] loss=2.88 avg=2.69\n",
            "[6516 | 3251.89] loss=3.12 avg=2.69\n",
            "[6517 | 3252.37] loss=2.86 avg=2.70\n",
            "[6518 | 3252.86] loss=2.61 avg=2.70\n",
            "[6519 | 3253.35] loss=3.07 avg=2.70\n",
            "[6520 | 3253.84] loss=3.03 avg=2.70\n",
            "[6521 | 3254.33] loss=2.61 avg=2.70\n",
            "[6522 | 3254.82] loss=2.50 avg=2.70\n",
            "[6523 | 3255.30] loss=2.78 avg=2.70\n",
            "[6524 | 3255.79] loss=2.59 avg=2.70\n",
            "[6525 | 3256.28] loss=2.61 avg=2.70\n",
            "[6526 | 3256.77] loss=2.73 avg=2.70\n",
            "[6527 | 3257.25] loss=2.69 avg=2.70\n",
            "[6528 | 3257.74] loss=2.79 avg=2.70\n",
            "[6529 | 3258.23] loss=2.73 avg=2.70\n",
            "[6530 | 3258.71] loss=2.69 avg=2.70\n",
            "[6531 | 3259.20] loss=3.11 avg=2.70\n",
            "[6532 | 3259.69] loss=2.56 avg=2.70\n",
            "[6533 | 3260.18] loss=2.79 avg=2.70\n",
            "[6534 | 3260.67] loss=2.55 avg=2.70\n",
            "[6535 | 3261.15] loss=2.32 avg=2.70\n",
            "[6536 | 3261.64] loss=2.71 avg=2.70\n",
            "[6537 | 3262.13] loss=2.78 avg=2.70\n",
            "[6538 | 3262.62] loss=2.44 avg=2.70\n",
            "[6539 | 3263.11] loss=2.80 avg=2.70\n",
            "[6540 | 3263.60] loss=3.03 avg=2.70\n",
            "[6541 | 3264.08] loss=2.55 avg=2.70\n",
            "[6542 | 3264.57] loss=3.05 avg=2.70\n",
            "[6543 | 3265.06] loss=2.44 avg=2.70\n",
            "[6544 | 3265.55] loss=2.90 avg=2.70\n",
            "[6545 | 3266.03] loss=2.83 avg=2.70\n",
            "[6546 | 3266.52] loss=2.96 avg=2.71\n",
            "[6547 | 3267.01] loss=2.72 avg=2.71\n",
            "[6548 | 3267.50] loss=2.61 avg=2.70\n",
            "[6549 | 3267.99] loss=2.86 avg=2.71\n",
            "[6550 | 3268.47] loss=2.72 avg=2.71\n",
            "[6551 | 3268.96] loss=2.48 avg=2.70\n",
            "[6552 | 3269.45] loss=2.83 avg=2.71\n",
            "[6553 | 3269.94] loss=2.45 avg=2.70\n",
            "[6554 | 3270.43] loss=2.68 avg=2.70\n",
            "[6555 | 3270.92] loss=2.90 avg=2.70\n",
            "[6556 | 3271.41] loss=2.47 avg=2.70\n",
            "[6557 | 3271.89] loss=1.28 avg=2.69\n",
            "[6558 | 3272.38] loss=2.49 avg=2.69\n",
            "[6559 | 3272.87] loss=2.35 avg=2.68\n",
            "[6560 | 3273.35] loss=2.57 avg=2.68\n",
            "[6561 | 3273.84] loss=2.83 avg=2.68\n",
            "[6562 | 3274.32] loss=2.65 avg=2.68\n",
            "[6563 | 3274.81] loss=2.48 avg=2.68\n",
            "[6564 | 3275.30] loss=2.58 avg=2.68\n",
            "[6565 | 3275.79] loss=2.50 avg=2.68\n",
            "[6566 | 3276.27] loss=2.85 avg=2.68\n",
            "[6567 | 3276.76] loss=2.84 avg=2.68\n",
            "[6568 | 3277.25] loss=2.71 avg=2.68\n",
            "[6569 | 3277.74] loss=2.81 avg=2.68\n",
            "[6570 | 3278.22] loss=2.53 avg=2.68\n",
            "[6571 | 3278.71] loss=2.81 avg=2.68\n",
            "[6572 | 3279.20] loss=2.64 avg=2.68\n",
            "[6573 | 3279.68] loss=2.84 avg=2.68\n",
            "[6574 | 3280.17] loss=2.79 avg=2.68\n",
            "[6575 | 3280.66] loss=2.63 avg=2.68\n",
            "[6576 | 3281.15] loss=2.59 avg=2.68\n",
            "[6577 | 3281.63] loss=2.47 avg=2.68\n",
            "[6578 | 3282.13] loss=2.18 avg=2.68\n",
            "[6579 | 3282.61] loss=2.81 avg=2.68\n",
            "[6580 | 3283.10] loss=2.35 avg=2.67\n",
            "[6581 | 3283.58] loss=2.99 avg=2.68\n",
            "[6582 | 3284.07] loss=1.59 avg=2.67\n",
            "[6583 | 3284.56] loss=2.51 avg=2.66\n",
            "[6584 | 3285.04] loss=2.76 avg=2.67\n",
            "[6585 | 3285.53] loss=2.87 avg=2.67\n",
            "[6586 | 3286.02] loss=2.37 avg=2.66\n",
            "[6587 | 3286.51] loss=2.72 avg=2.67\n",
            "[6588 | 3286.99] loss=2.77 avg=2.67\n",
            "[6589 | 3287.48] loss=2.20 avg=2.66\n",
            "[6590 | 3287.97] loss=2.63 avg=2.66\n",
            "[6591 | 3288.46] loss=2.61 avg=2.66\n",
            "[6592 | 3288.94] loss=2.88 avg=2.66\n",
            "[6593 | 3289.43] loss=3.02 avg=2.67\n",
            "[6594 | 3289.92] loss=2.51 avg=2.67\n",
            "[6595 | 3290.40] loss=2.52 avg=2.66\n",
            "[6596 | 3290.89] loss=2.63 avg=2.66\n",
            "[6597 | 3291.38] loss=2.23 avg=2.66\n",
            "[6598 | 3291.87] loss=2.78 avg=2.66\n",
            "[6599 | 3292.35] loss=2.67 avg=2.66\n",
            "[6600 | 3292.84] loss=2.43 avg=2.66\n",
            "[6601 | 3293.32] loss=2.67 avg=2.66\n",
            "[6602 | 3293.81] loss=2.64 avg=2.66\n",
            "[6603 | 3294.30] loss=2.67 avg=2.66\n",
            "[6604 | 3294.79] loss=2.86 avg=2.66\n",
            "[6605 | 3295.27] loss=2.72 avg=2.66\n",
            "[6606 | 3295.76] loss=2.75 avg=2.66\n",
            "[6607 | 3296.25] loss=2.86 avg=2.66\n",
            "[6608 | 3296.74] loss=3.24 avg=2.67\n",
            "[6609 | 3297.22] loss=2.83 avg=2.67\n",
            "[6610 | 3297.71] loss=2.70 avg=2.67\n",
            "[6611 | 3298.20] loss=2.60 avg=2.67\n",
            "[6612 | 3298.69] loss=2.88 avg=2.67\n",
            "[6613 | 3299.17] loss=2.56 avg=2.67\n",
            "[6614 | 3299.66] loss=2.57 avg=2.67\n",
            "[6615 | 3300.15] loss=2.63 avg=2.67\n",
            "[6616 | 3300.64] loss=2.95 avg=2.67\n",
            "[6617 | 3301.13] loss=2.91 avg=2.68\n",
            "[6618 | 3301.61] loss=2.45 avg=2.67\n",
            "[6619 | 3302.10] loss=1.99 avg=2.67\n",
            "[6620 | 3302.59] loss=2.65 avg=2.67\n",
            "[6621 | 3303.07] loss=2.60 avg=2.67\n",
            "[6622 | 3303.56] loss=2.62 avg=2.66\n",
            "[6623 | 3304.05] loss=2.50 avg=2.66\n",
            "[6624 | 3304.54] loss=2.73 avg=2.66\n",
            "[6625 | 3305.02] loss=2.79 avg=2.67\n",
            "[6626 | 3305.51] loss=2.64 avg=2.67\n",
            "[6627 | 3306.00] loss=2.59 avg=2.66\n",
            "[6628 | 3306.49] loss=2.56 avg=2.66\n",
            "[6629 | 3306.97] loss=2.55 avg=2.66\n",
            "[6630 | 3307.46] loss=3.06 avg=2.67\n",
            "[6631 | 3307.95] loss=2.84 avg=2.67\n",
            "[6632 | 3308.43] loss=2.39 avg=2.67\n",
            "[6633 | 3308.92] loss=2.57 avg=2.66\n",
            "[6634 | 3309.41] loss=2.85 avg=2.67\n",
            "[6635 | 3309.89] loss=2.68 avg=2.67\n",
            "[6636 | 3310.38] loss=2.80 avg=2.67\n",
            "[6637 | 3310.87] loss=2.66 avg=2.67\n",
            "[6638 | 3311.35] loss=2.74 avg=2.67\n",
            "[6639 | 3311.84] loss=2.47 avg=2.67\n",
            "[6640 | 3312.33] loss=2.64 avg=2.67\n",
            "[6641 | 3312.82] loss=2.60 avg=2.67\n",
            "[6642 | 3313.30] loss=2.92 avg=2.67\n",
            "[6643 | 3313.79] loss=2.62 avg=2.67\n",
            "[6644 | 3314.28] loss=2.37 avg=2.66\n",
            "[6645 | 3314.77] loss=2.93 avg=2.67\n",
            "[6646 | 3315.25] loss=2.70 avg=2.67\n",
            "[6647 | 3315.74] loss=2.75 avg=2.67\n",
            "[6648 | 3316.23] loss=2.65 avg=2.67\n",
            "[6649 | 3316.72] loss=2.89 avg=2.67\n",
            "[6650 | 3317.20] loss=2.56 avg=2.67\n",
            "[6651 | 3317.69] loss=2.85 avg=2.67\n",
            "[6652 | 3318.17] loss=2.61 avg=2.67\n",
            "[6653 | 3318.66] loss=2.97 avg=2.67\n",
            "[6654 | 3319.15] loss=2.80 avg=2.67\n",
            "[6655 | 3319.64] loss=2.79 avg=2.68\n",
            "[6656 | 3320.12] loss=2.70 avg=2.68\n",
            "[6657 | 3320.61] loss=2.81 avg=2.68\n",
            "[6658 | 3321.10] loss=2.85 avg=2.68\n",
            "[6659 | 3321.59] loss=2.64 avg=2.68\n",
            "[6660 | 3322.07] loss=2.94 avg=2.68\n",
            "[6661 | 3322.56] loss=2.20 avg=2.68\n",
            "[6662 | 3323.05] loss=2.74 avg=2.68\n",
            "[6663 | 3323.54] loss=2.73 avg=2.68\n",
            "[6664 | 3324.02] loss=2.77 avg=2.68\n",
            "[6665 | 3324.51] loss=2.84 avg=2.68\n",
            "[6666 | 3325.00] loss=2.57 avg=2.68\n",
            "[6667 | 3325.49] loss=2.71 avg=2.68\n",
            "[6668 | 3325.97] loss=2.98 avg=2.68\n",
            "[6669 | 3326.46] loss=2.39 avg=2.68\n",
            "[6670 | 3326.95] loss=2.43 avg=2.68\n",
            "[6671 | 3327.44] loss=2.78 avg=2.68\n",
            "[6672 | 3327.93] loss=2.28 avg=2.67\n",
            "[6673 | 3328.42] loss=2.22 avg=2.67\n",
            "[6674 | 3328.90] loss=2.93 avg=2.67\n",
            "[6675 | 3329.39] loss=2.42 avg=2.67\n",
            "[6676 | 3329.88] loss=2.48 avg=2.67\n",
            "[6677 | 3330.37] loss=2.87 avg=2.67\n",
            "[6678 | 3330.86] loss=2.68 avg=2.67\n",
            "[6679 | 3331.35] loss=2.45 avg=2.67\n",
            "[6680 | 3331.83] loss=2.69 avg=2.67\n",
            "[6681 | 3332.32] loss=2.44 avg=2.67\n",
            "[6682 | 3332.81] loss=3.10 avg=2.67\n",
            "[6683 | 3333.30] loss=2.91 avg=2.67\n",
            "[6684 | 3333.79] loss=2.71 avg=2.67\n",
            "[6685 | 3334.27] loss=2.65 avg=2.67\n",
            "[6686 | 3334.77] loss=2.68 avg=2.67\n",
            "[6687 | 3335.25] loss=2.82 avg=2.67\n",
            "[6688 | 3335.74] loss=2.51 avg=2.67\n",
            "[6689 | 3336.23] loss=2.74 avg=2.67\n",
            "[6690 | 3336.72] loss=3.07 avg=2.68\n",
            "[6691 | 3337.20] loss=2.67 avg=2.68\n",
            "[6692 | 3337.69] loss=2.58 avg=2.68\n",
            "[6693 | 3338.18] loss=2.69 avg=2.68\n",
            "[6694 | 3338.67] loss=2.35 avg=2.67\n",
            "[6695 | 3339.16] loss=2.39 avg=2.67\n",
            "[6696 | 3339.64] loss=2.68 avg=2.67\n",
            "[6697 | 3340.13] loss=2.72 avg=2.67\n",
            "[6698 | 3340.62] loss=2.67 avg=2.67\n",
            "[6699 | 3341.11] loss=2.87 avg=2.67\n",
            "[6700 | 3341.59] loss=2.90 avg=2.67\n",
            "[6701 | 3342.08] loss=2.42 avg=2.67\n",
            "[6702 | 3342.56] loss=2.69 avg=2.67\n",
            "[6703 | 3343.05] loss=2.33 avg=2.67\n",
            "[6704 | 3343.54] loss=2.74 avg=2.67\n",
            "[6705 | 3344.03] loss=2.76 avg=2.67\n",
            "[6706 | 3344.51] loss=2.38 avg=2.67\n",
            "[6707 | 3345.00] loss=2.58 avg=2.67\n",
            "[6708 | 3345.49] loss=3.19 avg=2.67\n",
            "[6709 | 3345.98] loss=2.92 avg=2.67\n",
            "[6710 | 3346.46] loss=2.46 avg=2.67\n",
            "[6711 | 3346.95] loss=2.48 avg=2.67\n",
            "[6712 | 3347.43] loss=2.93 avg=2.67\n",
            "[6713 | 3347.92] loss=2.94 avg=2.68\n",
            "[6714 | 3348.41] loss=2.28 avg=2.67\n",
            "[6715 | 3348.90] loss=2.88 avg=2.67\n",
            "[6716 | 3349.39] loss=2.72 avg=2.67\n",
            "[6717 | 3349.87] loss=2.51 avg=2.67\n",
            "[6718 | 3350.36] loss=2.55 avg=2.67\n",
            "[6719 | 3350.85] loss=2.53 avg=2.67\n",
            "[6720 | 3351.33] loss=2.89 avg=2.67\n",
            "[6721 | 3351.82] loss=2.44 avg=2.67\n",
            "[6722 | 3352.31] loss=2.60 avg=2.67\n",
            "[6723 | 3352.80] loss=2.87 avg=2.67\n",
            "[6724 | 3353.28] loss=2.66 avg=2.67\n",
            "[6725 | 3353.77] loss=2.50 avg=2.67\n",
            "[6726 | 3354.26] loss=2.95 avg=2.67\n",
            "[6727 | 3354.74] loss=2.70 avg=2.67\n",
            "[6728 | 3355.23] loss=2.58 avg=2.67\n",
            "[6729 | 3355.72] loss=2.76 avg=2.67\n",
            "[6730 | 3356.21] loss=3.01 avg=2.68\n",
            "[6731 | 3356.69] loss=2.72 avg=2.68\n",
            "[6732 | 3357.18] loss=1.98 avg=2.67\n",
            "[6733 | 3357.67] loss=3.14 avg=2.67\n",
            "[6734 | 3358.15] loss=2.66 avg=2.67\n",
            "[6735 | 3358.64] loss=2.13 avg=2.67\n",
            "[6736 | 3359.13] loss=2.93 avg=2.67\n",
            "[6737 | 3359.61] loss=1.79 avg=2.66\n",
            "[6738 | 3360.10] loss=2.38 avg=2.66\n",
            "[6739 | 3360.59] loss=2.47 avg=2.66\n",
            "[6740 | 3361.08] loss=2.91 avg=2.66\n",
            "[6741 | 3361.57] loss=2.78 avg=2.66\n",
            "[6742 | 3362.05] loss=2.68 avg=2.66\n",
            "[6743 | 3362.54] loss=2.67 avg=2.66\n",
            "[6744 | 3363.03] loss=2.78 avg=2.66\n",
            "[6745 | 3363.51] loss=2.64 avg=2.66\n",
            "[6746 | 3364.00] loss=2.38 avg=2.66\n",
            "[6747 | 3364.48] loss=2.77 avg=2.66\n",
            "[6748 | 3364.97] loss=2.83 avg=2.66\n",
            "[6749 | 3365.46] loss=2.53 avg=2.66\n",
            "[6750 | 3365.95] loss=2.20 avg=2.66\n",
            "[6751 | 3366.43] loss=2.54 avg=2.66\n",
            "[6752 | 3366.92] loss=2.53 avg=2.65\n",
            "[6753 | 3367.41] loss=2.72 avg=2.65\n",
            "[6754 | 3367.89] loss=2.74 avg=2.66\n",
            "[6755 | 3368.38] loss=2.52 avg=2.65\n",
            "[6756 | 3368.87] loss=2.41 avg=2.65\n",
            "[6757 | 3369.36] loss=2.85 avg=2.65\n",
            "[6758 | 3369.84] loss=2.62 avg=2.65\n",
            "[6759 | 3370.33] loss=2.63 avg=2.65\n",
            "[6760 | 3370.82] loss=2.66 avg=2.65\n",
            "[6761 | 3371.31] loss=2.75 avg=2.65\n",
            "[6762 | 3371.79] loss=2.73 avg=2.65\n",
            "[6763 | 3372.28] loss=2.86 avg=2.66\n",
            "[6764 | 3372.77] loss=2.91 avg=2.66\n",
            "[6765 | 3373.26] loss=2.90 avg=2.66\n",
            "[6766 | 3373.74] loss=2.82 avg=2.66\n",
            "[6767 | 3374.23] loss=2.51 avg=2.66\n",
            "[6768 | 3374.72] loss=2.48 avg=2.66\n",
            "[6769 | 3375.21] loss=2.96 avg=2.66\n",
            "[6770 | 3375.69] loss=2.91 avg=2.67\n",
            "[6771 | 3376.18] loss=2.72 avg=2.67\n",
            "[6772 | 3376.67] loss=2.76 avg=2.67\n",
            "[6773 | 3377.16] loss=2.75 avg=2.67\n",
            "[6774 | 3377.64] loss=3.12 avg=2.67\n",
            "[6775 | 3378.13] loss=2.68 avg=2.67\n",
            "[6776 | 3378.62] loss=2.51 avg=2.67\n",
            "[6777 | 3379.11] loss=2.48 avg=2.67\n",
            "[6778 | 3379.59] loss=2.63 avg=2.67\n",
            "[6779 | 3380.08] loss=2.43 avg=2.67\n",
            "[6780 | 3380.57] loss=2.76 avg=2.67\n",
            "[6781 | 3381.05] loss=2.62 avg=2.67\n",
            "[6782 | 3381.54] loss=2.61 avg=2.67\n",
            "[6783 | 3382.03] loss=2.69 avg=2.67\n",
            "[6784 | 3382.52] loss=2.82 avg=2.67\n",
            "[6785 | 3383.00] loss=2.73 avg=2.67\n",
            "[6786 | 3383.49] loss=2.61 avg=2.67\n",
            "[6787 | 3383.98] loss=2.40 avg=2.67\n",
            "[6788 | 3384.46] loss=3.08 avg=2.67\n",
            "[6789 | 3384.95] loss=2.65 avg=2.67\n",
            "[6790 | 3385.44] loss=2.86 avg=2.67\n",
            "[6791 | 3385.93] loss=2.97 avg=2.67\n",
            "[6792 | 3386.41] loss=2.77 avg=2.68\n",
            "[6793 | 3386.90] loss=2.48 avg=2.67\n",
            "[6794 | 3387.39] loss=2.60 avg=2.67\n",
            "[6795 | 3387.88] loss=2.66 avg=2.67\n",
            "[6796 | 3388.36] loss=2.55 avg=2.67\n",
            "[6797 | 3388.85] loss=2.47 avg=2.67\n",
            "[6798 | 3389.34] loss=1.83 avg=2.66\n",
            "[6799 | 3389.83] loss=2.77 avg=2.66\n",
            "[6800 | 3390.31] loss=2.59 avg=2.66\n",
            "[6801 | 3390.80] loss=2.69 avg=2.66\n",
            "[6802 | 3391.29] loss=2.87 avg=2.66\n",
            "[6803 | 3391.78] loss=2.58 avg=2.66\n",
            "[6804 | 3392.27] loss=2.68 avg=2.66\n",
            "[6805 | 3392.75] loss=2.85 avg=2.66\n",
            "[6806 | 3393.24] loss=2.73 avg=2.67\n",
            "[6807 | 3393.73] loss=2.70 avg=2.67\n",
            "[6808 | 3394.21] loss=2.78 avg=2.67\n",
            "[6809 | 3394.70] loss=2.47 avg=2.66\n",
            "[6810 | 3395.19] loss=2.54 avg=2.66\n",
            "[6811 | 3395.68] loss=2.55 avg=2.66\n",
            "[6812 | 3396.16] loss=2.63 avg=2.66\n",
            "[6813 | 3396.65] loss=2.63 avg=2.66\n",
            "[6814 | 3397.14] loss=2.44 avg=2.66\n",
            "[6815 | 3397.63] loss=2.55 avg=2.66\n",
            "[6816 | 3398.11] loss=2.31 avg=2.65\n",
            "[6817 | 3398.60] loss=2.87 avg=2.66\n",
            "[6818 | 3399.09] loss=2.82 avg=2.66\n",
            "[6819 | 3399.58] loss=2.63 avg=2.66\n",
            "[6820 | 3400.07] loss=2.82 avg=2.66\n",
            "[6821 | 3400.56] loss=2.69 avg=2.66\n",
            "[6822 | 3401.04] loss=2.40 avg=2.66\n",
            "[6823 | 3401.53] loss=2.73 avg=2.66\n",
            "[6824 | 3402.02] loss=2.70 avg=2.66\n",
            "[6825 | 3402.51] loss=2.84 avg=2.66\n",
            "[6826 | 3403.00] loss=2.85 avg=2.66\n",
            "[6827 | 3403.48] loss=2.67 avg=2.66\n",
            "[6828 | 3403.97] loss=2.51 avg=2.66\n",
            "[6829 | 3404.46] loss=1.81 avg=2.65\n",
            "[6830 | 3404.94] loss=2.72 avg=2.65\n",
            "[6831 | 3405.43] loss=3.00 avg=2.66\n",
            "[6832 | 3405.92] loss=2.89 avg=2.66\n",
            "[6833 | 3406.41] loss=2.84 avg=2.66\n",
            "[6834 | 3406.90] loss=2.56 avg=2.66\n",
            "[6835 | 3407.39] loss=2.65 avg=2.66\n",
            "[6836 | 3407.87] loss=2.64 avg=2.66\n",
            "[6837 | 3408.36] loss=2.30 avg=2.66\n",
            "[6838 | 3408.84] loss=2.85 avg=2.66\n",
            "[6839 | 3409.33] loss=2.36 avg=2.65\n",
            "[6840 | 3409.82] loss=2.91 avg=2.66\n",
            "[6841 | 3410.31] loss=2.63 avg=2.66\n",
            "[6842 | 3410.80] loss=2.73 avg=2.66\n",
            "[6843 | 3411.28] loss=2.44 avg=2.66\n",
            "[6844 | 3411.77] loss=2.81 avg=2.66\n",
            "[6845 | 3412.26] loss=2.84 avg=2.66\n",
            "[6846 | 3412.74] loss=2.82 avg=2.66\n",
            "[6847 | 3413.23] loss=2.56 avg=2.66\n",
            "[6848 | 3413.72] loss=2.84 avg=2.66\n",
            "[6849 | 3414.20] loss=1.84 avg=2.65\n",
            "[6850 | 3414.69] loss=2.60 avg=2.65\n",
            "[6851 | 3415.18] loss=2.86 avg=2.65\n",
            "[6852 | 3415.67] loss=1.65 avg=2.64\n",
            "[6853 | 3416.15] loss=2.25 avg=2.64\n",
            "[6854 | 3416.64] loss=2.59 avg=2.64\n",
            "[6855 | 3417.13] loss=2.64 avg=2.64\n",
            "[6856 | 3417.62] loss=2.76 avg=2.64\n",
            "[6857 | 3418.11] loss=2.52 avg=2.64\n",
            "[6858 | 3418.59] loss=2.60 avg=2.64\n",
            "[6859 | 3419.08] loss=2.56 avg=2.64\n",
            "[6860 | 3419.56] loss=2.96 avg=2.64\n",
            "[6861 | 3420.05] loss=2.63 avg=2.64\n",
            "[6862 | 3420.54] loss=2.92 avg=2.65\n",
            "[6863 | 3421.03] loss=2.55 avg=2.64\n",
            "[6864 | 3421.51] loss=2.57 avg=2.64\n",
            "[6865 | 3422.00] loss=1.81 avg=2.64\n",
            "[6866 | 3422.49] loss=2.72 avg=2.64\n",
            "[6867 | 3422.98] loss=2.70 avg=2.64\n",
            "[6868 | 3423.46] loss=2.71 avg=2.64\n",
            "[6869 | 3423.95] loss=2.64 avg=2.64\n",
            "[6870 | 3424.44] loss=2.94 avg=2.64\n",
            "[6871 | 3424.93] loss=2.51 avg=2.64\n",
            "[6872 | 3425.41] loss=2.57 avg=2.64\n",
            "[6873 | 3425.90] loss=2.81 avg=2.64\n",
            "[6874 | 3426.39] loss=3.13 avg=2.64\n",
            "[6875 | 3426.87] loss=2.89 avg=2.65\n",
            "[6876 | 3427.36] loss=2.97 avg=2.65\n",
            "[6877 | 3427.85] loss=2.48 avg=2.65\n",
            "[6878 | 3428.34] loss=2.83 avg=2.65\n",
            "[6879 | 3428.82] loss=2.88 avg=2.65\n",
            "[6880 | 3429.31] loss=2.54 avg=2.65\n",
            "[6881 | 3429.80] loss=2.77 avg=2.65\n",
            "[6882 | 3430.29] loss=2.68 avg=2.65\n",
            "[6883 | 3430.77] loss=2.51 avg=2.65\n",
            "[6884 | 3431.26] loss=2.60 avg=2.65\n",
            "[6885 | 3431.75] loss=2.48 avg=2.65\n",
            "[6886 | 3432.23] loss=2.85 avg=2.65\n",
            "[6887 | 3432.72] loss=2.88 avg=2.65\n",
            "[6888 | 3433.21] loss=2.49 avg=2.65\n",
            "[6889 | 3433.70] loss=2.50 avg=2.65\n",
            "[6890 | 3434.19] loss=2.53 avg=2.65\n",
            "[6891 | 3434.67] loss=2.55 avg=2.65\n",
            "[6892 | 3435.16] loss=3.03 avg=2.65\n",
            "[6893 | 3435.65] loss=2.38 avg=2.65\n",
            "[6894 | 3436.14] loss=2.87 avg=2.65\n",
            "[6895 | 3436.62] loss=3.02 avg=2.66\n",
            "[6896 | 3437.11] loss=2.66 avg=2.66\n",
            "[6897 | 3437.60] loss=3.03 avg=2.66\n",
            "[6898 | 3438.09] loss=2.71 avg=2.66\n",
            "[6899 | 3438.57] loss=2.72 avg=2.66\n",
            "[6900 | 3439.06] loss=2.86 avg=2.66\n",
            "[6901 | 3439.55] loss=2.55 avg=2.66\n",
            "[6902 | 3440.03] loss=2.60 avg=2.66\n",
            "[6903 | 3440.52] loss=2.73 avg=2.66\n",
            "[6904 | 3441.01] loss=2.86 avg=2.66\n",
            "[6905 | 3441.49] loss=2.78 avg=2.66\n",
            "[6906 | 3441.98] loss=2.71 avg=2.67\n",
            "[6907 | 3442.47] loss=2.65 avg=2.66\n",
            "[6908 | 3442.96] loss=3.08 avg=2.67\n",
            "[6909 | 3443.44] loss=2.54 avg=2.67\n",
            "[6910 | 3443.93] loss=2.44 avg=2.67\n",
            "[6911 | 3444.41] loss=2.53 avg=2.66\n",
            "[6912 | 3444.90] loss=2.26 avg=2.66\n",
            "[6913 | 3445.39] loss=2.51 avg=2.66\n",
            "[6914 | 3445.88] loss=2.50 avg=2.66\n",
            "[6915 | 3446.36] loss=2.57 avg=2.66\n",
            "[6916 | 3446.85] loss=2.45 avg=2.65\n",
            "[6917 | 3447.34] loss=2.48 avg=2.65\n",
            "[6918 | 3447.83] loss=2.74 avg=2.65\n",
            "[6919 | 3448.31] loss=3.20 avg=2.66\n",
            "[6920 | 3448.80] loss=2.34 avg=2.66\n",
            "[6921 | 3449.29] loss=2.37 avg=2.65\n",
            "[6922 | 3449.77] loss=2.73 avg=2.65\n",
            "[6923 | 3450.26] loss=2.59 avg=2.65\n",
            "[6924 | 3450.75] loss=2.66 avg=2.65\n",
            "[6925 | 3451.24] loss=2.45 avg=2.65\n",
            "[6926 | 3451.72] loss=2.62 avg=2.65\n",
            "[6927 | 3452.21] loss=2.27 avg=2.65\n",
            "[6928 | 3452.70] loss=2.76 avg=2.65\n",
            "[6929 | 3453.19] loss=2.87 avg=2.65\n",
            "[6930 | 3453.67] loss=2.42 avg=2.65\n",
            "[6931 | 3454.16] loss=2.56 avg=2.65\n",
            "[6932 | 3454.65] loss=2.45 avg=2.64\n",
            "[6933 | 3455.14] loss=2.50 avg=2.64\n",
            "[6934 | 3455.62] loss=2.63 avg=2.64\n",
            "[6935 | 3456.11] loss=2.83 avg=2.65\n",
            "[6936 | 3456.60] loss=2.69 avg=2.65\n",
            "[6937 | 3457.08] loss=2.79 avg=2.65\n",
            "[6938 | 3457.57] loss=2.81 avg=2.65\n",
            "[6939 | 3458.06] loss=2.84 avg=2.65\n",
            "[6940 | 3458.55] loss=2.62 avg=2.65\n",
            "[6941 | 3459.03] loss=2.98 avg=2.65\n",
            "[6942 | 3459.52] loss=2.59 avg=2.65\n",
            "[6943 | 3460.01] loss=2.76 avg=2.65\n",
            "[6944 | 3460.50] loss=2.69 avg=2.65\n",
            "[6945 | 3460.99] loss=2.86 avg=2.66\n",
            "[6946 | 3461.48] loss=2.74 avg=2.66\n",
            "[6947 | 3461.96] loss=3.04 avg=2.66\n",
            "[6948 | 3462.45] loss=2.80 avg=2.66\n",
            "[6949 | 3462.94] loss=2.75 avg=2.66\n",
            "[6950 | 3463.43] loss=2.98 avg=2.67\n",
            "[6951 | 3463.92] loss=2.86 avg=2.67\n",
            "[6952 | 3464.40] loss=2.60 avg=2.67\n",
            "[6953 | 3464.89] loss=2.81 avg=2.67\n",
            "[6954 | 3465.38] loss=2.77 avg=2.67\n",
            "[6955 | 3465.87] loss=2.70 avg=2.67\n",
            "[6956 | 3466.36] loss=2.53 avg=2.67\n",
            "[6957 | 3466.85] loss=2.98 avg=2.67\n",
            "[6958 | 3467.33] loss=2.53 avg=2.67\n",
            "[6959 | 3467.82] loss=2.51 avg=2.67\n",
            "[6960 | 3468.31] loss=3.07 avg=2.67\n",
            "[6961 | 3468.79] loss=2.58 avg=2.67\n",
            "[6962 | 3469.28] loss=2.77 avg=2.67\n",
            "[6963 | 3469.77] loss=2.83 avg=2.67\n",
            "[6964 | 3470.26] loss=2.76 avg=2.68\n",
            "[6965 | 3470.75] loss=2.43 avg=2.67\n",
            "[6966 | 3471.24] loss=2.49 avg=2.67\n",
            "[6967 | 3471.72] loss=2.98 avg=2.67\n",
            "[6968 | 3472.21] loss=2.67 avg=2.67\n",
            "[6969 | 3472.70] loss=2.57 avg=2.67\n",
            "[6970 | 3473.19] loss=2.65 avg=2.67\n",
            "[6971 | 3473.67] loss=2.39 avg=2.67\n",
            "[6972 | 3474.16] loss=2.95 avg=2.67\n",
            "[6973 | 3474.65] loss=2.37 avg=2.67\n",
            "[6974 | 3475.14] loss=2.60 avg=2.67\n",
            "[6975 | 3475.62] loss=2.84 avg=2.67\n",
            "[6976 | 3476.11] loss=3.26 avg=2.68\n",
            "[6977 | 3476.60] loss=2.73 avg=2.68\n",
            "[6978 | 3477.09] loss=3.06 avg=2.68\n",
            "[6979 | 3477.57] loss=2.31 avg=2.68\n",
            "[6980 | 3478.06] loss=2.92 avg=2.68\n",
            "[6981 | 3478.55] loss=2.41 avg=2.68\n",
            "[6982 | 3479.03] loss=2.34 avg=2.67\n",
            "[6983 | 3479.52] loss=2.53 avg=2.67\n",
            "[6984 | 3480.01] loss=2.60 avg=2.67\n",
            "[6985 | 3480.49] loss=2.37 avg=2.67\n",
            "[6986 | 3480.98] loss=2.66 avg=2.67\n",
            "[6987 | 3481.47] loss=2.76 avg=2.67\n",
            "[6988 | 3481.95] loss=2.74 avg=2.67\n",
            "[6989 | 3482.44] loss=2.71 avg=2.67\n",
            "[6990 | 3482.93] loss=3.03 avg=2.67\n",
            "[6991 | 3483.42] loss=2.75 avg=2.68\n",
            "[6992 | 3483.90] loss=2.25 avg=2.67\n",
            "[6993 | 3484.39] loss=2.75 avg=2.67\n",
            "[6994 | 3484.88] loss=2.69 avg=2.67\n",
            "[6995 | 3485.37] loss=2.91 avg=2.67\n",
            "[6996 | 3485.85] loss=2.76 avg=2.68\n",
            "[6997 | 3486.34] loss=2.65 avg=2.67\n",
            "[6998 | 3486.83] loss=2.68 avg=2.67\n",
            "[6999 | 3487.31] loss=2.58 avg=2.67\n",
            "Saving checkpoint/run1/model-7000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " “I am not a knight,” he said. “I am a knight of the Kingsguard.” \n",
            "“I am not a knight of the Kingsguard,” said Ser Kevan. “I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the Vale of Arryn.” \n",
            "“I am a knight of the\n",
            "\n",
            "[7000 | 3500.52] loss=2.74 avg=2.67\n",
            "[7001 | 3501.00] loss=2.40 avg=2.67\n",
            "[7002 | 3501.49] loss=2.99 avg=2.68\n",
            "[7003 | 3501.98] loss=2.55 avg=2.67\n",
            "[7004 | 3502.47] loss=2.32 avg=2.67\n",
            "[7005 | 3502.95] loss=2.86 avg=2.67\n",
            "[7006 | 3503.44] loss=2.51 avg=2.67\n",
            "[7007 | 3503.93] loss=2.73 avg=2.67\n",
            "[7008 | 3504.41] loss=2.76 avg=2.67\n",
            "[7009 | 3504.90] loss=3.20 avg=2.68\n",
            "[7010 | 3505.39] loss=2.81 avg=2.68\n",
            "[7011 | 3505.88] loss=2.98 avg=2.68\n",
            "[7012 | 3506.36] loss=2.60 avg=2.68\n",
            "[7013 | 3506.85] loss=2.71 avg=2.68\n",
            "[7014 | 3507.34] loss=2.37 avg=2.68\n",
            "[7015 | 3507.83] loss=2.39 avg=2.68\n",
            "[7016 | 3508.31] loss=2.57 avg=2.67\n",
            "[7017 | 3508.80] loss=2.34 avg=2.67\n",
            "[7018 | 3509.29] loss=2.52 avg=2.67\n",
            "[7019 | 3509.78] loss=2.67 avg=2.67\n",
            "[7020 | 3510.26] loss=2.81 avg=2.67\n",
            "[7021 | 3510.75] loss=3.07 avg=2.67\n",
            "[7022 | 3511.24] loss=2.84 avg=2.68\n",
            "[7023 | 3511.73] loss=2.87 avg=2.68\n",
            "[7024 | 3512.21] loss=2.83 avg=2.68\n",
            "[7025 | 3512.70] loss=2.38 avg=2.68\n",
            "[7026 | 3513.19] loss=2.98 avg=2.68\n",
            "[7027 | 3513.68] loss=2.75 avg=2.68\n",
            "[7028 | 3514.16] loss=2.83 avg=2.68\n",
            "[7029 | 3514.65] loss=2.93 avg=2.68\n",
            "[7030 | 3515.14] loss=2.97 avg=2.69\n",
            "[7031 | 3515.63] loss=2.68 avg=2.69\n",
            "[7032 | 3516.11] loss=2.81 avg=2.69\n",
            "[7033 | 3516.60] loss=2.94 avg=2.69\n",
            "[7034 | 3517.09] loss=2.81 avg=2.69\n",
            "[7035 | 3517.58] loss=2.71 avg=2.69\n",
            "[7036 | 3518.07] loss=2.84 avg=2.69\n",
            "[7037 | 3518.55] loss=2.45 avg=2.69\n",
            "[7038 | 3519.04] loss=2.47 avg=2.69\n",
            "[7039 | 3519.53] loss=2.65 avg=2.69\n",
            "[7040 | 3520.03] loss=2.74 avg=2.69\n",
            "[7041 | 3520.52] loss=2.85 avg=2.69\n",
            "[7042 | 3521.01] loss=2.91 avg=2.69\n",
            "[7043 | 3521.49] loss=2.61 avg=2.69\n",
            "[7044 | 3521.98] loss=2.78 avg=2.69\n",
            "[7045 | 3522.47] loss=2.17 avg=2.69\n",
            "[7046 | 3522.96] loss=2.48 avg=2.69\n",
            "[7047 | 3523.44] loss=2.69 avg=2.69\n",
            "[7048 | 3523.93] loss=2.83 avg=2.69\n",
            "[7049 | 3524.42] loss=2.42 avg=2.68\n",
            "[7050 | 3524.90] loss=2.75 avg=2.69\n",
            "[7051 | 3525.39] loss=2.92 avg=2.69\n",
            "[7052 | 3525.88] loss=2.55 avg=2.69\n",
            "[7053 | 3526.37] loss=2.71 avg=2.69\n",
            "[7054 | 3526.86] loss=2.74 avg=2.69\n",
            "[7055 | 3527.34] loss=2.59 avg=2.69\n",
            "[7056 | 3527.83] loss=2.94 avg=2.69\n",
            "[7057 | 3528.32] loss=2.53 avg=2.69\n",
            "[7058 | 3528.81] loss=3.06 avg=2.69\n",
            "[7059 | 3529.30] loss=2.83 avg=2.69\n",
            "[7060 | 3529.79] loss=2.85 avg=2.69\n",
            "[7061 | 3530.27] loss=2.68 avg=2.69\n",
            "[7062 | 3530.76] loss=2.75 avg=2.69\n",
            "[7063 | 3531.25] loss=2.59 avg=2.69\n",
            "[7064 | 3531.74] loss=2.55 avg=2.69\n",
            "[7065 | 3532.23] loss=2.31 avg=2.69\n",
            "[7066 | 3532.71] loss=2.52 avg=2.69\n",
            "[7067 | 3533.21] loss=2.91 avg=2.69\n",
            "[7068 | 3533.69] loss=2.69 avg=2.69\n",
            "[7069 | 3534.18] loss=2.64 avg=2.69\n",
            "[7070 | 3534.66] loss=2.96 avg=2.69\n",
            "[7071 | 3535.15] loss=2.90 avg=2.69\n",
            "[7072 | 3535.64] loss=1.54 avg=2.68\n",
            "[7073 | 3536.13] loss=2.09 avg=2.68\n",
            "[7074 | 3536.61] loss=2.47 avg=2.67\n",
            "[7075 | 3537.10] loss=2.61 avg=2.67\n",
            "[7076 | 3537.59] loss=2.88 avg=2.67\n",
            "[7077 | 3538.08] loss=2.61 avg=2.67\n",
            "[7078 | 3538.57] loss=2.17 avg=2.67\n",
            "[7079 | 3539.06] loss=2.56 avg=2.67\n",
            "[7080 | 3539.54] loss=2.52 avg=2.67\n",
            "[7081 | 3540.03] loss=2.61 avg=2.67\n",
            "[7082 | 3540.52] loss=2.50 avg=2.66\n",
            "[7083 | 3541.00] loss=3.05 avg=2.67\n",
            "[7084 | 3541.49] loss=2.89 avg=2.67\n",
            "[7085 | 3541.98] loss=2.47 avg=2.67\n",
            "[7086 | 3542.47] loss=2.82 avg=2.67\n",
            "[7087 | 3542.95] loss=2.86 avg=2.67\n",
            "[7088 | 3543.44] loss=2.75 avg=2.67\n",
            "[7089 | 3543.93] loss=2.78 avg=2.67\n",
            "[7090 | 3544.42] loss=2.98 avg=2.68\n",
            "[7091 | 3544.90] loss=2.73 avg=2.68\n",
            "[7092 | 3545.39] loss=2.73 avg=2.68\n",
            "[7093 | 3545.88] loss=2.62 avg=2.68\n",
            "[7094 | 3546.37] loss=2.69 avg=2.68\n",
            "[7095 | 3546.85] loss=2.41 avg=2.67\n",
            "[7096 | 3547.34] loss=2.84 avg=2.68\n",
            "[7097 | 3547.83] loss=2.77 avg=2.68\n",
            "[7098 | 3548.31] loss=2.91 avg=2.68\n",
            "[7099 | 3548.80] loss=2.58 avg=2.68\n",
            "[7100 | 3549.29] loss=2.59 avg=2.68\n",
            "[7101 | 3549.77] loss=2.75 avg=2.68\n",
            "[7102 | 3550.26] loss=2.72 avg=2.68\n",
            "[7103 | 3550.75] loss=2.49 avg=2.68\n",
            "[7104 | 3551.24] loss=2.74 avg=2.68\n",
            "[7105 | 3551.72] loss=2.58 avg=2.68\n",
            "[7106 | 3552.21] loss=2.70 avg=2.68\n",
            "[7107 | 3552.70] loss=2.62 avg=2.68\n",
            "[7108 | 3553.19] loss=2.57 avg=2.68\n",
            "[7109 | 3553.68] loss=2.41 avg=2.67\n",
            "[7110 | 3554.16] loss=2.59 avg=2.67\n",
            "[7111 | 3554.65] loss=2.27 avg=2.67\n",
            "[7112 | 3555.14] loss=2.69 avg=2.67\n",
            "[7113 | 3555.63] loss=2.63 avg=2.67\n",
            "[7114 | 3556.11] loss=2.97 avg=2.67\n",
            "[7115 | 3556.60] loss=3.02 avg=2.67\n",
            "[7116 | 3557.09] loss=2.86 avg=2.68\n",
            "[7117 | 3557.57] loss=2.58 avg=2.68\n",
            "[7118 | 3558.06] loss=2.66 avg=2.67\n",
            "[7119 | 3558.55] loss=2.50 avg=2.67\n",
            "[7120 | 3559.03] loss=3.05 avg=2.68\n",
            "[7121 | 3559.52] loss=2.74 avg=2.68\n",
            "[7122 | 3560.01] loss=2.92 avg=2.68\n",
            "[7123 | 3560.50] loss=2.78 avg=2.68\n",
            "[7124 | 3560.99] loss=2.45 avg=2.68\n",
            "[7125 | 3561.47] loss=2.82 avg=2.68\n",
            "[7126 | 3561.96] loss=2.37 avg=2.68\n",
            "[7127 | 3562.45] loss=2.90 avg=2.68\n",
            "[7128 | 3562.94] loss=2.65 avg=2.68\n",
            "[7129 | 3563.42] loss=3.10 avg=2.68\n",
            "[7130 | 3563.91] loss=2.31 avg=2.68\n",
            "[7131 | 3564.40] loss=2.64 avg=2.68\n",
            "[7132 | 3564.88] loss=2.56 avg=2.68\n",
            "[7133 | 3565.37] loss=2.43 avg=2.68\n",
            "[7134 | 3565.86] loss=2.88 avg=2.68\n",
            "[7135 | 3566.34] loss=2.66 avg=2.68\n",
            "[7136 | 3566.83] loss=2.85 avg=2.68\n",
            "[7137 | 3567.32] loss=2.51 avg=2.68\n",
            "[7138 | 3567.81] loss=2.75 avg=2.68\n",
            "[7139 | 3568.29] loss=2.60 avg=2.68\n",
            "[7140 | 3568.78] loss=2.59 avg=2.68\n",
            "[7141 | 3569.27] loss=2.61 avg=2.68\n",
            "[7142 | 3569.76] loss=2.55 avg=2.67\n",
            "[7143 | 3570.24] loss=2.44 avg=2.67\n",
            "[7144 | 3570.73] loss=2.62 avg=2.67\n",
            "[7145 | 3571.22] loss=2.58 avg=2.67\n",
            "[7146 | 3571.71] loss=2.54 avg=2.67\n",
            "[7147 | 3572.19] loss=2.68 avg=2.67\n",
            "[7148 | 3572.68] loss=2.52 avg=2.67\n",
            "[7149 | 3573.16] loss=2.70 avg=2.67\n",
            "[7150 | 3573.65] loss=2.77 avg=2.67\n",
            "[7151 | 3574.14] loss=2.53 avg=2.67\n",
            "[7152 | 3574.63] loss=2.35 avg=2.66\n",
            "[7153 | 3575.11] loss=2.67 avg=2.66\n",
            "[7154 | 3575.60] loss=2.38 avg=2.66\n",
            "[7155 | 3576.09] loss=2.78 avg=2.66\n",
            "[7156 | 3576.58] loss=2.85 avg=2.66\n",
            "[7157 | 3577.06] loss=2.50 avg=2.66\n",
            "[7158 | 3577.55] loss=2.55 avg=2.66\n",
            "[7159 | 3578.04] loss=2.60 avg=2.66\n",
            "[7160 | 3578.53] loss=2.88 avg=2.66\n",
            "[7161 | 3579.01] loss=2.58 avg=2.66\n",
            "[7162 | 3579.50] loss=2.90 avg=2.67\n",
            "[7163 | 3579.99] loss=2.54 avg=2.66\n",
            "[7164 | 3580.47] loss=2.48 avg=2.66\n",
            "[7165 | 3580.96] loss=2.56 avg=2.66\n",
            "[7166 | 3581.45] loss=2.37 avg=2.66\n",
            "[7167 | 3581.93] loss=2.33 avg=2.66\n",
            "[7168 | 3582.42] loss=2.58 avg=2.65\n",
            "[7169 | 3582.91] loss=2.76 avg=2.66\n",
            "[7170 | 3583.39] loss=2.84 avg=2.66\n",
            "[7171 | 3583.88] loss=2.98 avg=2.66\n",
            "[7172 | 3584.37] loss=2.87 avg=2.66\n",
            "[7173 | 3584.86] loss=2.86 avg=2.66\n",
            "[7174 | 3585.34] loss=2.75 avg=2.67\n",
            "[7175 | 3585.83] loss=3.06 avg=2.67\n",
            "[7176 | 3586.32] loss=2.91 avg=2.67\n",
            "[7177 | 3586.81] loss=2.81 avg=2.67\n",
            "[7178 | 3587.29] loss=2.71 avg=2.67\n",
            "[7179 | 3587.78] loss=2.42 avg=2.67\n",
            "[7180 | 3588.26] loss=2.68 avg=2.67\n",
            "[7181 | 3588.75] loss=2.63 avg=2.67\n",
            "[7182 | 3589.24] loss=3.34 avg=2.68\n",
            "[7183 | 3589.73] loss=2.75 avg=2.68\n",
            "[7184 | 3590.22] loss=2.81 avg=2.68\n",
            "[7185 | 3590.70] loss=2.48 avg=2.68\n",
            "[7186 | 3591.19] loss=2.62 avg=2.68\n",
            "[7187 | 3591.68] loss=2.59 avg=2.68\n",
            "[7188 | 3592.16] loss=2.88 avg=2.68\n",
            "[7189 | 3592.65] loss=2.81 avg=2.68\n",
            "[7190 | 3593.14] loss=2.84 avg=2.68\n",
            "[7191 | 3593.62] loss=2.54 avg=2.68\n",
            "[7192 | 3594.11] loss=2.71 avg=2.68\n",
            "[7193 | 3594.60] loss=2.76 avg=2.68\n",
            "[7194 | 3595.09] loss=2.96 avg=2.68\n",
            "[7195 | 3595.58] loss=2.68 avg=2.68\n",
            "[7196 | 3596.06] loss=2.77 avg=2.68\n",
            "[7197 | 3596.55] loss=2.65 avg=2.68\n",
            "[7198 | 3597.04] loss=2.74 avg=2.68\n",
            "[7199 | 3597.53] loss=2.70 avg=2.68\n",
            "[7200 | 3598.01] loss=2.71 avg=2.69\n",
            "[7201 | 3598.51] loss=2.90 avg=2.69\n",
            "[7202 | 3598.99] loss=2.86 avg=2.69\n",
            "[7203 | 3599.48] loss=2.91 avg=2.69\n",
            "[7204 | 3599.97] loss=2.62 avg=2.69\n",
            "[7205 | 3600.46] loss=2.52 avg=2.69\n",
            "[7206 | 3600.94] loss=2.47 avg=2.69\n",
            "[7207 | 3601.43] loss=2.63 avg=2.69\n",
            "[7208 | 3601.92] loss=2.89 avg=2.69\n",
            "[7209 | 3602.41] loss=2.78 avg=2.69\n",
            "[7210 | 3602.89] loss=2.74 avg=2.69\n",
            "[7211 | 3603.38] loss=2.78 avg=2.69\n",
            "[7212 | 3603.86] loss=2.70 avg=2.69\n",
            "[7213 | 3604.35] loss=2.80 avg=2.69\n",
            "[7214 | 3604.83] loss=2.77 avg=2.69\n",
            "[7215 | 3605.32] loss=2.42 avg=2.69\n",
            "[7216 | 3605.81] loss=2.52 avg=2.69\n",
            "[7217 | 3606.30] loss=2.23 avg=2.68\n",
            "[7218 | 3606.78] loss=2.33 avg=2.68\n",
            "[7219 | 3607.27] loss=2.74 avg=2.68\n",
            "[7220 | 3607.76] loss=2.86 avg=2.68\n",
            "[7221 | 3608.25] loss=2.72 avg=2.68\n",
            "[7222 | 3608.73] loss=2.74 avg=2.68\n",
            "[7223 | 3609.22] loss=2.41 avg=2.68\n",
            "[7224 | 3609.71] loss=2.66 avg=2.68\n",
            "[7225 | 3610.20] loss=2.89 avg=2.68\n",
            "[7226 | 3610.68] loss=2.78 avg=2.68\n",
            "[7227 | 3611.17] loss=2.77 avg=2.68\n",
            "[7228 | 3611.66] loss=2.74 avg=2.68\n",
            "[7229 | 3612.15] loss=1.49 avg=2.67\n",
            "[7230 | 3612.63] loss=2.54 avg=2.67\n",
            "[7231 | 3613.12] loss=2.55 avg=2.67\n",
            "[7232 | 3613.61] loss=2.91 avg=2.67\n",
            "[7233 | 3614.10] loss=2.24 avg=2.67\n",
            "[7234 | 3614.58] loss=2.49 avg=2.67\n",
            "[7235 | 3615.07] loss=2.98 avg=2.67\n",
            "[7236 | 3615.56] loss=2.29 avg=2.67\n",
            "[7237 | 3616.05] loss=2.63 avg=2.67\n",
            "[7238 | 3616.53] loss=2.50 avg=2.66\n",
            "[7239 | 3617.02] loss=2.86 avg=2.67\n",
            "[7240 | 3617.51] loss=2.95 avg=2.67\n",
            "[7241 | 3617.99] loss=2.73 avg=2.67\n",
            "[7242 | 3618.48] loss=2.91 avg=2.67\n",
            "[7243 | 3618.97] loss=2.65 avg=2.67\n",
            "[7244 | 3619.46] loss=2.39 avg=2.67\n",
            "[7245 | 3619.94] loss=2.76 avg=2.67\n",
            "[7246 | 3620.43] loss=2.43 avg=2.67\n",
            "[7247 | 3620.92] loss=2.37 avg=2.66\n",
            "[7248 | 3621.41] loss=2.59 avg=2.66\n",
            "[7249 | 3621.89] loss=2.60 avg=2.66\n",
            "[7250 | 3622.38] loss=2.80 avg=2.66\n",
            "[7251 | 3622.87] loss=2.85 avg=2.67\n",
            "[7252 | 3623.35] loss=2.41 avg=2.66\n",
            "[7253 | 3623.84] loss=2.38 avg=2.66\n",
            "[7254 | 3624.33] loss=1.84 avg=2.65\n",
            "[7255 | 3624.81] loss=2.77 avg=2.65\n",
            "[7256 | 3625.30] loss=2.58 avg=2.65\n",
            "[7257 | 3625.79] loss=2.63 avg=2.65\n",
            "[7258 | 3626.28] loss=2.74 avg=2.65\n",
            "[7259 | 3626.76] loss=2.42 avg=2.65\n",
            "[7260 | 3627.25] loss=3.13 avg=2.66\n",
            "[7261 | 3627.74] loss=2.40 avg=2.65\n",
            "[7262 | 3628.22] loss=2.39 avg=2.65\n",
            "[7263 | 3628.71] loss=2.50 avg=2.65\n",
            "[7264 | 3629.20] loss=2.53 avg=2.65\n",
            "[7265 | 3629.68] loss=2.65 avg=2.65\n",
            "[7266 | 3630.17] loss=2.68 avg=2.65\n",
            "[7267 | 3630.66] loss=2.43 avg=2.65\n",
            "[7268 | 3631.15] loss=2.56 avg=2.65\n",
            "[7269 | 3631.63] loss=2.54 avg=2.64\n",
            "[7270 | 3632.12] loss=2.78 avg=2.65\n",
            "[7271 | 3632.61] loss=2.46 avg=2.64\n",
            "[7272 | 3633.10] loss=2.50 avg=2.64\n",
            "[7273 | 3633.58] loss=2.70 avg=2.64\n",
            "[7274 | 3634.07] loss=2.95 avg=2.65\n",
            "[7275 | 3634.56] loss=2.65 avg=2.65\n",
            "[7276 | 3635.04] loss=2.71 avg=2.65\n",
            "[7277 | 3635.53] loss=2.81 avg=2.65\n",
            "[7278 | 3636.02] loss=2.42 avg=2.65\n",
            "[7279 | 3636.51] loss=2.89 avg=2.65\n",
            "[7280 | 3636.99] loss=2.77 avg=2.65\n",
            "[7281 | 3637.48] loss=2.73 avg=2.65\n",
            "[7282 | 3637.97] loss=2.72 avg=2.65\n",
            "[7283 | 3638.46] loss=2.34 avg=2.65\n",
            "[7284 | 3638.94] loss=2.61 avg=2.65\n",
            "[7285 | 3639.43] loss=2.84 avg=2.65\n",
            "[7286 | 3639.92] loss=2.60 avg=2.65\n",
            "[7287 | 3640.40] loss=1.75 avg=2.64\n",
            "[7288 | 3640.89] loss=3.00 avg=2.64\n",
            "[7289 | 3641.38] loss=2.69 avg=2.64\n",
            "[7290 | 3641.86] loss=2.87 avg=2.65\n",
            "[7291 | 3642.35] loss=2.44 avg=2.64\n",
            "[7292 | 3642.84] loss=2.45 avg=2.64\n",
            "[7293 | 3643.33] loss=2.84 avg=2.64\n",
            "[7294 | 3643.81] loss=2.66 avg=2.64\n",
            "[7295 | 3644.30] loss=2.87 avg=2.65\n",
            "[7296 | 3644.79] loss=3.01 avg=2.65\n",
            "[7297 | 3645.28] loss=2.88 avg=2.65\n",
            "[7298 | 3645.76] loss=2.69 avg=2.65\n",
            "[7299 | 3646.25] loss=2.53 avg=2.65\n",
            "[7300 | 3646.74] loss=2.57 avg=2.65\n",
            "[7301 | 3647.23] loss=2.55 avg=2.65\n",
            "[7302 | 3647.71] loss=3.02 avg=2.65\n",
            "[7303 | 3648.20] loss=2.34 avg=2.65\n",
            "[7304 | 3648.69] loss=2.79 avg=2.65\n",
            "[7305 | 3649.17] loss=2.41 avg=2.65\n",
            "[7306 | 3649.66] loss=2.02 avg=2.64\n",
            "[7307 | 3650.15] loss=2.46 avg=2.64\n",
            "[7308 | 3650.64] loss=2.91 avg=2.64\n",
            "[7309 | 3651.12] loss=2.47 avg=2.64\n",
            "[7310 | 3651.61] loss=2.73 avg=2.64\n",
            "[7311 | 3652.10] loss=2.90 avg=2.65\n",
            "[7312 | 3652.58] loss=2.71 avg=2.65\n",
            "[7313 | 3653.07] loss=2.84 avg=2.65\n",
            "[7314 | 3653.56] loss=2.28 avg=2.64\n",
            "[7315 | 3654.05] loss=3.04 avg=2.65\n",
            "[7316 | 3654.53] loss=2.67 avg=2.65\n",
            "[7317 | 3655.02] loss=2.34 avg=2.65\n",
            "[7318 | 3655.51] loss=2.76 avg=2.65\n",
            "[7319 | 3655.99] loss=2.58 avg=2.65\n",
            "[7320 | 3656.48] loss=2.44 avg=2.64\n",
            "[7321 | 3656.97] loss=2.87 avg=2.65\n",
            "[7322 | 3657.46] loss=2.14 avg=2.64\n",
            "[7323 | 3657.95] loss=2.91 avg=2.64\n",
            "[7324 | 3658.43] loss=2.44 avg=2.64\n",
            "[7325 | 3658.92] loss=2.68 avg=2.64\n",
            "[7326 | 3659.41] loss=2.67 avg=2.64\n",
            "[7327 | 3659.90] loss=3.10 avg=2.65\n",
            "[7328 | 3660.38] loss=2.63 avg=2.65\n",
            "[7329 | 3660.87] loss=2.67 avg=2.65\n",
            "[7330 | 3661.36] loss=2.72 avg=2.65\n",
            "[7331 | 3661.85] loss=2.67 avg=2.65\n",
            "[7332 | 3662.33] loss=2.50 avg=2.65\n",
            "[7333 | 3662.82] loss=2.64 avg=2.65\n",
            "[7334 | 3663.31] loss=2.86 avg=2.65\n",
            "[7335 | 3663.80] loss=2.66 avg=2.65\n",
            "[7336 | 3664.29] loss=2.63 avg=2.65\n",
            "[7337 | 3664.77] loss=2.62 avg=2.65\n",
            "[7338 | 3665.26] loss=2.72 avg=2.65\n",
            "[7339 | 3665.75] loss=2.87 avg=2.65\n",
            "[7340 | 3666.24] loss=2.80 avg=2.65\n",
            "[7341 | 3666.72] loss=2.49 avg=2.65\n",
            "[7342 | 3667.20] loss=2.84 avg=2.65\n",
            "[7343 | 3667.69] loss=2.80 avg=2.65\n",
            "[7344 | 3668.17] loss=2.58 avg=2.65\n",
            "[7345 | 3668.66] loss=2.41 avg=2.65\n",
            "[7346 | 3669.14] loss=2.12 avg=2.65\n",
            "[7347 | 3669.63] loss=2.86 avg=2.65\n",
            "[7348 | 3670.12] loss=2.44 avg=2.65\n",
            "[7349 | 3670.61] loss=2.41 avg=2.64\n",
            "[7350 | 3671.10] loss=2.48 avg=2.64\n",
            "[7351 | 3671.58] loss=2.53 avg=2.64\n",
            "[7352 | 3672.07] loss=2.54 avg=2.64\n",
            "[7353 | 3672.56] loss=2.71 avg=2.64\n",
            "[7354 | 3673.05] loss=2.56 avg=2.64\n",
            "[7355 | 3673.54] loss=2.61 avg=2.64\n",
            "[7356 | 3674.03] loss=2.83 avg=2.64\n",
            "[7357 | 3674.51] loss=2.41 avg=2.64\n",
            "[7358 | 3675.00] loss=2.65 avg=2.64\n",
            "[7359 | 3675.49] loss=2.93 avg=2.64\n",
            "[7360 | 3675.97] loss=2.74 avg=2.64\n",
            "[7361 | 3676.46] loss=2.39 avg=2.64\n",
            "[7362 | 3676.95] loss=2.57 avg=2.64\n",
            "[7363 | 3677.44] loss=2.88 avg=2.64\n",
            "[7364 | 3677.93] loss=2.66 avg=2.64\n",
            "[7365 | 3678.42] loss=2.79 avg=2.64\n",
            "[7366 | 3678.90] loss=2.54 avg=2.64\n",
            "[7367 | 3679.39] loss=2.72 avg=2.64\n",
            "[7368 | 3679.88] loss=2.69 avg=2.64\n",
            "[7369 | 3680.36] loss=2.62 avg=2.64\n",
            "[7370 | 3680.85] loss=2.84 avg=2.65\n",
            "[7371 | 3681.34] loss=3.03 avg=2.65\n",
            "[7372 | 3681.83] loss=2.52 avg=2.65\n",
            "[7373 | 3682.31] loss=2.31 avg=2.65\n",
            "[7374 | 3682.80] loss=2.53 avg=2.64\n",
            "[7375 | 3683.28] loss=2.49 avg=2.64\n",
            "[7376 | 3683.77] loss=2.53 avg=2.64\n",
            "[7377 | 3684.26] loss=2.73 avg=2.64\n",
            "[7378 | 3684.75] loss=2.60 avg=2.64\n",
            "[7379 | 3685.23] loss=2.17 avg=2.64\n",
            "[7380 | 3685.72] loss=2.81 avg=2.64\n",
            "[7381 | 3686.21] loss=2.47 avg=2.64\n",
            "[7382 | 3686.70] loss=2.60 avg=2.64\n",
            "[7383 | 3687.19] loss=2.81 avg=2.64\n",
            "[7384 | 3687.67] loss=2.81 avg=2.64\n",
            "[7385 | 3688.16] loss=2.93 avg=2.64\n",
            "[7386 | 3688.65] loss=2.88 avg=2.65\n",
            "[7387 | 3689.14] loss=2.78 avg=2.65\n",
            "[7388 | 3689.62] loss=3.07 avg=2.65\n",
            "[7389 | 3690.11] loss=3.02 avg=2.65\n",
            "[7390 | 3690.59] loss=2.61 avg=2.65\n",
            "[7391 | 3691.08] loss=2.49 avg=2.65\n",
            "[7392 | 3691.57] loss=2.61 avg=2.65\n",
            "[7393 | 3692.06] loss=2.58 avg=2.65\n",
            "[7394 | 3692.54] loss=2.45 avg=2.65\n",
            "[7395 | 3693.04] loss=2.45 avg=2.65\n",
            "[7396 | 3693.52] loss=2.76 avg=2.65\n",
            "[7397 | 3694.01] loss=2.85 avg=2.65\n",
            "[7398 | 3694.49] loss=3.01 avg=2.65\n",
            "[7399 | 3694.98] loss=2.98 avg=2.66\n",
            "[7400 | 3695.47] loss=2.29 avg=2.65\n",
            "[7401 | 3695.96] loss=2.98 avg=2.66\n",
            "[7402 | 3696.44] loss=2.93 avg=2.66\n",
            "[7403 | 3696.93] loss=3.28 avg=2.67\n",
            "[7404 | 3697.42] loss=2.61 avg=2.67\n",
            "[7405 | 3697.91] loss=2.49 avg=2.66\n",
            "[7406 | 3698.39] loss=2.44 avg=2.66\n",
            "[7407 | 3698.88] loss=2.30 avg=2.66\n",
            "[7408 | 3699.37] loss=2.64 avg=2.66\n",
            "[7409 | 3699.86] loss=2.77 avg=2.66\n",
            "[7410 | 3700.34] loss=2.68 avg=2.66\n",
            "[7411 | 3700.83] loss=2.68 avg=2.66\n",
            "[7412 | 3701.32] loss=2.48 avg=2.66\n",
            "[7413 | 3701.81] loss=2.48 avg=2.66\n",
            "[7414 | 3702.29] loss=2.64 avg=2.66\n",
            "[7415 | 3702.78] loss=2.82 avg=2.66\n",
            "[7416 | 3703.27] loss=2.59 avg=2.66\n",
            "[7417 | 3703.76] loss=2.48 avg=2.65\n",
            "[7418 | 3704.24] loss=2.60 avg=2.65\n",
            "[7419 | 3704.73] loss=2.49 avg=2.65\n",
            "[7420 | 3705.22] loss=2.40 avg=2.65\n",
            "[7421 | 3705.71] loss=2.90 avg=2.65\n",
            "[7422 | 3706.19] loss=2.70 avg=2.65\n",
            "[7423 | 3706.68] loss=2.90 avg=2.66\n",
            "[7424 | 3707.17] loss=2.81 avg=2.66\n",
            "[7425 | 3707.65] loss=2.61 avg=2.66\n",
            "[7426 | 3708.14] loss=2.90 avg=2.66\n",
            "[7427 | 3708.62] loss=1.98 avg=2.65\n",
            "[7428 | 3709.11] loss=2.34 avg=2.65\n",
            "[7429 | 3709.60] loss=2.61 avg=2.65\n",
            "[7430 | 3710.09] loss=2.48 avg=2.65\n",
            "[7431 | 3710.57] loss=2.72 avg=2.65\n",
            "[7432 | 3711.06] loss=2.64 avg=2.65\n",
            "[7433 | 3711.55] loss=2.43 avg=2.65\n",
            "[7434 | 3712.03] loss=2.80 avg=2.65\n",
            "[7435 | 3712.52] loss=2.31 avg=2.64\n",
            "[7436 | 3713.01] loss=2.84 avg=2.65\n",
            "[7437 | 3713.50] loss=2.72 avg=2.65\n",
            "[7438 | 3713.98] loss=2.38 avg=2.64\n",
            "[7439 | 3714.47] loss=2.67 avg=2.64\n",
            "[7440 | 3714.96] loss=2.65 avg=2.64\n",
            "[7441 | 3715.45] loss=2.63 avg=2.64\n",
            "[7442 | 3715.93] loss=2.58 avg=2.64\n",
            "[7443 | 3716.42] loss=2.48 avg=2.64\n",
            "[7444 | 3716.90] loss=2.63 avg=2.64\n",
            "[7445 | 3717.39] loss=2.78 avg=2.64\n",
            "[7446 | 3717.88] loss=2.22 avg=2.64\n",
            "[7447 | 3718.37] loss=2.69 avg=2.64\n",
            "[7448 | 3718.85] loss=2.25 avg=2.64\n",
            "[7449 | 3719.34] loss=2.66 avg=2.64\n",
            "[7450 | 3719.83] loss=2.50 avg=2.63\n",
            "[7451 | 3720.31] loss=2.82 avg=2.64\n",
            "[7452 | 3720.80] loss=2.62 avg=2.64\n",
            "[7453 | 3721.29] loss=2.62 avg=2.64\n",
            "[7454 | 3721.78] loss=2.90 avg=2.64\n",
            "[7455 | 3722.26] loss=2.82 avg=2.64\n",
            "[7456 | 3722.75] loss=2.59 avg=2.64\n",
            "[7457 | 3723.23] loss=2.95 avg=2.64\n",
            "[7458 | 3723.73] loss=2.65 avg=2.64\n",
            "[7459 | 3724.21] loss=2.61 avg=2.64\n",
            "[7460 | 3724.70] loss=2.74 avg=2.64\n",
            "[7461 | 3725.19] loss=2.75 avg=2.64\n",
            "[7462 | 3725.67] loss=2.95 avg=2.65\n",
            "[7463 | 3726.16] loss=2.92 avg=2.65\n",
            "[7464 | 3726.64] loss=2.60 avg=2.65\n",
            "[7465 | 3727.14] loss=2.55 avg=2.65\n",
            "[7466 | 3727.62] loss=2.84 avg=2.65\n",
            "[7467 | 3728.11] loss=2.51 avg=2.65\n",
            "[7468 | 3728.60] loss=2.15 avg=2.64\n",
            "[7469 | 3729.09] loss=2.23 avg=2.64\n",
            "[7470 | 3729.57] loss=2.68 avg=2.64\n",
            "[7471 | 3730.06] loss=2.97 avg=2.64\n",
            "[7472 | 3730.55] loss=2.58 avg=2.64\n",
            "[7473 | 3731.03] loss=2.57 avg=2.64\n",
            "[7474 | 3731.52] loss=2.69 avg=2.64\n",
            "[7475 | 3732.01] loss=2.65 avg=2.64\n",
            "[7476 | 3732.50] loss=2.80 avg=2.64\n",
            "[7477 | 3732.99] loss=2.80 avg=2.65\n",
            "[7478 | 3733.48] loss=2.95 avg=2.65\n",
            "[7479 | 3733.97] loss=2.75 avg=2.65\n",
            "[7480 | 3734.45] loss=2.38 avg=2.65\n",
            "[7481 | 3734.94] loss=2.69 avg=2.65\n",
            "[7482 | 3735.43] loss=2.95 avg=2.65\n",
            "[7483 | 3735.91] loss=2.49 avg=2.65\n",
            "[7484 | 3736.40] loss=2.74 avg=2.65\n",
            "[7485 | 3736.89] loss=1.78 avg=2.64\n",
            "[7486 | 3737.38] loss=2.23 avg=2.64\n",
            "[7487 | 3737.87] loss=2.60 avg=2.64\n",
            "[7488 | 3738.36] loss=2.70 avg=2.64\n",
            "[7489 | 3738.84] loss=2.40 avg=2.64\n",
            "[7490 | 3739.33] loss=2.61 avg=2.64\n",
            "[7491 | 3739.82] loss=2.54 avg=2.63\n",
            "[7492 | 3740.31] loss=2.61 avg=2.63\n",
            "[7493 | 3740.80] loss=2.67 avg=2.63\n",
            "[7494 | 3741.28] loss=2.78 avg=2.64\n",
            "[7495 | 3741.77] loss=1.76 avg=2.63\n",
            "[7496 | 3742.26] loss=2.47 avg=2.63\n",
            "[7497 | 3742.75] loss=2.70 avg=2.63\n",
            "[7498 | 3743.24] loss=2.67 avg=2.63\n",
            "[7499 | 3743.73] loss=2.53 avg=2.63\n",
            "[7500 | 3744.21] loss=2.54 avg=2.62\n",
            "[7501 | 3744.70] loss=2.83 avg=2.63\n",
            "[7502 | 3745.19] loss=2.96 avg=2.63\n",
            "[7503 | 3745.67] loss=2.59 avg=2.63\n",
            "[7504 | 3746.16] loss=2.61 avg=2.63\n",
            "[7505 | 3746.65] loss=2.58 avg=2.63\n",
            "[7506 | 3747.13] loss=2.87 avg=2.63\n",
            "[7507 | 3747.62] loss=2.82 avg=2.63\n",
            "[7508 | 3748.11] loss=2.46 avg=2.63\n",
            "[7509 | 3748.60] loss=2.85 avg=2.63\n",
            "[7510 | 3749.08] loss=2.42 avg=2.63\n",
            "[7511 | 3749.57] loss=2.79 avg=2.63\n",
            "[7512 | 3750.06] loss=2.53 avg=2.63\n",
            "[7513 | 3750.54] loss=2.64 avg=2.63\n",
            "[7514 | 3751.03] loss=2.75 avg=2.63\n",
            "[7515 | 3751.52] loss=2.74 avg=2.63\n",
            "[7516 | 3752.01] loss=2.40 avg=2.63\n",
            "[7517 | 3752.49] loss=2.48 avg=2.63\n",
            "[7518 | 3752.98] loss=2.43 avg=2.63\n",
            "[7519 | 3753.47] loss=2.43 avg=2.63\n",
            "[7520 | 3753.95] loss=2.52 avg=2.63\n",
            "[7521 | 3754.44] loss=2.52 avg=2.62\n",
            "[7522 | 3754.93] loss=2.64 avg=2.62\n",
            "[7523 | 3755.42] loss=2.71 avg=2.63\n",
            "[7524 | 3755.90] loss=2.65 avg=2.63\n",
            "[7525 | 3756.39] loss=2.89 avg=2.63\n",
            "[7526 | 3756.88] loss=2.49 avg=2.63\n",
            "[7527 | 3757.36] loss=2.03 avg=2.62\n",
            "[7528 | 3757.85] loss=2.85 avg=2.62\n",
            "[7529 | 3758.34] loss=2.88 avg=2.63\n",
            "[7530 | 3758.82] loss=2.58 avg=2.63\n",
            "[7531 | 3759.31] loss=2.70 avg=2.63\n",
            "[7532 | 3759.80] loss=2.69 avg=2.63\n",
            "[7533 | 3760.28] loss=2.72 avg=2.63\n",
            "[7534 | 3760.77] loss=2.67 avg=2.63\n",
            "[7535 | 3761.26] loss=2.66 avg=2.63\n",
            "[7536 | 3761.75] loss=2.40 avg=2.63\n",
            "[7537 | 3762.24] loss=1.37 avg=2.61\n",
            "[7538 | 3762.72] loss=2.86 avg=2.62\n",
            "[7539 | 3763.21] loss=2.77 avg=2.62\n",
            "[7540 | 3763.69] loss=2.45 avg=2.62\n",
            "[7541 | 3764.18] loss=2.83 avg=2.62\n",
            "[7542 | 3764.67] loss=2.54 avg=2.62\n",
            "[7543 | 3765.16] loss=2.48 avg=2.62\n",
            "[7544 | 3765.64] loss=2.42 avg=2.61\n",
            "[7545 | 3766.13] loss=2.57 avg=2.61\n",
            "[7546 | 3766.61] loss=2.80 avg=2.62\n",
            "[7547 | 3767.10] loss=2.91 avg=2.62\n",
            "[7548 | 3767.59] loss=2.55 avg=2.62\n",
            "[7549 | 3768.08] loss=2.58 avg=2.62\n",
            "[7550 | 3768.56] loss=2.62 avg=2.62\n",
            "[7551 | 3769.05] loss=2.74 avg=2.62\n",
            "[7552 | 3769.54] loss=2.73 avg=2.62\n",
            "[7553 | 3770.02] loss=2.45 avg=2.62\n",
            "[7554 | 3770.51] loss=2.63 avg=2.62\n",
            "[7555 | 3771.00] loss=2.66 avg=2.62\n",
            "[7556 | 3771.49] loss=1.81 avg=2.61\n",
            "[7557 | 3771.97] loss=2.65 avg=2.61\n",
            "[7558 | 3772.46] loss=2.43 avg=2.61\n",
            "[7559 | 3772.94] loss=2.58 avg=2.61\n",
            "[7560 | 3773.43] loss=2.19 avg=2.60\n",
            "[7561 | 3773.92] loss=2.60 avg=2.60\n",
            "[7562 | 3774.41] loss=2.82 avg=2.61\n",
            "[7563 | 3774.89] loss=1.34 avg=2.59\n",
            "[7564 | 3775.38] loss=2.82 avg=2.60\n",
            "[7565 | 3775.87] loss=2.72 avg=2.60\n",
            "[7566 | 3776.36] loss=2.60 avg=2.60\n",
            "[7567 | 3776.84] loss=2.40 avg=2.60\n",
            "[7568 | 3777.33] loss=2.85 avg=2.60\n",
            "[7569 | 3777.82] loss=2.31 avg=2.60\n",
            "[7570 | 3778.30] loss=2.60 avg=2.60\n",
            "[7571 | 3778.79] loss=2.66 avg=2.60\n",
            "[7572 | 3779.28] loss=2.39 avg=2.59\n",
            "[7573 | 3779.77] loss=2.28 avg=2.59\n",
            "[7574 | 3780.25] loss=2.46 avg=2.59\n",
            "[7575 | 3780.74] loss=2.63 avg=2.59\n",
            "[7576 | 3781.23] loss=2.99 avg=2.59\n",
            "[7577 | 3781.72] loss=2.37 avg=2.59\n",
            "[7578 | 3782.20] loss=2.43 avg=2.59\n",
            "[7579 | 3782.69] loss=2.36 avg=2.59\n",
            "[7580 | 3783.18] loss=2.84 avg=2.59\n",
            "[7581 | 3783.67] loss=2.53 avg=2.59\n",
            "[7582 | 3784.15] loss=2.33 avg=2.59\n",
            "[7583 | 3784.64] loss=2.94 avg=2.59\n",
            "[7584 | 3785.13] loss=2.62 avg=2.59\n",
            "[7585 | 3785.61] loss=2.56 avg=2.59\n",
            "[7586 | 3786.10] loss=2.62 avg=2.59\n",
            "[7587 | 3786.59] loss=2.27 avg=2.59\n",
            "[7588 | 3787.08] loss=2.91 avg=2.59\n",
            "[7589 | 3787.56] loss=2.71 avg=2.59\n",
            "[7590 | 3788.05] loss=2.69 avg=2.59\n",
            "[7591 | 3788.54] loss=2.72 avg=2.59\n",
            "[7592 | 3789.03] loss=2.36 avg=2.59\n",
            "[7593 | 3789.51] loss=2.75 avg=2.59\n",
            "[7594 | 3790.00] loss=2.64 avg=2.59\n",
            "[7595 | 3790.49] loss=2.93 avg=2.60\n",
            "[7596 | 3790.97] loss=2.70 avg=2.60\n",
            "[7597 | 3791.46] loss=2.69 avg=2.60\n",
            "[7598 | 3791.95] loss=2.69 avg=2.60\n",
            "[7599 | 3792.43] loss=2.47 avg=2.60\n",
            "[7600 | 3792.92] loss=2.58 avg=2.60\n",
            "[7601 | 3793.41] loss=2.37 avg=2.60\n",
            "[7602 | 3793.90] loss=2.72 avg=2.60\n",
            "[7603 | 3794.39] loss=2.31 avg=2.59\n",
            "[7604 | 3794.88] loss=2.79 avg=2.60\n",
            "[7605 | 3795.36] loss=2.53 avg=2.60\n",
            "[7606 | 3795.85] loss=2.63 avg=2.60\n",
            "[7607 | 3796.34] loss=2.80 avg=2.60\n",
            "[7608 | 3796.82] loss=2.57 avg=2.60\n",
            "[7609 | 3797.31] loss=2.40 avg=2.60\n",
            "[7610 | 3797.80] loss=2.56 avg=2.60\n",
            "[7611 | 3798.29] loss=2.41 avg=2.59\n",
            "[7612 | 3798.77] loss=2.93 avg=2.60\n",
            "[7613 | 3799.26] loss=2.55 avg=2.60\n",
            "[7614 | 3799.75] loss=2.55 avg=2.60\n",
            "[7615 | 3800.24] loss=2.93 avg=2.60\n",
            "[7616 | 3800.72] loss=2.84 avg=2.60\n",
            "[7617 | 3801.21] loss=2.76 avg=2.60\n",
            "[7618 | 3801.70] loss=2.69 avg=2.60\n",
            "[7619 | 3802.19] loss=3.13 avg=2.61\n",
            "[7620 | 3802.67] loss=2.65 avg=2.61\n",
            "[7621 | 3803.16] loss=2.89 avg=2.61\n",
            "[7622 | 3803.65] loss=2.44 avg=2.61\n",
            "[7623 | 3804.14] loss=2.72 avg=2.61\n",
            "[7624 | 3804.63] loss=2.64 avg=2.61\n",
            "[7625 | 3805.12] loss=2.40 avg=2.61\n",
            "[7626 | 3805.60] loss=2.51 avg=2.61\n",
            "[7627 | 3806.09] loss=2.92 avg=2.61\n",
            "[7628 | 3806.58] loss=2.48 avg=2.61\n",
            "[7629 | 3807.07] loss=2.34 avg=2.61\n",
            "[7630 | 3807.55] loss=2.93 avg=2.61\n",
            "[7631 | 3808.04] loss=2.39 avg=2.61\n",
            "[7632 | 3808.53] loss=2.51 avg=2.61\n",
            "[7633 | 3809.02] loss=2.47 avg=2.61\n",
            "[7634 | 3809.50] loss=2.67 avg=2.61\n",
            "[7635 | 3809.99] loss=2.52 avg=2.61\n",
            "[7636 | 3810.48] loss=2.52 avg=2.61\n",
            "[7637 | 3810.97] loss=2.56 avg=2.61\n",
            "[7638 | 3811.46] loss=2.73 avg=2.61\n",
            "[7639 | 3811.95] loss=2.87 avg=2.61\n",
            "[7640 | 3812.43] loss=2.82 avg=2.61\n",
            "[7641 | 3812.92] loss=2.57 avg=2.61\n",
            "[7642 | 3813.41] loss=2.61 avg=2.61\n",
            "[7643 | 3813.89] loss=2.95 avg=2.61\n",
            "[7644 | 3814.38] loss=2.58 avg=2.61\n",
            "[7645 | 3814.87] loss=2.63 avg=2.61\n",
            "[7646 | 3815.36] loss=2.91 avg=2.62\n",
            "[7647 | 3815.84] loss=2.47 avg=2.62\n",
            "[7648 | 3816.33] loss=2.77 avg=2.62\n",
            "[7649 | 3816.82] loss=2.85 avg=2.62\n",
            "[7650 | 3817.30] loss=2.71 avg=2.62\n",
            "[7651 | 3817.81] loss=2.67 avg=2.62\n",
            "[7652 | 3818.30] loss=2.68 avg=2.62\n",
            "[7653 | 3818.79] loss=1.68 avg=2.61\n",
            "[7654 | 3819.27] loss=2.40 avg=2.61\n",
            "[7655 | 3819.76] loss=2.52 avg=2.61\n",
            "[7656 | 3820.25] loss=2.26 avg=2.61\n",
            "[7657 | 3820.74] loss=2.71 avg=2.61\n",
            "[7658 | 3821.22] loss=2.91 avg=2.61\n",
            "[7659 | 3821.71] loss=2.97 avg=2.61\n",
            "[7660 | 3822.20] loss=2.59 avg=2.61\n",
            "[7661 | 3822.68] loss=2.62 avg=2.61\n",
            "[7662 | 3823.17] loss=2.65 avg=2.61\n",
            "[7663 | 3823.66] loss=2.68 avg=2.61\n",
            "[7664 | 3824.15] loss=2.91 avg=2.62\n",
            "[7665 | 3824.63] loss=2.76 avg=2.62\n",
            "[7666 | 3825.12] loss=2.47 avg=2.62\n",
            "[7667 | 3825.61] loss=2.42 avg=2.62\n",
            "[7668 | 3826.09] loss=2.59 avg=2.62\n",
            "[7669 | 3826.58] loss=2.60 avg=2.61\n",
            "[7670 | 3827.07] loss=2.61 avg=2.61\n",
            "[7671 | 3827.55] loss=2.42 avg=2.61\n",
            "[7672 | 3828.04] loss=2.96 avg=2.62\n",
            "[7673 | 3828.53] loss=2.69 avg=2.62\n",
            "[7674 | 3829.02] loss=2.83 avg=2.62\n",
            "[7675 | 3829.50] loss=2.65 avg=2.62\n",
            "[7676 | 3829.99] loss=2.65 avg=2.62\n",
            "[7677 | 3830.48] loss=2.77 avg=2.62\n",
            "[7678 | 3830.96] loss=3.01 avg=2.63\n",
            "[7679 | 3831.45] loss=2.90 avg=2.63\n",
            "[7680 | 3831.94] loss=2.35 avg=2.63\n",
            "[7681 | 3832.42] loss=2.65 avg=2.63\n",
            "[7682 | 3832.91] loss=2.94 avg=2.63\n",
            "[7683 | 3833.40] loss=2.41 avg=2.63\n",
            "[7684 | 3833.88] loss=2.13 avg=2.62\n",
            "[7685 | 3834.37] loss=2.56 avg=2.62\n",
            "[7686 | 3834.86] loss=2.83 avg=2.62\n",
            "[7687 | 3835.34] loss=2.51 avg=2.62\n",
            "[7688 | 3835.83] loss=2.94 avg=2.63\n",
            "[7689 | 3836.32] loss=2.76 avg=2.63\n",
            "[7690 | 3836.81] loss=2.50 avg=2.63\n",
            "[7691 | 3837.29] loss=2.56 avg=2.62\n",
            "[7692 | 3837.78] loss=2.53 avg=2.62\n",
            "[7693 | 3838.27] loss=2.53 avg=2.62\n",
            "[7694 | 3838.75] loss=2.87 avg=2.63\n",
            "[7695 | 3839.24] loss=2.38 avg=2.62\n",
            "[7696 | 3839.73] loss=2.59 avg=2.62\n",
            "[7697 | 3840.22] loss=2.88 avg=2.62\n",
            "[7698 | 3840.70] loss=2.41 avg=2.62\n",
            "[7699 | 3841.19] loss=2.82 avg=2.62\n",
            "[7700 | 3841.67] loss=2.53 avg=2.62\n",
            "[7701 | 3842.16] loss=1.59 avg=2.61\n",
            "[7702 | 3842.65] loss=2.71 avg=2.61\n",
            "[7703 | 3843.14] loss=2.75 avg=2.62\n",
            "[7704 | 3843.62] loss=3.08 avg=2.62\n",
            "[7705 | 3844.11] loss=2.54 avg=2.62\n",
            "[7706 | 3844.60] loss=2.73 avg=2.62\n",
            "[7707 | 3845.09] loss=2.85 avg=2.62\n",
            "[7708 | 3845.57] loss=2.84 avg=2.63\n",
            "[7709 | 3846.06] loss=2.99 avg=2.63\n",
            "[7710 | 3846.55] loss=2.45 avg=2.63\n",
            "[7711 | 3847.03] loss=2.77 avg=2.63\n",
            "[7712 | 3847.52] loss=2.80 avg=2.63\n",
            "[7713 | 3848.01] loss=2.59 avg=2.63\n",
            "[7714 | 3848.50] loss=2.58 avg=2.63\n",
            "[7715 | 3848.98] loss=2.56 avg=2.63\n",
            "[7716 | 3849.47] loss=2.92 avg=2.63\n",
            "[7717 | 3849.96] loss=2.35 avg=2.63\n",
            "[7718 | 3850.44] loss=2.66 avg=2.63\n",
            "[7719 | 3850.93] loss=2.67 avg=2.63\n",
            "[7720 | 3851.42] loss=2.88 avg=2.63\n",
            "[7721 | 3851.91] loss=2.63 avg=2.63\n",
            "[7722 | 3852.39] loss=2.51 avg=2.63\n",
            "[7723 | 3852.88] loss=2.56 avg=2.63\n",
            "[7724 | 3853.37] loss=2.65 avg=2.63\n",
            "[7725 | 3853.86] loss=2.37 avg=2.63\n",
            "[7726 | 3854.35] loss=2.62 avg=2.63\n",
            "[7727 | 3854.84] loss=2.62 avg=2.63\n",
            "[7728 | 3855.32] loss=2.29 avg=2.62\n",
            "[7729 | 3855.81] loss=2.58 avg=2.62\n",
            "[7730 | 3856.29] loss=2.58 avg=2.62\n",
            "[7731 | 3856.78] loss=2.51 avg=2.62\n",
            "[7732 | 3857.27] loss=2.47 avg=2.62\n",
            "[7733 | 3857.75] loss=2.17 avg=2.62\n",
            "[7734 | 3858.24] loss=2.41 avg=2.61\n",
            "[7735 | 3858.73] loss=2.60 avg=2.61\n",
            "[7736 | 3859.22] loss=2.97 avg=2.62\n",
            "[7737 | 3859.71] loss=3.00 avg=2.62\n",
            "[7738 | 3860.19] loss=2.83 avg=2.62\n",
            "[7739 | 3860.68] loss=2.79 avg=2.62\n",
            "[7740 | 3861.17] loss=2.37 avg=2.62\n",
            "[7741 | 3861.65] loss=2.53 avg=2.62\n",
            "[7742 | 3862.14] loss=2.37 avg=2.62\n",
            "[7743 | 3862.63] loss=2.37 avg=2.62\n",
            "[7744 | 3863.12] loss=2.66 avg=2.62\n",
            "[7745 | 3863.61] loss=2.77 avg=2.62\n",
            "[7746 | 3864.10] loss=2.50 avg=2.62\n",
            "[7747 | 3864.58] loss=3.12 avg=2.62\n",
            "[7748 | 3865.07] loss=2.74 avg=2.62\n",
            "[7749 | 3865.56] loss=2.74 avg=2.62\n",
            "[7750 | 3866.04] loss=2.73 avg=2.63\n",
            "[7751 | 3866.53] loss=2.83 avg=2.63\n",
            "[7752 | 3867.02] loss=2.76 avg=2.63\n",
            "[7753 | 3867.51] loss=2.68 avg=2.63\n",
            "[7754 | 3867.99] loss=2.62 avg=2.63\n",
            "[7755 | 3868.47] loss=2.58 avg=2.63\n",
            "[7756 | 3868.96] loss=2.89 avg=2.63\n",
            "[7757 | 3869.44] loss=2.72 avg=2.63\n",
            "[7758 | 3869.93] loss=2.39 avg=2.63\n",
            "[7759 | 3870.42] loss=1.87 avg=2.62\n",
            "[7760 | 3870.90] loss=2.80 avg=2.62\n",
            "[7761 | 3871.39] loss=2.87 avg=2.63\n",
            "[7762 | 3871.88] loss=2.88 avg=2.63\n",
            "[7763 | 3872.37] loss=2.92 avg=2.63\n",
            "[7764 | 3872.85] loss=2.30 avg=2.63\n",
            "[7765 | 3873.34] loss=2.69 avg=2.63\n",
            "[7766 | 3873.83] loss=2.88 avg=2.63\n",
            "[7767 | 3874.32] loss=2.75 avg=2.63\n",
            "[7768 | 3874.81] loss=2.49 avg=2.63\n",
            "[7769 | 3875.29] loss=2.94 avg=2.63\n",
            "[7770 | 3875.78] loss=2.49 avg=2.63\n",
            "[7771 | 3876.27] loss=2.47 avg=2.63\n",
            "[7772 | 3876.76] loss=2.46 avg=2.63\n",
            "[7773 | 3877.25] loss=2.56 avg=2.63\n",
            "[7774 | 3877.73] loss=2.62 avg=2.63\n",
            "[7775 | 3878.22] loss=2.76 avg=2.63\n",
            "[7776 | 3878.71] loss=2.63 avg=2.63\n",
            "[7777 | 3879.20] loss=2.38 avg=2.63\n",
            "[7778 | 3879.69] loss=2.98 avg=2.63\n",
            "[7779 | 3880.17] loss=2.74 avg=2.63\n",
            "[7780 | 3880.66] loss=2.70 avg=2.63\n",
            "[7781 | 3881.15] loss=1.89 avg=2.63\n",
            "[7782 | 3881.64] loss=2.76 avg=2.63\n",
            "[7783 | 3882.13] loss=2.37 avg=2.62\n",
            "[7784 | 3882.61] loss=2.74 avg=2.63\n",
            "[7785 | 3883.10] loss=2.80 avg=2.63\n",
            "[7786 | 3883.59] loss=2.72 avg=2.63\n",
            "[7787 | 3884.07] loss=2.51 avg=2.63\n",
            "[7788 | 3884.56] loss=2.73 avg=2.63\n",
            "[7789 | 3885.05] loss=2.51 avg=2.63\n",
            "[7790 | 3885.54] loss=1.73 avg=2.62\n",
            "[7791 | 3886.03] loss=2.63 avg=2.62\n",
            "[7792 | 3886.51] loss=2.50 avg=2.62\n",
            "[7793 | 3887.00] loss=2.89 avg=2.62\n",
            "[7794 | 3887.49] loss=2.44 avg=2.62\n",
            "[7795 | 3887.98] loss=2.63 avg=2.62\n",
            "[7796 | 3888.46] loss=2.51 avg=2.62\n",
            "[7797 | 3888.95] loss=2.53 avg=2.62\n",
            "[7798 | 3889.44] loss=2.72 avg=2.62\n",
            "[7799 | 3889.93] loss=2.63 avg=2.62\n",
            "[7800 | 3890.41] loss=2.72 avg=2.62\n",
            "[7801 | 3890.90] loss=2.59 avg=2.62\n",
            "[7802 | 3891.39] loss=2.71 avg=2.62\n",
            "[7803 | 3891.87] loss=2.72 avg=2.62\n",
            "[7804 | 3892.36] loss=2.44 avg=2.62\n",
            "[7805 | 3892.85] loss=2.49 avg=2.62\n",
            "[7806 | 3893.34] loss=1.60 avg=2.61\n",
            "[7807 | 3893.82] loss=2.44 avg=2.61\n",
            "[7808 | 3894.31] loss=2.38 avg=2.60\n",
            "[7809 | 3894.79] loss=2.39 avg=2.60\n",
            "[7810 | 3895.28] loss=2.74 avg=2.60\n",
            "[7811 | 3895.77] loss=2.60 avg=2.60\n",
            "[7812 | 3896.26] loss=2.76 avg=2.60\n",
            "[7813 | 3896.74] loss=2.49 avg=2.60\n",
            "[7814 | 3897.22] loss=2.50 avg=2.60\n",
            "[7815 | 3897.72] loss=2.64 avg=2.60\n",
            "[7816 | 3898.20] loss=2.69 avg=2.60\n",
            "[7817 | 3898.69] loss=2.61 avg=2.60\n",
            "[7818 | 3899.18] loss=2.64 avg=2.60\n",
            "[7819 | 3899.67] loss=2.50 avg=2.60\n",
            "[7820 | 3900.15] loss=2.90 avg=2.61\n",
            "[7821 | 3900.64] loss=2.42 avg=2.60\n",
            "[7822 | 3901.12] loss=2.68 avg=2.60\n",
            "[7823 | 3901.61] loss=2.63 avg=2.60\n",
            "[7824 | 3902.10] loss=2.66 avg=2.61\n",
            "[7825 | 3902.59] loss=2.85 avg=2.61\n",
            "[7826 | 3903.07] loss=2.51 avg=2.61\n",
            "[7827 | 3903.56] loss=2.80 avg=2.61\n",
            "[7828 | 3904.05] loss=2.36 avg=2.61\n",
            "[7829 | 3904.54] loss=2.60 avg=2.61\n",
            "[7830 | 3905.02] loss=2.83 avg=2.61\n",
            "[7831 | 3905.51] loss=2.57 avg=2.61\n",
            "[7832 | 3906.00] loss=2.58 avg=2.61\n",
            "[7833 | 3906.48] loss=2.45 avg=2.61\n",
            "[7834 | 3906.97] loss=2.43 avg=2.60\n",
            "[7835 | 3907.46] loss=2.93 avg=2.61\n",
            "[7836 | 3907.95] loss=2.46 avg=2.61\n",
            "[7837 | 3908.43] loss=2.64 avg=2.61\n",
            "[7838 | 3908.92] loss=2.65 avg=2.61\n",
            "[7839 | 3909.41] loss=2.64 avg=2.61\n",
            "[7840 | 3909.89] loss=2.31 avg=2.60\n",
            "[7841 | 3910.38] loss=2.62 avg=2.60\n",
            "[7842 | 3910.87] loss=2.56 avg=2.60\n",
            "[7843 | 3911.35] loss=2.76 avg=2.61\n",
            "[7844 | 3911.84] loss=2.71 avg=2.61\n",
            "[7845 | 3912.33] loss=2.75 avg=2.61\n",
            "[7846 | 3912.81] loss=2.48 avg=2.61\n",
            "[7847 | 3913.30] loss=2.09 avg=2.60\n",
            "[7848 | 3913.79] loss=2.64 avg=2.60\n",
            "[7849 | 3914.27] loss=2.51 avg=2.60\n",
            "[7850 | 3914.76] loss=2.47 avg=2.60\n",
            "[7851 | 3915.25] loss=2.63 avg=2.60\n",
            "[7852 | 3915.74] loss=2.71 avg=2.60\n",
            "[7853 | 3916.22] loss=2.93 avg=2.60\n",
            "[7854 | 3916.71] loss=2.36 avg=2.60\n",
            "[7855 | 3917.20] loss=2.78 avg=2.60\n",
            "[7856 | 3917.69] loss=2.78 avg=2.61\n",
            "[7857 | 3918.17] loss=2.91 avg=2.61\n",
            "[7858 | 3918.66] loss=2.56 avg=2.61\n",
            "[7859 | 3919.15] loss=2.29 avg=2.60\n",
            "[7860 | 3919.64] loss=2.83 avg=2.61\n",
            "[7861 | 3920.13] loss=2.70 avg=2.61\n",
            "[7862 | 3920.62] loss=2.76 avg=2.61\n",
            "[7863 | 3921.10] loss=2.34 avg=2.61\n",
            "[7864 | 3921.59] loss=2.53 avg=2.61\n",
            "[7865 | 3922.08] loss=2.52 avg=2.61\n",
            "[7866 | 3922.56] loss=2.78 avg=2.61\n",
            "[7867 | 3923.05] loss=2.77 avg=2.61\n",
            "[7868 | 3923.54] loss=3.09 avg=2.61\n",
            "[7869 | 3924.03] loss=2.79 avg=2.62\n",
            "[7870 | 3924.51] loss=2.74 avg=2.62\n",
            "[7871 | 3925.00] loss=2.67 avg=2.62\n",
            "[7872 | 3925.49] loss=2.73 avg=2.62\n",
            "[7873 | 3925.97] loss=2.62 avg=2.62\n",
            "[7874 | 3926.46] loss=2.80 avg=2.62\n",
            "[7875 | 3926.95] loss=2.75 avg=2.62\n",
            "[7876 | 3927.44] loss=2.66 avg=2.62\n",
            "[7877 | 3927.92] loss=2.74 avg=2.62\n",
            "[7878 | 3928.41] loss=2.74 avg=2.62\n",
            "[7879 | 3928.90] loss=2.73 avg=2.63\n",
            "[7880 | 3929.39] loss=2.32 avg=2.62\n",
            "[7881 | 3929.87] loss=2.46 avg=2.62\n",
            "[7882 | 3930.36] loss=2.64 avg=2.62\n",
            "[7883 | 3930.85] loss=2.69 avg=2.62\n",
            "[7884 | 3931.34] loss=2.64 avg=2.62\n",
            "[7885 | 3931.82] loss=2.49 avg=2.62\n",
            "[7886 | 3932.31] loss=2.71 avg=2.62\n",
            "[7887 | 3932.80] loss=2.50 avg=2.62\n",
            "[7888 | 3933.29] loss=3.06 avg=2.62\n",
            "[7889 | 3933.78] loss=2.69 avg=2.62\n",
            "[7890 | 3934.26] loss=2.83 avg=2.63\n",
            "[7891 | 3934.75] loss=2.38 avg=2.62\n",
            "[7892 | 3935.24] loss=2.54 avg=2.62\n",
            "[7893 | 3935.73] loss=2.44 avg=2.62\n",
            "[7894 | 3936.21] loss=2.66 avg=2.62\n",
            "[7895 | 3936.70] loss=2.96 avg=2.63\n",
            "[7896 | 3937.19] loss=2.72 avg=2.63\n",
            "[7897 | 3937.68] loss=2.58 avg=2.63\n",
            "[7898 | 3938.17] loss=2.79 avg=2.63\n",
            "[7899 | 3938.65] loss=2.22 avg=2.62\n",
            "[7900 | 3939.14] loss=2.56 avg=2.62\n",
            "[7901 | 3939.62] loss=2.72 avg=2.62\n",
            "[7902 | 3940.12] loss=2.94 avg=2.63\n",
            "[7903 | 3940.60] loss=2.55 avg=2.63\n",
            "[7904 | 3941.09] loss=2.41 avg=2.62\n",
            "[7905 | 3941.58] loss=2.26 avg=2.62\n",
            "[7906 | 3942.07] loss=2.54 avg=2.62\n",
            "[7907 | 3942.56] loss=2.49 avg=2.62\n",
            "[7908 | 3943.04] loss=2.57 avg=2.62\n",
            "[7909 | 3943.53] loss=2.57 avg=2.62\n",
            "[7910 | 3944.02] loss=2.86 avg=2.62\n",
            "[7911 | 3944.51] loss=2.44 avg=2.62\n",
            "[7912 | 3944.99] loss=2.58 avg=2.62\n",
            "[7913 | 3945.48] loss=2.25 avg=2.61\n",
            "[7914 | 3945.96] loss=2.35 avg=2.61\n",
            "[7915 | 3946.46] loss=2.51 avg=2.61\n",
            "[7916 | 3946.94] loss=2.78 avg=2.61\n",
            "[7917 | 3947.43] loss=2.79 avg=2.61\n",
            "[7918 | 3947.92] loss=2.79 avg=2.62\n",
            "[7919 | 3948.41] loss=2.69 avg=2.62\n",
            "[7920 | 3948.89] loss=2.33 avg=2.61\n",
            "[7921 | 3949.38] loss=2.72 avg=2.61\n",
            "[7922 | 3949.87] loss=2.57 avg=2.61\n",
            "[7923 | 3950.36] loss=2.61 avg=2.61\n",
            "[7924 | 3950.84] loss=2.88 avg=2.62\n",
            "[7925 | 3951.33] loss=2.44 avg=2.61\n",
            "[7926 | 3951.82] loss=2.49 avg=2.61\n",
            "[7927 | 3952.31] loss=2.57 avg=2.61\n",
            "[7928 | 3952.79] loss=2.41 avg=2.61\n",
            "[7929 | 3953.28] loss=2.90 avg=2.61\n",
            "[7930 | 3953.77] loss=2.62 avg=2.61\n",
            "[7931 | 3954.25] loss=2.90 avg=2.62\n",
            "[7932 | 3954.74] loss=2.61 avg=2.62\n",
            "[7933 | 3955.23] loss=2.74 avg=2.62\n",
            "[7934 | 3955.72] loss=2.70 avg=2.62\n",
            "[7935 | 3956.20] loss=2.61 avg=2.62\n",
            "[7936 | 3956.69] loss=2.75 avg=2.62\n",
            "[7937 | 3957.18] loss=2.48 avg=2.62\n",
            "[7938 | 3957.66] loss=2.72 avg=2.62\n",
            "[7939 | 3958.15] loss=2.42 avg=2.62\n",
            "[7940 | 3958.64] loss=2.55 avg=2.62\n",
            "[7941 | 3959.13] loss=2.80 avg=2.62\n",
            "[7942 | 3959.61] loss=2.81 avg=2.62\n",
            "[7943 | 3960.10] loss=2.38 avg=2.62\n",
            "[7944 | 3960.59] loss=2.38 avg=2.62\n",
            "[7945 | 3961.08] loss=3.02 avg=2.62\n",
            "[7946 | 3961.56] loss=2.35 avg=2.62\n",
            "[7947 | 3962.05] loss=2.33 avg=2.61\n",
            "[7948 | 3962.54] loss=2.45 avg=2.61\n",
            "[7949 | 3963.02] loss=2.81 avg=2.61\n",
            "[7950 | 3963.51] loss=2.46 avg=2.61\n",
            "[7951 | 3964.00] loss=2.82 avg=2.62\n",
            "[7952 | 3964.49] loss=2.65 avg=2.62\n",
            "[7953 | 3964.97] loss=2.86 avg=2.62\n",
            "[7954 | 3965.46] loss=2.82 avg=2.62\n",
            "[7955 | 3965.94] loss=2.70 avg=2.62\n",
            "[7956 | 3966.43] loss=2.78 avg=2.62\n",
            "[7957 | 3966.92] loss=2.53 avg=2.62\n",
            "[7958 | 3967.41] loss=2.70 avg=2.62\n",
            "[7959 | 3967.89] loss=2.76 avg=2.62\n",
            "[7960 | 3968.38] loss=2.97 avg=2.63\n",
            "[7961 | 3968.87] loss=2.24 avg=2.62\n",
            "[7962 | 3969.35] loss=2.50 avg=2.62\n",
            "[7963 | 3969.84] loss=2.65 avg=2.62\n",
            "[7964 | 3970.32] loss=2.49 avg=2.62\n",
            "[7965 | 3970.81] loss=2.26 avg=2.62\n",
            "[7966 | 3971.30] loss=2.64 avg=2.62\n",
            "[7967 | 3971.79] loss=2.78 avg=2.62\n",
            "[7968 | 3972.27] loss=2.69 avg=2.62\n",
            "[7969 | 3972.76] loss=2.75 avg=2.62\n",
            "[7970 | 3973.25] loss=2.55 avg=2.62\n",
            "[7971 | 3973.74] loss=2.80 avg=2.62\n",
            "[7972 | 3974.22] loss=2.70 avg=2.62\n",
            "[7973 | 3974.71] loss=2.74 avg=2.62\n",
            "[7974 | 3975.20] loss=2.48 avg=2.62\n",
            "[7975 | 3975.69] loss=3.18 avg=2.63\n",
            "[7976 | 3976.17] loss=2.66 avg=2.63\n",
            "[7977 | 3976.66] loss=2.47 avg=2.63\n",
            "[7978 | 3977.15] loss=2.49 avg=2.63\n",
            "[7979 | 3977.64] loss=2.81 avg=2.63\n",
            "[7980 | 3978.13] loss=2.95 avg=2.63\n",
            "[7981 | 3978.61] loss=2.64 avg=2.63\n",
            "[7982 | 3979.10] loss=2.93 avg=2.63\n",
            "[7983 | 3979.58] loss=2.83 avg=2.64\n",
            "[7984 | 3980.07] loss=2.50 avg=2.63\n",
            "[7985 | 3980.56] loss=2.92 avg=2.64\n",
            "[7986 | 3981.05] loss=2.30 avg=2.63\n",
            "[7987 | 3981.53] loss=2.79 avg=2.64\n",
            "[7988 | 3982.02] loss=2.31 avg=2.63\n",
            "[7989 | 3982.51] loss=2.51 avg=2.63\n",
            "[7990 | 3982.99] loss=2.83 avg=2.63\n",
            "[7991 | 3983.48] loss=2.36 avg=2.63\n",
            "[7992 | 3983.97] loss=2.38 avg=2.63\n",
            "[7993 | 3984.45] loss=2.52 avg=2.63\n",
            "[7994 | 3984.94] loss=2.74 avg=2.63\n",
            "[7995 | 3985.43] loss=2.81 avg=2.63\n",
            "[7996 | 3985.92] loss=2.73 avg=2.63\n",
            "[7997 | 3986.40] loss=2.45 avg=2.63\n",
            "[7998 | 3986.89] loss=2.77 avg=2.63\n",
            "[7999 | 3987.38] loss=2.82 avg=2.63\n",
            "Saving checkpoint/run1/model-8000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "” \n",
            "“I have a gift for you,” said the queen. “I have a gift for you,” she said, “and I’ll give you a \n",
            "little bit of it.” \n",
            "“I will,” said the queen. “I will.” \n",
            "“You will,” said the knight. “I will.” \n",
            "“I will,” said the queen. “I will.” \n",
            "“I will,” said the knight. “I will.” \n",
            "“I will,” said the queen. “I will.” \n",
            "The queen’s eyes were red, and her mouth was full of tears. “I will,” she said, “but I’ll give you a little bit of it. \n",
            "I will give you a little bit of it.” \n",
            "“I will,” said the knight. “I will.” \n",
            "“I will,” said the queen. “I will.” \n",
            "The knight gave her a hand to steady herself. “I will,” she said, “but I’ll give you a little bit of it.” \n",
            "The queen’s eyes were red, and her mouth was full of tears. “I will,” she said, “but I’ll give you a little bit of it.” \n",
            "The knight gave her a hand to steady herself. “I will,” she said, “but I’ll give you a little bit of it.” \n",
            "The queen’s eyes were red, and her mouth was full of tears. “I will,” she said, “but I’ll give you a little bit of it.” \n",
            "The knight gave her a hand to steady herself. “I will,” she said, “but I’ll give you a little bit of it.” \n",
            "The queen’s eyes were red, and her mouth was full of tears. “I will,” she said, “but I’ll give you a little bit of it.” \n",
            "The knight gave her a hand to steady herself. “I will,” she said, “but I’ll give you a little bit of it.” \n",
            "The queen’s eyes were red, and her mouth was full of tears. “I will,” she said, “but I’ll give you a little bit of it.” \n",
            "The knight gave her a hand to steady herself. “I will,” she said, “but I’ll give you a little bit of it.” \n",
            "The queen’s eyes were red, and her mouth was full of tears. “I will,” she said, “but I’ll give you a little bit of it.” \n",
            "The knight gave her a hand to steady herself. “I will,” she said, “but I’ll give you a little bit of it.” \n",
            "The queen’s eyes were red, and her mouth was full of tears. “I will,” she said, “but I’ll give you a little bit of it.” \n",
            "The knight gave her a hand to steady herself. “I will,” she said, “but I’ll give you a little bit of it.” \n",
            "The queen’s eyes were red, and her mouth was full of tears. “I will,” she said, “but I’ll give you a little bit of it.” \n",
            "The knight gave her a hand to steady herself. “I will,” she said, “but I’ll give you a little bit of it.” \n",
            "The queen’s eyes were red, and her mouth was full of tears. “I will,” she said, “but I’ll give you a little bit of it.” \n",
            "The knight gave her a hand to steady herself. “I will,” she said, “but I’ll give you a little bit of it.” \n",
            "The queen’s eyes were red, and her mouth was full of tears. “I will,” she said, “but I’ll give you a little bit of it.” \n",
            "The knight gave her a hand to\n",
            "\n",
            "[8000 | 4000.82] loss=2.67 avg=2.63\n",
            "[8001 | 4001.31] loss=2.36 avg=2.63\n",
            "[8002 | 4001.79] loss=2.75 avg=2.63\n",
            "[8003 | 4002.28] loss=2.72 avg=2.63\n",
            "[8004 | 4002.76] loss=1.73 avg=2.62\n",
            "[8005 | 4003.26] loss=2.70 avg=2.62\n",
            "[8006 | 4003.75] loss=2.76 avg=2.63\n",
            "[8007 | 4004.24] loss=2.90 avg=2.63\n",
            "[8008 | 4004.72] loss=2.81 avg=2.63\n",
            "[8009 | 4005.21] loss=3.04 avg=2.63\n",
            "[8010 | 4005.70] loss=2.80 avg=2.64\n",
            "[8011 | 4006.19] loss=2.63 avg=2.64\n",
            "[8012 | 4006.68] loss=2.30 avg=2.63\n",
            "[8013 | 4007.17] loss=2.20 avg=2.63\n",
            "[8014 | 4007.65] loss=2.67 avg=2.63\n",
            "[8015 | 4008.14] loss=2.82 avg=2.63\n",
            "[8016 | 4008.63] loss=2.50 avg=2.63\n",
            "[8017 | 4009.12] loss=2.74 avg=2.63\n",
            "[8018 | 4009.61] loss=2.77 avg=2.63\n",
            "[8019 | 4010.09] loss=2.99 avg=2.64\n",
            "[8020 | 4010.58] loss=2.98 avg=2.64\n",
            "[8021 | 4011.07] loss=2.64 avg=2.64\n",
            "[8022 | 4011.56] loss=2.93 avg=2.64\n",
            "[8023 | 4012.04] loss=2.59 avg=2.64\n",
            "[8024 | 4012.53] loss=2.58 avg=2.64\n",
            "[8025 | 4013.02] loss=2.51 avg=2.64\n",
            "[8026 | 4013.51] loss=2.50 avg=2.64\n",
            "[8027 | 4013.99] loss=2.25 avg=2.63\n",
            "[8028 | 4014.48] loss=2.57 avg=2.63\n",
            "[8029 | 4014.97] loss=2.62 avg=2.63\n",
            "[8030 | 4015.46] loss=2.59 avg=2.63\n",
            "[8031 | 4015.95] loss=2.77 avg=2.63\n",
            "[8032 | 4016.43] loss=2.56 avg=2.63\n",
            "[8033 | 4016.92] loss=2.50 avg=2.63\n",
            "[8034 | 4017.41] loss=2.76 avg=2.63\n",
            "[8035 | 4017.90] loss=2.83 avg=2.64\n",
            "[8036 | 4018.38] loss=2.57 avg=2.63\n",
            "[8037 | 4018.87] loss=2.92 avg=2.64\n",
            "[8038 | 4019.36] loss=2.81 avg=2.64\n",
            "[8039 | 4019.84] loss=2.38 avg=2.64\n",
            "[8040 | 4020.33] loss=2.76 avg=2.64\n",
            "[8041 | 4020.82] loss=2.78 avg=2.64\n",
            "[8042 | 4021.31] loss=2.47 avg=2.64\n",
            "[8043 | 4021.79] loss=2.63 avg=2.64\n",
            "[8044 | 4022.28] loss=2.48 avg=2.64\n",
            "[8045 | 4022.77] loss=2.62 avg=2.64\n",
            "[8046 | 4023.25] loss=2.70 avg=2.64\n",
            "[8047 | 4023.74] loss=2.64 avg=2.64\n",
            "[8048 | 4024.23] loss=2.60 avg=2.64\n",
            "[8049 | 4024.71] loss=2.64 avg=2.64\n",
            "[8050 | 4025.20] loss=2.88 avg=2.64\n",
            "[8051 | 4025.69] loss=2.95 avg=2.64\n",
            "[8052 | 4026.17] loss=2.83 avg=2.64\n",
            "[8053 | 4026.66] loss=2.56 avg=2.64\n",
            "[8054 | 4027.15] loss=2.57 avg=2.64\n",
            "[8055 | 4027.64] loss=2.73 avg=2.64\n",
            "[8056 | 4028.12] loss=2.52 avg=2.64\n",
            "[8057 | 4028.61] loss=2.58 avg=2.64\n",
            "[8058 | 4029.10] loss=2.68 avg=2.64\n",
            "[8059 | 4029.58] loss=2.55 avg=2.64\n",
            "[8060 | 4030.07] loss=2.58 avg=2.64\n",
            "[8061 | 4030.56] loss=3.08 avg=2.64\n",
            "[8062 | 4031.05] loss=2.64 avg=2.64\n",
            "[8063 | 4031.53] loss=2.50 avg=2.64\n",
            "[8064 | 4032.02] loss=2.68 avg=2.64\n",
            "[8065 | 4032.51] loss=2.56 avg=2.64\n",
            "[8066 | 4032.99] loss=2.90 avg=2.64\n",
            "[8067 | 4033.48] loss=2.80 avg=2.65\n",
            "[8068 | 4033.97] loss=2.66 avg=2.65\n",
            "[8069 | 4034.45] loss=2.72 avg=2.65\n",
            "[8070 | 4034.94] loss=2.39 avg=2.64\n",
            "[8071 | 4035.43] loss=2.31 avg=2.64\n",
            "[8072 | 4035.91] loss=2.74 avg=2.64\n",
            "[8073 | 4036.40] loss=2.55 avg=2.64\n",
            "[8074 | 4036.89] loss=2.30 avg=2.64\n",
            "[8075 | 4037.38] loss=2.58 avg=2.64\n",
            "[8076 | 4037.86] loss=2.50 avg=2.64\n",
            "[8077 | 4038.35] loss=2.83 avg=2.64\n",
            "[8078 | 4038.84] loss=2.56 avg=2.64\n",
            "[8079 | 4039.32] loss=2.59 avg=2.64\n",
            "[8080 | 4039.81] loss=2.50 avg=2.64\n",
            "[8081 | 4040.30] loss=2.72 avg=2.64\n",
            "[8082 | 4040.79] loss=2.35 avg=2.63\n",
            "[8083 | 4041.27] loss=2.39 avg=2.63\n",
            "[8084 | 4041.76] loss=2.52 avg=2.63\n",
            "[8085 | 4042.25] loss=2.44 avg=2.63\n",
            "[8086 | 4042.73] loss=2.81 avg=2.63\n",
            "[8087 | 4043.22] loss=2.88 avg=2.63\n",
            "[8088 | 4043.70] loss=2.82 avg=2.63\n",
            "[8089 | 4044.19] loss=2.58 avg=2.63\n",
            "[8090 | 4044.68] loss=2.87 avg=2.64\n",
            "[8091 | 4045.17] loss=2.70 avg=2.64\n",
            "[8092 | 4045.66] loss=2.85 avg=2.64\n",
            "[8093 | 4046.15] loss=2.64 avg=2.64\n",
            "[8094 | 4046.63] loss=2.61 avg=2.64\n",
            "[8095 | 4047.12] loss=2.82 avg=2.64\n",
            "[8096 | 4047.61] loss=2.51 avg=2.64\n",
            "[8097 | 4048.09] loss=2.65 avg=2.64\n",
            "[8098 | 4048.58] loss=2.41 avg=2.64\n",
            "[8099 | 4049.07] loss=2.40 avg=2.63\n",
            "[8100 | 4049.56] loss=2.68 avg=2.63\n",
            "[8101 | 4050.04] loss=2.65 avg=2.63\n",
            "[8102 | 4050.53] loss=2.83 avg=2.64\n",
            "[8103 | 4051.02] loss=2.56 avg=2.64\n",
            "[8104 | 4051.51] loss=2.74 avg=2.64\n",
            "[8105 | 4051.99] loss=1.74 avg=2.63\n",
            "[8106 | 4052.48] loss=2.54 avg=2.63\n",
            "[8107 | 4052.97] loss=2.88 avg=2.63\n",
            "[8108 | 4053.46] loss=2.35 avg=2.63\n",
            "[8109 | 4053.94] loss=2.24 avg=2.62\n",
            "[8110 | 4054.43] loss=2.47 avg=2.62\n",
            "[8111 | 4054.92] loss=2.60 avg=2.62\n",
            "[8112 | 4055.41] loss=2.46 avg=2.62\n",
            "[8113 | 4055.90] loss=2.17 avg=2.62\n",
            "[8114 | 4056.38] loss=2.71 avg=2.62\n",
            "[8115 | 4056.87] loss=2.57 avg=2.62\n",
            "[8116 | 4057.35] loss=2.55 avg=2.62\n",
            "[8117 | 4057.85] loss=2.63 avg=2.62\n",
            "[8118 | 4058.33] loss=2.55 avg=2.61\n",
            "[8119 | 4058.82] loss=2.64 avg=2.61\n",
            "[8120 | 4059.30] loss=2.53 avg=2.61\n",
            "[8121 | 4059.79] loss=2.65 avg=2.61\n",
            "[8122 | 4060.28] loss=2.18 avg=2.61\n",
            "[8123 | 4060.77] loss=2.78 avg=2.61\n",
            "[8124 | 4061.25] loss=2.67 avg=2.61\n",
            "[8125 | 4061.74] loss=2.64 avg=2.61\n",
            "[8126 | 4062.23] loss=2.74 avg=2.61\n",
            "[8127 | 4062.71] loss=2.61 avg=2.61\n",
            "[8128 | 4063.20] loss=2.63 avg=2.61\n",
            "[8129 | 4063.69] loss=2.67 avg=2.61\n",
            "[8130 | 4064.18] loss=2.75 avg=2.62\n",
            "[8131 | 4064.66] loss=2.48 avg=2.61\n",
            "[8132 | 4065.15] loss=2.69 avg=2.62\n",
            "[8133 | 4065.64] loss=2.77 avg=2.62\n",
            "[8134 | 4066.13] loss=2.62 avg=2.62\n",
            "[8135 | 4066.62] loss=2.71 avg=2.62\n",
            "[8136 | 4067.10] loss=2.70 avg=2.62\n",
            "[8137 | 4067.59] loss=2.79 avg=2.62\n",
            "[8138 | 4068.08] loss=2.42 avg=2.62\n",
            "[8139 | 4068.56] loss=2.34 avg=2.62\n",
            "[8140 | 4069.05] loss=2.53 avg=2.61\n",
            "[8141 | 4069.54] loss=2.81 avg=2.62\n",
            "[8142 | 4070.03] loss=2.72 avg=2.62\n",
            "[8143 | 4070.51] loss=2.66 avg=2.62\n",
            "[8144 | 4071.00] loss=2.57 avg=2.62\n",
            "[8145 | 4071.49] loss=2.57 avg=2.62\n",
            "[8146 | 4071.97] loss=2.44 avg=2.62\n",
            "[8147 | 4072.47] loss=2.11 avg=2.61\n",
            "[8148 | 4072.95] loss=2.83 avg=2.61\n",
            "[8149 | 4073.44] loss=2.95 avg=2.62\n",
            "[8150 | 4073.93] loss=2.74 avg=2.62\n",
            "[8151 | 4074.42] loss=2.77 avg=2.62\n",
            "[8152 | 4074.91] loss=2.81 avg=2.62\n",
            "[8153 | 4075.39] loss=2.53 avg=2.62\n",
            "[8154 | 4075.88] loss=2.69 avg=2.62\n",
            "[8155 | 4076.37] loss=2.74 avg=2.62\n",
            "[8156 | 4076.86] loss=2.77 avg=2.62\n",
            "[8157 | 4077.34] loss=2.56 avg=2.62\n",
            "[8158 | 4077.83] loss=2.47 avg=2.62\n",
            "[8159 | 4078.32] loss=1.90 avg=2.61\n",
            "[8160 | 4078.81] loss=2.96 avg=2.62\n",
            "[8161 | 4079.30] loss=2.67 avg=2.62\n",
            "[8162 | 4079.78] loss=2.70 avg=2.62\n",
            "[8163 | 4080.27] loss=2.35 avg=2.62\n",
            "[8164 | 4080.76] loss=2.43 avg=2.61\n",
            "[8165 | 4081.24] loss=2.66 avg=2.61\n",
            "[8166 | 4081.73] loss=3.11 avg=2.62\n",
            "[8167 | 4082.22] loss=2.77 avg=2.62\n",
            "[8168 | 4082.71] loss=1.92 avg=2.61\n",
            "[8169 | 4083.20] loss=2.65 avg=2.61\n",
            "[8170 | 4083.69] loss=2.66 avg=2.61\n",
            "[8171 | 4084.18] loss=2.36 avg=2.61\n",
            "[8172 | 4084.66] loss=2.40 avg=2.61\n",
            "[8173 | 4085.15] loss=2.67 avg=2.61\n",
            "[8174 | 4085.63] loss=2.29 avg=2.61\n",
            "[8175 | 4086.12] loss=2.51 avg=2.61\n",
            "[8176 | 4086.61] loss=2.28 avg=2.60\n",
            "[8177 | 4087.10] loss=2.72 avg=2.60\n",
            "[8178 | 4087.58] loss=2.64 avg=2.60\n",
            "[8179 | 4088.07] loss=2.44 avg=2.60\n",
            "[8180 | 4088.55] loss=3.07 avg=2.61\n",
            "[8181 | 4089.04] loss=2.85 avg=2.61\n",
            "[8182 | 4089.53] loss=2.59 avg=2.61\n",
            "[8183 | 4090.02] loss=2.66 avg=2.61\n",
            "[8184 | 4090.51] loss=2.67 avg=2.61\n",
            "[8185 | 4090.99] loss=2.20 avg=2.61\n",
            "[8186 | 4091.48] loss=2.70 avg=2.61\n",
            "[8187 | 4091.96] loss=2.53 avg=2.61\n",
            "[8188 | 4092.45] loss=2.60 avg=2.61\n",
            "[8189 | 4092.94] loss=2.55 avg=2.61\n",
            "[8190 | 4093.43] loss=3.00 avg=2.61\n",
            "[8191 | 4093.91] loss=3.00 avg=2.61\n",
            "[8192 | 4094.40] loss=2.76 avg=2.62\n",
            "[8193 | 4094.89] loss=2.81 avg=2.62\n",
            "[8194 | 4095.38] loss=2.44 avg=2.62\n",
            "[8195 | 4095.86] loss=2.87 avg=2.62\n",
            "[8196 | 4096.35] loss=2.80 avg=2.62\n",
            "[8197 | 4096.84] loss=2.88 avg=2.62\n",
            "[8198 | 4097.32] loss=3.05 avg=2.63\n",
            "[8199 | 4097.81] loss=2.48 avg=2.63\n",
            "[8200 | 4098.30] loss=2.58 avg=2.63\n",
            "[8201 | 4098.79] loss=3.12 avg=2.63\n",
            "[8202 | 4099.27] loss=2.98 avg=2.63\n",
            "[8203 | 4099.76] loss=2.51 avg=2.63\n",
            "[8204 | 4100.25] loss=2.36 avg=2.63\n",
            "[8205 | 4100.74] loss=2.60 avg=2.63\n",
            "[8206 | 4101.22] loss=2.60 avg=2.63\n",
            "[8207 | 4101.71] loss=2.75 avg=2.63\n",
            "[8208 | 4102.20] loss=2.26 avg=2.63\n",
            "[8209 | 4102.68] loss=2.73 avg=2.63\n",
            "[8210 | 4103.17] loss=2.96 avg=2.63\n",
            "[8211 | 4103.65] loss=2.78 avg=2.63\n",
            "[8212 | 4104.15] loss=2.58 avg=2.63\n",
            "[8213 | 4104.63] loss=2.65 avg=2.63\n",
            "[8214 | 4105.12] loss=2.83 avg=2.63\n",
            "[8215 | 4105.61] loss=2.46 avg=2.63\n",
            "[8216 | 4106.09] loss=2.62 avg=2.63\n",
            "[8217 | 4106.58] loss=2.83 avg=2.63\n",
            "[8218 | 4107.07] loss=2.48 avg=2.63\n",
            "[8219 | 4107.56] loss=2.96 avg=2.64\n",
            "[8220 | 4108.04] loss=2.46 avg=2.63\n",
            "[8221 | 4108.53] loss=2.50 avg=2.63\n",
            "[8222 | 4109.02] loss=2.66 avg=2.63\n",
            "[8223 | 4109.50] loss=2.63 avg=2.63\n",
            "[8224 | 4109.99] loss=2.75 avg=2.63\n",
            "[8225 | 4110.48] loss=2.71 avg=2.63\n",
            "[8226 | 4110.97] loss=2.27 avg=2.63\n",
            "[8227 | 4111.46] loss=2.68 avg=2.63\n",
            "[8228 | 4111.94] loss=2.62 avg=2.63\n",
            "[8229 | 4112.43] loss=2.59 avg=2.63\n",
            "[8230 | 4112.92] loss=2.53 avg=2.63\n",
            "[8231 | 4113.40] loss=2.63 avg=2.63\n",
            "[8232 | 4113.89] loss=2.99 avg=2.63\n",
            "[8233 | 4114.38] loss=2.79 avg=2.64\n",
            "[8234 | 4114.87] loss=2.73 avg=2.64\n",
            "[8235 | 4115.35] loss=2.69 avg=2.64\n",
            "[8236 | 4115.84] loss=2.96 avg=2.64\n",
            "[8237 | 4116.33] loss=2.97 avg=2.64\n",
            "[8238 | 4116.81] loss=2.92 avg=2.65\n",
            "[8239 | 4117.30] loss=2.52 avg=2.64\n",
            "[8240 | 4117.79] loss=2.66 avg=2.65\n",
            "[8241 | 4118.28] loss=2.54 avg=2.64\n",
            "[8242 | 4118.77] loss=2.27 avg=2.64\n",
            "[8243 | 4119.25] loss=2.68 avg=2.64\n",
            "[8244 | 4119.74] loss=2.71 avg=2.64\n",
            "[8245 | 4120.23] loss=2.50 avg=2.64\n",
            "[8246 | 4120.71] loss=2.35 avg=2.64\n",
            "[8247 | 4121.20] loss=2.31 avg=2.63\n",
            "[8248 | 4121.69] loss=2.74 avg=2.63\n",
            "[8249 | 4122.17] loss=2.53 avg=2.63\n",
            "[8250 | 4122.66] loss=2.47 avg=2.63\n",
            "[8251 | 4123.15] loss=2.64 avg=2.63\n",
            "[8252 | 4123.64] loss=2.53 avg=2.63\n",
            "[8253 | 4124.12] loss=2.38 avg=2.63\n",
            "[8254 | 4124.61] loss=2.51 avg=2.63\n",
            "[8255 | 4125.10] loss=2.87 avg=2.63\n",
            "[8256 | 4125.58] loss=2.89 avg=2.63\n",
            "[8257 | 4126.07] loss=2.15 avg=2.63\n",
            "[8258 | 4126.56] loss=2.65 avg=2.63\n",
            "[8259 | 4127.05] loss=2.74 avg=2.63\n",
            "[8260 | 4127.53] loss=2.54 avg=2.63\n",
            "[8261 | 4128.02] loss=2.60 avg=2.63\n",
            "[8262 | 4128.51] loss=2.51 avg=2.63\n",
            "[8263 | 4129.00] loss=2.52 avg=2.63\n",
            "[8264 | 4129.48] loss=2.78 avg=2.63\n",
            "[8265 | 4129.97] loss=2.86 avg=2.63\n",
            "[8266 | 4130.46] loss=2.32 avg=2.63\n",
            "[8267 | 4130.95] loss=2.52 avg=2.63\n",
            "[8268 | 4131.43] loss=2.42 avg=2.62\n",
            "[8269 | 4131.92] loss=2.73 avg=2.62\n",
            "[8270 | 4132.41] loss=2.78 avg=2.63\n",
            "[8271 | 4132.89] loss=2.56 avg=2.63\n",
            "[8272 | 4133.38] loss=2.28 avg=2.62\n",
            "[8273 | 4133.87] loss=2.47 avg=2.62\n",
            "[8274 | 4134.36] loss=2.56 avg=2.62\n",
            "[8275 | 4134.84] loss=2.72 avg=2.62\n",
            "[8276 | 4135.33] loss=2.69 avg=2.62\n",
            "[8277 | 4135.82] loss=2.66 avg=2.62\n",
            "[8278 | 4136.31] loss=2.78 avg=2.62\n",
            "[8279 | 4136.80] loss=2.54 avg=2.62\n",
            "[8280 | 4137.29] loss=2.87 avg=2.63\n",
            "[8281 | 4137.77] loss=2.76 avg=2.63\n",
            "[8282 | 4138.26] loss=2.38 avg=2.62\n",
            "[8283 | 4138.75] loss=2.49 avg=2.62\n",
            "[8284 | 4139.24] loss=2.47 avg=2.62\n",
            "[8285 | 4139.72] loss=2.89 avg=2.62\n",
            "[8286 | 4140.21] loss=2.45 avg=2.62\n",
            "[8287 | 4140.70] loss=2.56 avg=2.62\n",
            "[8288 | 4141.19] loss=2.48 avg=2.62\n",
            "[8289 | 4141.68] loss=2.48 avg=2.62\n",
            "[8290 | 4142.17] loss=2.71 avg=2.62\n",
            "[8291 | 4142.65] loss=2.65 avg=2.62\n",
            "[8292 | 4143.14] loss=2.34 avg=2.62\n",
            "[8293 | 4143.63] loss=2.70 avg=2.62\n",
            "[8294 | 4144.12] loss=2.65 avg=2.62\n",
            "[8295 | 4144.61] loss=2.67 avg=2.62\n",
            "[8296 | 4145.09] loss=2.61 avg=2.62\n",
            "[8297 | 4145.58] loss=2.80 avg=2.62\n",
            "[8298 | 4146.07] loss=1.46 avg=2.61\n",
            "[8299 | 4146.56] loss=2.51 avg=2.61\n",
            "[8300 | 4147.05] loss=2.80 avg=2.61\n",
            "[8301 | 4147.54] loss=2.25 avg=2.61\n",
            "[8302 | 4148.02] loss=2.32 avg=2.60\n",
            "[8303 | 4148.51] loss=2.39 avg=2.60\n",
            "[8304 | 4149.00] loss=2.76 avg=2.60\n",
            "[8305 | 4149.49] loss=2.76 avg=2.60\n",
            "[8306 | 4149.98] loss=2.49 avg=2.60\n",
            "[8307 | 4150.47] loss=2.57 avg=2.60\n",
            "[8308 | 4150.96] loss=2.54 avg=2.60\n",
            "[8309 | 4151.44] loss=2.72 avg=2.60\n",
            "[8310 | 4151.93] loss=2.59 avg=2.60\n",
            "[8311 | 4152.42] loss=2.24 avg=2.60\n",
            "[8312 | 4152.91] loss=2.84 avg=2.60\n",
            "[8313 | 4153.39] loss=2.78 avg=2.60\n",
            "[8314 | 4153.88] loss=2.69 avg=2.60\n",
            "[8315 | 4154.36] loss=2.58 avg=2.60\n",
            "[8316 | 4154.85] loss=2.63 avg=2.60\n",
            "[8317 | 4155.34] loss=2.61 avg=2.60\n",
            "[8318 | 4155.83] loss=2.96 avg=2.61\n",
            "[8319 | 4156.31] loss=2.87 avg=2.61\n",
            "[8320 | 4156.80] loss=2.59 avg=2.61\n",
            "[8321 | 4157.29] loss=2.49 avg=2.61\n",
            "[8322 | 4157.77] loss=1.33 avg=2.60\n",
            "[8323 | 4158.26] loss=2.56 avg=2.60\n",
            "[8324 | 4158.75] loss=2.40 avg=2.59\n",
            "[8325 | 4159.24] loss=2.70 avg=2.60\n",
            "[8326 | 4159.72] loss=2.92 avg=2.60\n",
            "[8327 | 4160.21] loss=2.55 avg=2.60\n",
            "[8328 | 4160.70] loss=2.48 avg=2.60\n",
            "[8329 | 4161.19] loss=2.19 avg=2.59\n",
            "[8330 | 4161.67] loss=2.85 avg=2.60\n",
            "[8331 | 4162.16] loss=2.26 avg=2.59\n",
            "[8332 | 4162.65] loss=2.58 avg=2.59\n",
            "[8333 | 4163.13] loss=2.62 avg=2.59\n",
            "[8334 | 4163.62] loss=2.48 avg=2.59\n",
            "[8335 | 4164.11] loss=2.74 avg=2.59\n",
            "[8336 | 4164.59] loss=2.57 avg=2.59\n",
            "[8337 | 4165.08] loss=2.52 avg=2.59\n",
            "[8338 | 4165.57] loss=2.79 avg=2.59\n",
            "[8339 | 4166.06] loss=2.71 avg=2.59\n",
            "[8340 | 4166.54] loss=2.58 avg=2.59\n",
            "[8341 | 4167.03] loss=2.70 avg=2.60\n",
            "[8342 | 4167.52] loss=1.86 avg=2.59\n",
            "[8343 | 4168.00] loss=2.59 avg=2.59\n",
            "[8344 | 4168.49] loss=2.83 avg=2.59\n",
            "[8345 | 4168.98] loss=2.66 avg=2.59\n",
            "[8346 | 4169.47] loss=2.76 avg=2.59\n",
            "[8347 | 4169.95] loss=2.64 avg=2.59\n",
            "[8348 | 4170.44] loss=2.70 avg=2.59\n",
            "[8349 | 4170.93] loss=2.61 avg=2.60\n",
            "[8350 | 4171.41] loss=2.70 avg=2.60\n",
            "[8351 | 4171.90] loss=2.49 avg=2.59\n",
            "[8352 | 4172.39] loss=2.69 avg=2.60\n",
            "[8353 | 4172.87] loss=2.38 avg=2.59\n",
            "[8354 | 4173.37] loss=2.54 avg=2.59\n",
            "[8355 | 4173.85] loss=2.47 avg=2.59\n",
            "[8356 | 4174.34] loss=2.65 avg=2.59\n",
            "[8357 | 4174.83] loss=2.48 avg=2.59\n",
            "[8358 | 4175.31] loss=1.84 avg=2.58\n",
            "[8359 | 4175.80] loss=2.51 avg=2.58\n",
            "[8360 | 4176.29] loss=2.38 avg=2.58\n",
            "[8361 | 4176.77] loss=2.66 avg=2.58\n",
            "[8362 | 4177.26] loss=2.61 avg=2.58\n",
            "[8363 | 4177.75] loss=2.67 avg=2.58\n",
            "[8364 | 4178.24] loss=2.77 avg=2.58\n",
            "[8365 | 4178.72] loss=2.69 avg=2.59\n",
            "[8366 | 4179.21] loss=1.76 avg=2.58\n",
            "[8367 | 4179.70] loss=2.80 avg=2.58\n",
            "[8368 | 4180.19] loss=2.59 avg=2.58\n",
            "[8369 | 4180.67] loss=2.78 avg=2.58\n",
            "[8370 | 4181.16] loss=2.40 avg=2.58\n",
            "[8371 | 4181.64] loss=3.10 avg=2.59\n",
            "[8372 | 4182.13] loss=2.61 avg=2.59\n",
            "[8373 | 4182.62] loss=2.98 avg=2.59\n",
            "[8374 | 4183.11] loss=2.53 avg=2.59\n",
            "[8375 | 4183.60] loss=2.57 avg=2.59\n",
            "[8376 | 4184.09] loss=2.49 avg=2.59\n",
            "[8377 | 4184.57] loss=2.77 avg=2.59\n",
            "[8378 | 4185.06] loss=2.55 avg=2.59\n",
            "[8379 | 4185.55] loss=2.75 avg=2.59\n",
            "[8380 | 4186.03] loss=2.48 avg=2.59\n",
            "[8381 | 4186.52] loss=2.57 avg=2.59\n",
            "[8382 | 4187.00] loss=2.89 avg=2.59\n",
            "[8383 | 4187.49] loss=2.81 avg=2.59\n",
            "[8384 | 4187.98] loss=2.63 avg=2.60\n",
            "[8385 | 4188.47] loss=2.81 avg=2.60\n",
            "[8386 | 4188.96] loss=2.76 avg=2.60\n",
            "[8387 | 4189.45] loss=2.46 avg=2.60\n",
            "[8388 | 4189.93] loss=2.04 avg=2.59\n",
            "[8389 | 4190.42] loss=2.80 avg=2.59\n",
            "[8390 | 4190.91] loss=2.82 avg=2.60\n",
            "[8391 | 4191.39] loss=2.94 avg=2.60\n",
            "[8392 | 4191.88] loss=2.57 avg=2.60\n",
            "[8393 | 4192.37] loss=2.41 avg=2.60\n",
            "[8394 | 4192.85] loss=2.72 avg=2.60\n",
            "[8395 | 4193.34] loss=2.25 avg=2.60\n",
            "[8396 | 4193.83] loss=2.46 avg=2.59\n",
            "[8397 | 4194.32] loss=2.34 avg=2.59\n",
            "[8398 | 4194.80] loss=2.41 avg=2.59\n",
            "[8399 | 4195.29] loss=2.93 avg=2.59\n",
            "[8400 | 4195.78] loss=2.53 avg=2.59\n",
            "[8401 | 4196.27] loss=2.60 avg=2.59\n",
            "[8402 | 4196.76] loss=2.54 avg=2.59\n",
            "[8403 | 4197.24] loss=2.52 avg=2.59\n",
            "[8404 | 4197.73] loss=2.54 avg=2.59\n",
            "[8405 | 4198.22] loss=2.69 avg=2.59\n",
            "[8406 | 4198.71] loss=2.72 avg=2.59\n",
            "[8407 | 4199.20] loss=2.80 avg=2.59\n",
            "[8408 | 4199.69] loss=2.71 avg=2.60\n",
            "[8409 | 4200.17] loss=2.86 avg=2.60\n",
            "[8410 | 4200.66] loss=2.51 avg=2.60\n",
            "[8411 | 4201.14] loss=2.63 avg=2.60\n",
            "[8412 | 4201.63] loss=2.85 avg=2.60\n",
            "[8413 | 4202.12] loss=2.57 avg=2.60\n",
            "[8414 | 4202.61] loss=2.77 avg=2.60\n",
            "[8415 | 4203.09] loss=2.73 avg=2.60\n",
            "[8416 | 4203.58] loss=2.63 avg=2.60\n",
            "[8417 | 4204.07] loss=2.16 avg=2.60\n",
            "[8418 | 4204.56] loss=2.66 avg=2.60\n",
            "[8419 | 4205.04] loss=2.69 avg=2.60\n",
            "[8420 | 4205.53] loss=2.66 avg=2.60\n",
            "[8421 | 4206.02] loss=2.36 avg=2.60\n",
            "[8422 | 4206.51] loss=2.67 avg=2.60\n",
            "[8423 | 4206.99] loss=2.84 avg=2.60\n",
            "[8424 | 4207.48] loss=2.29 avg=2.60\n",
            "[8425 | 4207.97] loss=2.22 avg=2.60\n",
            "[8426 | 4208.46] loss=2.81 avg=2.60\n",
            "[8427 | 4208.95] loss=2.58 avg=2.60\n",
            "[8428 | 4209.43] loss=2.65 avg=2.60\n",
            "[8429 | 4209.92] loss=2.76 avg=2.60\n",
            "[8430 | 4210.41] loss=2.67 avg=2.60\n",
            "[8431 | 4210.90] loss=2.56 avg=2.60\n",
            "[8432 | 4211.38] loss=2.06 avg=2.59\n",
            "[8433 | 4211.87] loss=2.83 avg=2.60\n",
            "[8434 | 4212.36] loss=2.58 avg=2.60\n",
            "[8435 | 4212.85] loss=2.58 avg=2.60\n",
            "[8436 | 4213.34] loss=2.30 avg=2.59\n",
            "[8437 | 4213.83] loss=2.54 avg=2.59\n",
            "[8438 | 4214.32] loss=2.17 avg=2.59\n",
            "[8439 | 4214.81] loss=2.09 avg=2.58\n",
            "[8440 | 4215.29] loss=2.12 avg=2.58\n",
            "[8441 | 4215.78] loss=2.49 avg=2.58\n",
            "[8442 | 4216.27] loss=2.74 avg=2.58\n",
            "[8443 | 4216.76] loss=2.77 avg=2.58\n",
            "[8444 | 4217.24] loss=2.73 avg=2.58\n",
            "[8445 | 4217.73] loss=2.40 avg=2.58\n",
            "[8446 | 4218.22] loss=2.36 avg=2.58\n",
            "[8447 | 4218.71] loss=2.59 avg=2.58\n",
            "[8448 | 4219.19] loss=2.53 avg=2.58\n",
            "[8449 | 4219.68] loss=2.30 avg=2.58\n",
            "[8450 | 4220.17] loss=2.47 avg=2.57\n",
            "[8451 | 4220.66] loss=2.75 avg=2.58\n",
            "[8452 | 4221.14] loss=2.69 avg=2.58\n",
            "[8453 | 4221.63] loss=2.70 avg=2.58\n",
            "[8454 | 4222.12] loss=2.80 avg=2.58\n",
            "[8455 | 4222.60] loss=1.79 avg=2.57\n",
            "[8456 | 4223.09] loss=3.04 avg=2.58\n",
            "[8457 | 4223.58] loss=2.55 avg=2.58\n",
            "[8458 | 4224.06] loss=2.17 avg=2.57\n",
            "[8459 | 4224.55] loss=2.86 avg=2.58\n",
            "[8460 | 4225.04] loss=2.73 avg=2.58\n",
            "[8461 | 4225.52] loss=2.61 avg=2.58\n",
            "[8462 | 4226.01] loss=2.63 avg=2.58\n",
            "[8463 | 4226.50] loss=2.34 avg=2.58\n",
            "[8464 | 4226.99] loss=2.40 avg=2.57\n",
            "[8465 | 4227.47] loss=2.65 avg=2.58\n",
            "[8466 | 4227.96] loss=2.46 avg=2.57\n",
            "[8467 | 4228.45] loss=2.07 avg=2.57\n",
            "[8468 | 4228.94] loss=2.60 avg=2.57\n",
            "[8469 | 4229.42] loss=2.57 avg=2.57\n",
            "[8470 | 4229.91] loss=2.40 avg=2.57\n",
            "[8471 | 4230.40] loss=2.61 avg=2.57\n",
            "[8472 | 4230.89] loss=2.84 avg=2.57\n",
            "[8473 | 4231.37] loss=2.83 avg=2.57\n",
            "[8474 | 4231.86] loss=2.92 avg=2.58\n",
            "[8475 | 4232.35] loss=2.34 avg=2.57\n",
            "[8476 | 4232.84] loss=2.35 avg=2.57\n",
            "[8477 | 4233.33] loss=2.65 avg=2.57\n",
            "[8478 | 4233.81] loss=3.12 avg=2.58\n",
            "[8479 | 4234.30] loss=2.67 avg=2.58\n",
            "[8480 | 4234.78] loss=2.44 avg=2.58\n",
            "[8481 | 4235.27] loss=2.57 avg=2.58\n",
            "[8482 | 4235.76] loss=2.21 avg=2.57\n",
            "[8483 | 4236.25] loss=2.88 avg=2.58\n",
            "[8484 | 4236.73] loss=2.58 avg=2.58\n",
            "[8485 | 4237.22] loss=2.73 avg=2.58\n",
            "[8486 | 4237.71] loss=1.24 avg=2.57\n",
            "[8487 | 4238.19] loss=2.88 avg=2.57\n",
            "[8488 | 4238.68] loss=2.75 avg=2.57\n",
            "[8489 | 4239.17] loss=1.48 avg=2.56\n",
            "[8490 | 4239.66] loss=2.72 avg=2.56\n",
            "[8491 | 4240.14] loss=2.54 avg=2.56\n",
            "[8492 | 4240.63] loss=2.80 avg=2.56\n",
            "[8493 | 4241.12] loss=2.49 avg=2.56\n",
            "[8494 | 4241.61] loss=2.79 avg=2.56\n",
            "[8495 | 4242.10] loss=2.62 avg=2.57\n",
            "[8496 | 4242.58] loss=2.68 avg=2.57\n",
            "[8497 | 4243.07] loss=2.17 avg=2.56\n",
            "[8498 | 4243.55] loss=2.54 avg=2.56\n",
            "[8499 | 4244.04] loss=2.53 avg=2.56\n",
            "[8500 | 4244.53] loss=2.60 avg=2.56\n",
            "[8501 | 4245.02] loss=2.66 avg=2.56\n",
            "[8502 | 4245.50] loss=2.46 avg=2.56\n",
            "[8503 | 4245.99] loss=2.61 avg=2.56\n",
            "[8504 | 4246.48] loss=2.67 avg=2.56\n",
            "[8505 | 4246.96] loss=2.66 avg=2.56\n",
            "[8506 | 4247.45] loss=2.38 avg=2.56\n",
            "[8507 | 4247.94] loss=2.78 avg=2.57\n",
            "[8508 | 4248.43] loss=2.64 avg=2.57\n",
            "[8509 | 4248.91] loss=2.67 avg=2.57\n",
            "[8510 | 4249.40] loss=2.46 avg=2.57\n",
            "[8511 | 4249.89] loss=2.79 avg=2.57\n",
            "[8512 | 4250.37] loss=2.72 avg=2.57\n",
            "[8513 | 4250.86] loss=2.77 avg=2.57\n",
            "[8514 | 4251.35] loss=2.57 avg=2.57\n",
            "[8515 | 4251.83] loss=2.56 avg=2.57\n",
            "[8516 | 4252.32] loss=2.62 avg=2.57\n",
            "[8517 | 4252.81] loss=2.45 avg=2.57\n",
            "[8518 | 4253.29] loss=2.59 avg=2.57\n",
            "[8519 | 4253.78] loss=2.71 avg=2.57\n",
            "[8520 | 4254.27] loss=2.76 avg=2.57\n",
            "[8521 | 4254.76] loss=2.76 avg=2.58\n",
            "[8522 | 4255.24] loss=2.35 avg=2.57\n",
            "[8523 | 4255.73] loss=2.19 avg=2.57\n",
            "[8524 | 4256.22] loss=2.44 avg=2.57\n",
            "[8525 | 4256.70] loss=2.59 avg=2.57\n",
            "[8526 | 4257.19] loss=2.95 avg=2.57\n",
            "[8527 | 4257.68] loss=2.72 avg=2.57\n",
            "[8528 | 4258.17] loss=2.57 avg=2.57\n",
            "[8529 | 4258.65] loss=2.82 avg=2.58\n",
            "[8530 | 4259.14] loss=2.66 avg=2.58\n",
            "[8531 | 4259.63] loss=2.64 avg=2.58\n",
            "[8532 | 4260.11] loss=2.53 avg=2.58\n",
            "[8533 | 4260.60] loss=2.40 avg=2.58\n",
            "[8534 | 4261.09] loss=2.18 avg=2.57\n",
            "[8535 | 4261.58] loss=2.53 avg=2.57\n",
            "[8536 | 4262.07] loss=2.59 avg=2.57\n",
            "[8537 | 4262.55] loss=2.72 avg=2.57\n",
            "[8538 | 4263.04] loss=2.67 avg=2.57\n",
            "[8539 | 4263.53] loss=2.58 avg=2.57\n",
            "[8540 | 4264.02] loss=2.48 avg=2.57\n",
            "[8541 | 4264.50] loss=2.74 avg=2.57\n",
            "[8542 | 4264.99] loss=2.56 avg=2.57\n",
            "[8543 | 4265.48] loss=2.66 avg=2.58\n",
            "[8544 | 4265.96] loss=2.51 avg=2.57\n",
            "[8545 | 4266.45] loss=2.69 avg=2.58\n",
            "[8546 | 4266.94] loss=2.39 avg=2.57\n",
            "[8547 | 4267.43] loss=2.85 avg=2.58\n",
            "[8548 | 4267.91] loss=2.86 avg=2.58\n",
            "[8549 | 4268.40] loss=2.66 avg=2.58\n",
            "[8550 | 4268.89] loss=2.66 avg=2.58\n",
            "[8551 | 4269.37] loss=2.88 avg=2.58\n",
            "[8552 | 4269.86] loss=2.46 avg=2.58\n",
            "[8553 | 4270.35] loss=2.99 avg=2.59\n",
            "[8554 | 4270.84] loss=2.76 avg=2.59\n",
            "[8555 | 4271.33] loss=2.37 avg=2.59\n",
            "[8556 | 4271.82] loss=2.85 avg=2.59\n",
            "[8557 | 4272.31] loss=2.27 avg=2.59\n",
            "[8558 | 4272.79] loss=2.60 avg=2.59\n",
            "[8559 | 4273.28] loss=2.76 avg=2.59\n",
            "[8560 | 4273.77] loss=2.57 avg=2.59\n",
            "[8561 | 4274.26] loss=2.74 avg=2.59\n",
            "[8562 | 4274.74] loss=2.53 avg=2.59\n",
            "[8563 | 4275.23] loss=1.67 avg=2.58\n",
            "[8564 | 4275.72] loss=2.92 avg=2.58\n",
            "[8565 | 4276.21] loss=2.65 avg=2.58\n",
            "[8566 | 4276.70] loss=2.15 avg=2.58\n",
            "[8567 | 4277.18] loss=3.11 avg=2.58\n",
            "[8568 | 4277.67] loss=2.60 avg=2.58\n",
            "[8569 | 4278.16] loss=2.56 avg=2.58\n",
            "[8570 | 4278.65] loss=2.77 avg=2.59\n",
            "[8571 | 4279.14] loss=2.43 avg=2.58\n",
            "[8572 | 4279.63] loss=2.73 avg=2.59\n",
            "[8573 | 4280.12] loss=2.33 avg=2.58\n",
            "[8574 | 4280.61] loss=2.59 avg=2.58\n",
            "[8575 | 4281.09] loss=2.71 avg=2.59\n",
            "[8576 | 4281.58] loss=2.95 avg=2.59\n",
            "[8577 | 4282.07] loss=2.56 avg=2.59\n",
            "[8578 | 4282.56] loss=2.56 avg=2.59\n",
            "[8579 | 4283.05] loss=2.61 avg=2.59\n",
            "[8580 | 4283.53] loss=1.68 avg=2.58\n",
            "[8581 | 4284.02] loss=2.57 avg=2.58\n",
            "[8582 | 4284.51] loss=2.90 avg=2.58\n",
            "[8583 | 4285.00] loss=2.64 avg=2.58\n",
            "[8584 | 4285.48] loss=2.62 avg=2.58\n",
            "[8585 | 4285.97] loss=2.45 avg=2.58\n",
            "[8586 | 4286.46] loss=1.58 avg=2.57\n",
            "[8587 | 4286.94] loss=2.40 avg=2.57\n",
            "[8588 | 4287.43] loss=2.19 avg=2.57\n",
            "[8589 | 4287.92] loss=2.45 avg=2.57\n",
            "[8590 | 4288.41] loss=2.51 avg=2.56\n",
            "[8591 | 4288.89] loss=2.66 avg=2.57\n",
            "[8592 | 4289.38] loss=2.42 avg=2.56\n",
            "[8593 | 4289.87] loss=2.58 avg=2.56\n",
            "[8594 | 4290.36] loss=2.44 avg=2.56\n",
            "[8595 | 4290.84] loss=2.64 avg=2.56\n",
            "[8596 | 4291.33] loss=2.53 avg=2.56\n",
            "[8597 | 4291.82] loss=2.43 avg=2.56\n",
            "[8598 | 4292.31] loss=2.67 avg=2.56\n",
            "[8599 | 4292.79] loss=2.61 avg=2.56\n",
            "[8600 | 4293.28] loss=2.58 avg=2.56\n",
            "[8601 | 4293.77] loss=2.44 avg=2.56\n",
            "[8602 | 4294.25] loss=2.58 avg=2.56\n",
            "[8603 | 4294.74] loss=2.95 avg=2.57\n",
            "[8604 | 4295.23] loss=2.34 avg=2.56\n",
            "[8605 | 4295.72] loss=2.29 avg=2.56\n",
            "[8606 | 4296.20] loss=2.45 avg=2.56\n",
            "[8607 | 4296.69] loss=2.94 avg=2.56\n",
            "[8608 | 4297.18] loss=2.64 avg=2.57\n",
            "[8609 | 4297.67] loss=2.70 avg=2.57\n",
            "[8610 | 4298.15] loss=2.72 avg=2.57\n",
            "[8611 | 4298.64] loss=2.64 avg=2.57\n",
            "[8612 | 4299.12] loss=2.96 avg=2.57\n",
            "[8613 | 4299.61] loss=2.85 avg=2.58\n",
            "[8614 | 4300.10] loss=2.55 avg=2.58\n",
            "[8615 | 4300.59] loss=2.53 avg=2.57\n",
            "[8616 | 4301.07] loss=2.30 avg=2.57\n",
            "[8617 | 4301.56] loss=2.68 avg=2.57\n",
            "[8618 | 4302.05] loss=2.61 avg=2.57\n",
            "[8619 | 4302.53] loss=2.56 avg=2.57\n",
            "[8620 | 4303.02] loss=2.66 avg=2.57\n",
            "[8621 | 4303.51] loss=2.79 avg=2.58\n",
            "[8622 | 4304.00] loss=2.68 avg=2.58\n",
            "[8623 | 4304.48] loss=2.39 avg=2.58\n",
            "[8624 | 4304.97] loss=2.99 avg=2.58\n",
            "[8625 | 4305.46] loss=2.38 avg=2.58\n",
            "[8626 | 4305.95] loss=2.65 avg=2.58\n",
            "[8627 | 4306.43] loss=2.39 avg=2.58\n",
            "[8628 | 4306.92] loss=2.60 avg=2.58\n",
            "[8629 | 4307.41] loss=2.29 avg=2.57\n",
            "[8630 | 4307.89] loss=2.87 avg=2.58\n",
            "[8631 | 4308.38] loss=2.81 avg=2.58\n",
            "[8632 | 4308.87] loss=2.76 avg=2.58\n",
            "[8633 | 4309.36] loss=2.55 avg=2.58\n",
            "[8634 | 4309.84] loss=2.79 avg=2.58\n",
            "[8635 | 4310.33] loss=2.67 avg=2.58\n",
            "[8636 | 4310.82] loss=2.68 avg=2.58\n",
            "[8637 | 4311.31] loss=2.75 avg=2.59\n",
            "[8638 | 4311.79] loss=2.89 avg=2.59\n",
            "[8639 | 4312.28] loss=2.61 avg=2.59\n",
            "[8640 | 4312.77] loss=2.49 avg=2.59\n",
            "[8641 | 4313.26] loss=2.99 avg=2.59\n",
            "[8642 | 4313.74] loss=2.88 avg=2.60\n",
            "[8643 | 4314.23] loss=2.44 avg=2.59\n",
            "[8644 | 4314.72] loss=2.65 avg=2.59\n",
            "[8645 | 4315.20] loss=2.79 avg=2.60\n",
            "[8646 | 4315.69] loss=2.60 avg=2.60\n",
            "[8647 | 4316.18] loss=2.44 avg=2.59\n",
            "[8648 | 4316.67] loss=2.39 avg=2.59\n",
            "[8649 | 4317.15] loss=2.82 avg=2.60\n",
            "[8650 | 4317.64] loss=2.11 avg=2.59\n",
            "[8651 | 4318.12] loss=2.32 avg=2.59\n",
            "[8652 | 4318.62] loss=2.33 avg=2.58\n",
            "[8653 | 4319.10] loss=2.35 avg=2.58\n",
            "[8654 | 4319.59] loss=2.47 avg=2.58\n",
            "[8655 | 4320.08] loss=2.02 avg=2.58\n",
            "[8656 | 4320.56] loss=2.68 avg=2.58\n",
            "[8657 | 4321.05] loss=2.45 avg=2.58\n",
            "[8658 | 4321.54] loss=2.66 avg=2.58\n",
            "[8659 | 4322.02] loss=2.59 avg=2.58\n",
            "[8660 | 4322.51] loss=2.50 avg=2.58\n",
            "[8661 | 4323.00] loss=2.79 avg=2.58\n",
            "[8662 | 4323.49] loss=2.75 avg=2.58\n",
            "[8663 | 4323.98] loss=2.58 avg=2.58\n",
            "[8664 | 4324.46] loss=1.70 avg=2.57\n",
            "[8665 | 4324.95] loss=2.64 avg=2.57\n",
            "[8666 | 4325.44] loss=2.65 avg=2.57\n",
            "[8667 | 4325.92] loss=2.61 avg=2.57\n",
            "[8668 | 4326.41] loss=2.53 avg=2.57\n",
            "[8669 | 4326.90] loss=2.69 avg=2.57\n",
            "[8670 | 4327.38] loss=2.79 avg=2.58\n",
            "[8671 | 4327.87] loss=2.60 avg=2.58\n",
            "[8672 | 4328.36] loss=2.51 avg=2.58\n",
            "[8673 | 4328.85] loss=2.39 avg=2.57\n",
            "[8674 | 4329.34] loss=2.20 avg=2.57\n",
            "[8675 | 4329.82] loss=2.48 avg=2.57\n",
            "[8676 | 4330.31] loss=2.49 avg=2.57\n",
            "[8677 | 4330.80] loss=2.88 avg=2.57\n",
            "[8678 | 4331.28] loss=2.50 avg=2.57\n",
            "[8679 | 4331.77] loss=2.30 avg=2.57\n",
            "[8680 | 4332.26] loss=2.56 avg=2.57\n",
            "[8681 | 4332.75] loss=2.15 avg=2.56\n",
            "[8682 | 4333.23] loss=2.48 avg=2.56\n",
            "[8683 | 4333.72] loss=2.57 avg=2.56\n",
            "[8684 | 4334.21] loss=2.38 avg=2.56\n",
            "[8685 | 4334.70] loss=2.74 avg=2.56\n",
            "[8686 | 4335.18] loss=2.55 avg=2.56\n",
            "[8687 | 4335.67] loss=3.11 avg=2.57\n",
            "[8688 | 4336.16] loss=2.75 avg=2.57\n",
            "[8689 | 4336.64] loss=2.85 avg=2.57\n",
            "[8690 | 4337.13] loss=2.94 avg=2.58\n",
            "[8691 | 4337.62] loss=2.81 avg=2.58\n",
            "[8692 | 4338.11] loss=2.57 avg=2.58\n",
            "[8693 | 4338.59] loss=2.89 avg=2.58\n",
            "[8694 | 4339.08] loss=2.35 avg=2.58\n",
            "[8695 | 4339.57] loss=2.90 avg=2.58\n",
            "[8696 | 4340.06] loss=2.46 avg=2.58\n",
            "[8697 | 4340.55] loss=2.44 avg=2.58\n",
            "[8698 | 4341.03] loss=2.65 avg=2.58\n",
            "[8699 | 4341.52] loss=2.56 avg=2.58\n",
            "[8700 | 4342.01] loss=2.33 avg=2.58\n",
            "[8701 | 4342.50] loss=2.45 avg=2.58\n",
            "[8702 | 4342.98] loss=2.47 avg=2.58\n",
            "[8703 | 4343.47] loss=2.66 avg=2.58\n",
            "[8704 | 4343.96] loss=2.44 avg=2.58\n",
            "[8705 | 4344.45] loss=2.69 avg=2.58\n",
            "[8706 | 4344.94] loss=2.67 avg=2.58\n",
            "[8707 | 4345.43] loss=2.89 avg=2.58\n",
            "[8708 | 4345.91] loss=2.48 avg=2.58\n",
            "[8709 | 4346.40] loss=2.67 avg=2.58\n",
            "[8710 | 4346.89] loss=2.66 avg=2.58\n",
            "[8711 | 4347.38] loss=2.81 avg=2.58\n",
            "[8712 | 4347.87] loss=2.82 avg=2.59\n",
            "[8713 | 4348.36] loss=2.64 avg=2.59\n",
            "[8714 | 4348.84] loss=2.82 avg=2.59\n",
            "[8715 | 4349.33] loss=2.78 avg=2.59\n",
            "[8716 | 4349.82] loss=2.73 avg=2.59\n",
            "[8717 | 4350.31] loss=2.61 avg=2.59\n",
            "[8718 | 4350.79] loss=2.80 avg=2.59\n",
            "[8719 | 4351.28] loss=2.56 avg=2.59\n",
            "[8720 | 4351.77] loss=2.51 avg=2.59\n",
            "[8721 | 4352.26] loss=2.55 avg=2.59\n",
            "[8722 | 4352.75] loss=2.27 avg=2.59\n",
            "[8723 | 4353.24] loss=2.51 avg=2.59\n",
            "[8724 | 4353.72] loss=2.61 avg=2.59\n",
            "[8725 | 4354.21] loss=2.87 avg=2.59\n",
            "[8726 | 4354.70] loss=2.66 avg=2.59\n",
            "[8727 | 4355.19] loss=2.78 avg=2.59\n",
            "[8728 | 4355.67] loss=2.67 avg=2.59\n",
            "[8729 | 4356.16] loss=2.59 avg=2.59\n",
            "[8730 | 4356.65] loss=2.70 avg=2.60\n",
            "[8731 | 4357.14] loss=2.45 avg=2.59\n",
            "[8732 | 4357.63] loss=2.65 avg=2.59\n",
            "[8733 | 4358.12] loss=3.01 avg=2.60\n",
            "[8734 | 4358.61] loss=2.63 avg=2.60\n",
            "[8735 | 4359.10] loss=2.97 avg=2.60\n",
            "[8736 | 4359.59] loss=2.71 avg=2.60\n",
            "[8737 | 4360.08] loss=2.22 avg=2.60\n",
            "[8738 | 4360.57] loss=2.60 avg=2.60\n",
            "[8739 | 4361.05] loss=3.00 avg=2.60\n",
            "[8740 | 4361.54] loss=2.51 avg=2.60\n",
            "[8741 | 4362.02] loss=2.26 avg=2.60\n",
            "[8742 | 4362.51] loss=2.69 avg=2.60\n",
            "[8743 | 4363.00] loss=2.44 avg=2.60\n",
            "[8744 | 4363.49] loss=2.58 avg=2.60\n",
            "[8745 | 4363.98] loss=2.72 avg=2.60\n",
            "[8746 | 4364.46] loss=2.44 avg=2.60\n",
            "[8747 | 4364.95] loss=2.64 avg=2.60\n",
            "[8748 | 4365.44] loss=2.64 avg=2.60\n",
            "[8749 | 4365.93] loss=2.15 avg=2.60\n",
            "[8750 | 4366.41] loss=2.67 avg=2.60\n",
            "[8751 | 4366.90] loss=2.62 avg=2.60\n",
            "[8752 | 4367.39] loss=2.51 avg=2.60\n",
            "[8753 | 4367.88] loss=2.68 avg=2.60\n",
            "[8754 | 4368.36] loss=2.71 avg=2.60\n",
            "[8755 | 4368.85] loss=2.52 avg=2.60\n",
            "[8756 | 4369.34] loss=2.71 avg=2.60\n",
            "[8757 | 4369.83] loss=2.33 avg=2.59\n",
            "[8758 | 4370.31] loss=2.68 avg=2.60\n",
            "[8759 | 4370.80] loss=2.19 avg=2.59\n",
            "[8760 | 4371.28] loss=2.46 avg=2.59\n",
            "[8761 | 4371.77] loss=2.71 avg=2.59\n",
            "[8762 | 4372.26] loss=2.73 avg=2.59\n",
            "[8763 | 4372.75] loss=2.51 avg=2.59\n",
            "[8764 | 4373.23] loss=2.65 avg=2.59\n",
            "[8765 | 4373.72] loss=2.74 avg=2.59\n",
            "[8766 | 4374.21] loss=2.85 avg=2.60\n",
            "[8767 | 4374.70] loss=3.00 avg=2.60\n",
            "[8768 | 4375.18] loss=2.57 avg=2.60\n",
            "[8769 | 4375.67] loss=2.06 avg=2.60\n",
            "[8770 | 4376.16] loss=2.58 avg=2.59\n",
            "[8771 | 4376.64] loss=2.54 avg=2.59\n",
            "[8772 | 4377.13] loss=2.35 avg=2.59\n",
            "[8773 | 4377.62] loss=2.26 avg=2.59\n",
            "[8774 | 4378.11] loss=1.50 avg=2.58\n",
            "[8775 | 4378.59] loss=2.72 avg=2.58\n",
            "[8776 | 4379.08] loss=2.30 avg=2.58\n",
            "[8777 | 4379.56] loss=2.81 avg=2.58\n",
            "[8778 | 4380.06] loss=2.64 avg=2.58\n",
            "[8779 | 4380.54] loss=2.70 avg=2.58\n",
            "[8780 | 4381.03] loss=2.61 avg=2.58\n",
            "[8781 | 4381.51] loss=3.00 avg=2.58\n",
            "[8782 | 4382.00] loss=2.54 avg=2.58\n",
            "[8783 | 4382.49] loss=2.53 avg=2.58\n",
            "[8784 | 4382.98] loss=2.70 avg=2.59\n",
            "[8785 | 4383.47] loss=2.91 avg=2.59\n",
            "[8786 | 4383.95] loss=2.43 avg=2.59\n",
            "[8787 | 4384.44] loss=2.33 avg=2.58\n",
            "[8788 | 4384.92] loss=2.09 avg=2.58\n",
            "[8789 | 4385.42] loss=2.77 avg=2.58\n",
            "[8790 | 4385.90] loss=2.59 avg=2.58\n",
            "[8791 | 4386.39] loss=2.52 avg=2.58\n",
            "[8792 | 4386.88] loss=2.42 avg=2.58\n",
            "[8793 | 4387.36] loss=2.75 avg=2.58\n",
            "[8794 | 4387.85] loss=2.56 avg=2.58\n",
            "[8795 | 4388.34] loss=2.61 avg=2.58\n",
            "[8796 | 4388.82] loss=1.58 avg=2.57\n",
            "[8797 | 4389.31] loss=2.58 avg=2.57\n",
            "[8798 | 4389.80] loss=2.59 avg=2.57\n",
            "[8799 | 4390.29] loss=1.05 avg=2.56\n",
            "[8800 | 4390.77] loss=2.20 avg=2.55\n",
            "[8801 | 4391.26] loss=2.56 avg=2.55\n",
            "[8802 | 4391.75] loss=2.31 avg=2.55\n",
            "[8803 | 4392.23] loss=2.88 avg=2.55\n",
            "[8804 | 4392.72] loss=2.68 avg=2.55\n",
            "[8805 | 4393.21] loss=2.59 avg=2.56\n",
            "[8806 | 4393.69] loss=2.59 avg=2.56\n",
            "[8807 | 4394.18] loss=2.36 avg=2.55\n",
            "[8808 | 4394.67] loss=2.57 avg=2.55\n",
            "[8809 | 4395.15] loss=2.63 avg=2.55\n",
            "[8810 | 4395.64] loss=2.59 avg=2.55\n",
            "[8811 | 4396.13] loss=2.82 avg=2.56\n",
            "[8812 | 4396.61] loss=2.55 avg=2.56\n",
            "[8813 | 4397.10] loss=2.28 avg=2.55\n",
            "[8814 | 4397.59] loss=2.66 avg=2.56\n",
            "[8815 | 4398.07] loss=2.62 avg=2.56\n",
            "[8816 | 4398.56] loss=2.28 avg=2.55\n",
            "[8817 | 4399.05] loss=2.87 avg=2.56\n",
            "[8818 | 4399.54] loss=2.39 avg=2.55\n",
            "[8819 | 4400.02] loss=2.63 avg=2.56\n",
            "[8820 | 4400.51] loss=2.72 avg=2.56\n",
            "[8821 | 4400.99] loss=2.72 avg=2.56\n",
            "[8822 | 4401.48] loss=2.64 avg=2.56\n",
            "[8823 | 4401.97] loss=2.35 avg=2.56\n",
            "[8824 | 4402.46] loss=2.73 avg=2.56\n",
            "[8825 | 4402.95] loss=2.53 avg=2.56\n",
            "[8826 | 4403.44] loss=2.74 avg=2.56\n",
            "[8827 | 4403.92] loss=2.82 avg=2.56\n",
            "[8828 | 4404.41] loss=2.51 avg=2.56\n",
            "[8829 | 4404.90] loss=2.86 avg=2.57\n",
            "[8830 | 4405.39] loss=2.97 avg=2.57\n",
            "[8831 | 4405.87] loss=2.79 avg=2.57\n",
            "[8832 | 4406.36] loss=2.36 avg=2.57\n",
            "[8833 | 4406.85] loss=2.67 avg=2.57\n",
            "[8834 | 4407.34] loss=2.88 avg=2.57\n",
            "[8835 | 4407.83] loss=2.23 avg=2.57\n",
            "[8836 | 4408.31] loss=2.99 avg=2.57\n",
            "[8837 | 4408.80] loss=2.26 avg=2.57\n",
            "[8838 | 4409.29] loss=2.37 avg=2.57\n",
            "[8839 | 4409.78] loss=2.49 avg=2.57\n",
            "[8840 | 4410.27] loss=2.58 avg=2.57\n",
            "[8841 | 4410.75] loss=2.49 avg=2.57\n",
            "[8842 | 4411.24] loss=2.66 avg=2.57\n",
            "[8843 | 4411.73] loss=2.66 avg=2.57\n",
            "[8844 | 4412.22] loss=2.65 avg=2.57\n",
            "[8845 | 4412.71] loss=2.34 avg=2.57\n",
            "[8846 | 4413.19] loss=2.77 avg=2.57\n",
            "[8847 | 4413.68] loss=2.77 avg=2.57\n",
            "[8848 | 4414.17] loss=2.28 avg=2.57\n",
            "[8849 | 4414.66] loss=2.69 avg=2.57\n",
            "[8850 | 4415.15] loss=2.77 avg=2.57\n",
            "[8851 | 4415.63] loss=2.83 avg=2.58\n",
            "[8852 | 4416.12] loss=2.72 avg=2.58\n",
            "[8853 | 4416.61] loss=2.54 avg=2.58\n",
            "[8854 | 4417.10] loss=2.64 avg=2.58\n",
            "[8855 | 4417.59] loss=2.48 avg=2.58\n",
            "[8856 | 4418.07] loss=2.47 avg=2.58\n",
            "[8857 | 4418.56] loss=2.67 avg=2.58\n",
            "[8858 | 4419.05] loss=2.49 avg=2.58\n",
            "[8859 | 4419.54] loss=2.43 avg=2.57\n",
            "[8860 | 4420.03] loss=2.54 avg=2.57\n",
            "[8861 | 4420.52] loss=2.55 avg=2.57\n",
            "[8862 | 4421.00] loss=2.57 avg=2.57\n",
            "[8863 | 4421.49] loss=2.15 avg=2.57\n",
            "[8864 | 4421.98] loss=2.59 avg=2.57\n",
            "[8865 | 4422.47] loss=2.72 avg=2.57\n",
            "[8866 | 4422.96] loss=2.43 avg=2.57\n",
            "[8867 | 4423.44] loss=2.49 avg=2.57\n",
            "[8868 | 4423.93] loss=2.68 avg=2.57\n",
            "[8869 | 4424.42] loss=2.31 avg=2.57\n",
            "[8870 | 4424.91] loss=2.87 avg=2.57\n",
            "[8871 | 4425.39] loss=2.55 avg=2.57\n",
            "[8872 | 4425.88] loss=2.62 avg=2.57\n",
            "[8873 | 4426.37] loss=2.72 avg=2.57\n",
            "[8874 | 4426.86] loss=2.94 avg=2.58\n",
            "[8875 | 4427.34] loss=2.92 avg=2.58\n",
            "[8876 | 4427.83] loss=3.17 avg=2.58\n",
            "[8877 | 4428.32] loss=2.68 avg=2.59\n",
            "[8878 | 4428.80] loss=2.68 avg=2.59\n",
            "[8879 | 4429.29] loss=2.35 avg=2.58\n",
            "[8880 | 4429.78] loss=2.48 avg=2.58\n",
            "[8881 | 4430.26] loss=2.84 avg=2.59\n",
            "[8882 | 4430.75] loss=2.49 avg=2.58\n",
            "[8883 | 4431.24] loss=2.98 avg=2.59\n",
            "[8884 | 4431.73] loss=2.63 avg=2.59\n",
            "[8885 | 4432.21] loss=2.80 avg=2.59\n",
            "[8886 | 4432.70] loss=2.63 avg=2.59\n",
            "[8887 | 4433.19] loss=2.58 avg=2.59\n",
            "[8888 | 4433.68] loss=2.49 avg=2.59\n",
            "[8889 | 4434.16] loss=2.88 avg=2.59\n",
            "[8890 | 4434.65] loss=2.45 avg=2.59\n",
            "[8891 | 4435.14] loss=2.64 avg=2.59\n",
            "[8892 | 4435.63] loss=2.69 avg=2.59\n",
            "[8893 | 4436.11] loss=2.74 avg=2.59\n",
            "[8894 | 4436.60] loss=2.46 avg=2.59\n",
            "[8895 | 4437.09] loss=2.76 avg=2.60\n",
            "[8896 | 4437.57] loss=2.91 avg=2.60\n",
            "[8897 | 4438.06] loss=2.79 avg=2.60\n",
            "[8898 | 4438.55] loss=2.88 avg=2.60\n",
            "[8899 | 4439.03] loss=2.68 avg=2.60\n",
            "[8900 | 4439.52] loss=2.90 avg=2.61\n",
            "[8901 | 4440.01] loss=2.76 avg=2.61\n",
            "[8902 | 4440.49] loss=3.02 avg=2.61\n",
            "[8903 | 4440.98] loss=2.72 avg=2.61\n",
            "[8904 | 4441.47] loss=2.39 avg=2.61\n",
            "[8905 | 4441.95] loss=3.25 avg=2.62\n",
            "[8906 | 4442.44] loss=2.55 avg=2.62\n",
            "[8907 | 4442.93] loss=2.56 avg=2.62\n",
            "[8908 | 4443.42] loss=2.47 avg=2.61\n",
            "[8909 | 4443.90] loss=2.62 avg=2.61\n",
            "[8910 | 4444.39] loss=2.53 avg=2.61\n",
            "[8911 | 4444.88] loss=2.70 avg=2.61\n",
            "[8912 | 4445.37] loss=2.65 avg=2.62\n",
            "[8913 | 4445.85] loss=2.70 avg=2.62\n",
            "[8914 | 4446.34] loss=2.48 avg=2.61\n",
            "[8915 | 4446.83] loss=2.48 avg=2.61\n",
            "[8916 | 4447.32] loss=2.32 avg=2.61\n",
            "[8917 | 4447.80] loss=2.53 avg=2.61\n",
            "[8918 | 4448.28] loss=2.77 avg=2.61\n",
            "[8919 | 4448.77] loss=2.76 avg=2.61\n",
            "[8920 | 4449.26] loss=2.60 avg=2.61\n",
            "[8921 | 4449.75] loss=2.34 avg=2.61\n",
            "[8922 | 4450.23] loss=2.67 avg=2.61\n",
            "[8923 | 4450.72] loss=2.71 avg=2.61\n",
            "[8924 | 4451.21] loss=2.26 avg=2.61\n",
            "[8925 | 4451.70] loss=2.68 avg=2.61\n",
            "[8926 | 4452.18] loss=2.58 avg=2.61\n",
            "[8927 | 4452.67] loss=2.47 avg=2.61\n",
            "[8928 | 4453.16] loss=2.75 avg=2.61\n",
            "[8929 | 4453.64] loss=2.72 avg=2.61\n",
            "[8930 | 4454.13] loss=2.48 avg=2.61\n",
            "[8931 | 4454.62] loss=2.75 avg=2.61\n",
            "[8932 | 4455.11] loss=2.78 avg=2.61\n",
            "[8933 | 4455.59] loss=2.62 avg=2.61\n",
            "[8934 | 4456.08] loss=2.43 avg=2.61\n",
            "[8935 | 4456.57] loss=2.61 avg=2.61\n",
            "[8936 | 4457.06] loss=2.82 avg=2.61\n",
            "[8937 | 4457.54] loss=2.59 avg=2.61\n",
            "[8938 | 4458.03] loss=3.19 avg=2.62\n",
            "[8939 | 4458.52] loss=2.91 avg=2.62\n",
            "[8940 | 4459.01] loss=2.46 avg=2.62\n",
            "[8941 | 4459.49] loss=2.46 avg=2.62\n",
            "[8942 | 4459.98] loss=2.16 avg=2.61\n",
            "[8943 | 4460.47] loss=2.66 avg=2.61\n",
            "[8944 | 4460.96] loss=2.58 avg=2.61\n",
            "[8945 | 4461.44] loss=2.65 avg=2.61\n",
            "[8946 | 4461.93] loss=2.51 avg=2.61\n",
            "[8947 | 4462.42] loss=2.81 avg=2.61\n",
            "[8948 | 4462.90] loss=2.46 avg=2.61\n",
            "[8949 | 4463.39] loss=2.44 avg=2.61\n",
            "[8950 | 4463.88] loss=2.56 avg=2.61\n",
            "[8951 | 4464.37] loss=2.80 avg=2.61\n",
            "[8952 | 4464.86] loss=2.38 avg=2.61\n",
            "[8953 | 4465.35] loss=3.00 avg=2.61\n",
            "[8954 | 4465.84] loss=2.49 avg=2.61\n",
            "[8955 | 4466.32] loss=2.58 avg=2.61\n",
            "[8956 | 4466.81] loss=2.25 avg=2.61\n",
            "[8957 | 4467.30] loss=2.53 avg=2.61\n",
            "[8958 | 4467.78] loss=2.74 avg=2.61\n",
            "[8959 | 4468.27] loss=2.84 avg=2.61\n",
            "[8960 | 4468.75] loss=2.30 avg=2.61\n",
            "[8961 | 4469.24] loss=2.54 avg=2.61\n",
            "[8962 | 4469.73] loss=2.43 avg=2.61\n",
            "[8963 | 4470.22] loss=2.12 avg=2.60\n",
            "[8964 | 4470.70] loss=2.63 avg=2.60\n",
            "[8965 | 4471.19] loss=2.84 avg=2.60\n",
            "[8966 | 4471.68] loss=2.33 avg=2.60\n",
            "[8967 | 4472.16] loss=2.50 avg=2.60\n",
            "[8968 | 4472.65] loss=2.94 avg=2.60\n",
            "[8969 | 4473.14] loss=2.29 avg=2.60\n",
            "[8970 | 4473.63] loss=2.58 avg=2.60\n",
            "[8971 | 4474.11] loss=2.82 avg=2.60\n",
            "[8972 | 4474.60] loss=2.34 avg=2.60\n",
            "[8973 | 4475.09] loss=2.19 avg=2.60\n",
            "[8974 | 4475.58] loss=2.80 avg=2.60\n",
            "[8975 | 4476.06] loss=2.49 avg=2.60\n",
            "[8976 | 4476.55] loss=2.66 avg=2.60\n",
            "[8977 | 4477.04] loss=2.68 avg=2.60\n",
            "[8978 | 4477.53] loss=2.33 avg=2.59\n",
            "[8979 | 4478.02] loss=2.48 avg=2.59\n",
            "[8980 | 4478.51] loss=2.86 avg=2.60\n",
            "[8981 | 4479.00] loss=2.68 avg=2.60\n",
            "[8982 | 4479.48] loss=1.47 avg=2.59\n",
            "[8983 | 4479.97] loss=2.49 avg=2.59\n",
            "[8984 | 4480.46] loss=2.59 avg=2.59\n",
            "[8985 | 4480.95] loss=2.52 avg=2.58\n",
            "[8986 | 4481.43] loss=2.75 avg=2.59\n",
            "[8987 | 4481.92] loss=2.73 avg=2.59\n",
            "[8988 | 4482.41] loss=2.66 avg=2.59\n",
            "[8989 | 4482.90] loss=2.75 avg=2.59\n",
            "[8990 | 4483.38] loss=3.00 avg=2.59\n",
            "[8991 | 4483.87] loss=2.24 avg=2.59\n",
            "[8992 | 4484.36] loss=2.70 avg=2.59\n",
            "[8993 | 4484.85] loss=2.78 avg=2.59\n",
            "[8994 | 4485.34] loss=2.17 avg=2.59\n",
            "[8995 | 4485.83] loss=2.58 avg=2.59\n",
            "[8996 | 4486.32] loss=2.70 avg=2.59\n",
            "[8997 | 4486.80] loss=2.76 avg=2.59\n",
            "[8998 | 4487.29] loss=2.56 avg=2.59\n",
            "[8999 | 4487.78] loss=2.39 avg=2.59\n",
            "Saving checkpoint/run1/model-9000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " and the \n",
            "Page 5\n",
            "\n",
            "solar winds. The sun was rising, and the moon was rising. The wind was blowing, and the moon was \n",
            "rising. \n",
            "\"The sun is rising,\" he said. \"The moon is rising.\" \n",
            "\"The moon is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The moon is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising.\" \n",
            "\"The sun is rising,\" said the man. \"The sun is rising\n",
            "\n",
            "[9000 | 4501.34] loss=2.35 avg=2.59\n",
            "[9001 | 4501.82] loss=2.54 avg=2.59\n",
            "[9002 | 4502.31] loss=2.23 avg=2.58\n",
            "[9003 | 4502.80] loss=2.45 avg=2.58\n",
            "[9004 | 4503.29] loss=2.94 avg=2.59\n",
            "[9005 | 4503.77] loss=2.77 avg=2.59\n",
            "[9006 | 4504.26] loss=2.67 avg=2.59\n",
            "[9007 | 4504.75] loss=2.42 avg=2.59\n",
            "[9008 | 4505.23] loss=2.79 avg=2.59\n",
            "[9009 | 4505.73] loss=3.05 avg=2.59\n",
            "[9010 | 4506.21] loss=2.85 avg=2.60\n",
            "[9011 | 4506.70] loss=2.75 avg=2.60\n",
            "[9012 | 4507.19] loss=2.33 avg=2.59\n",
            "[9013 | 4507.68] loss=2.46 avg=2.59\n",
            "[9014 | 4508.16] loss=2.56 avg=2.59\n",
            "[9015 | 4508.65] loss=2.69 avg=2.59\n",
            "[9016 | 4509.13] loss=2.72 avg=2.59\n",
            "[9017 | 4509.62] loss=2.37 avg=2.59\n",
            "[9018 | 4510.11] loss=2.54 avg=2.59\n",
            "[9019 | 4510.59] loss=2.24 avg=2.59\n",
            "[9020 | 4511.08] loss=2.85 avg=2.59\n",
            "[9021 | 4511.57] loss=2.47 avg=2.59\n",
            "[9022 | 4512.06] loss=2.57 avg=2.59\n",
            "[9023 | 4512.54] loss=2.50 avg=2.59\n",
            "[9024 | 4513.03] loss=2.55 avg=2.59\n",
            "[9025 | 4513.51] loss=2.10 avg=2.58\n",
            "[9026 | 4514.00] loss=2.63 avg=2.58\n",
            "[9027 | 4514.49] loss=2.47 avg=2.58\n",
            "[9028 | 4514.98] loss=2.35 avg=2.58\n",
            "[9029 | 4515.46] loss=2.27 avg=2.58\n",
            "[9030 | 4515.95] loss=2.27 avg=2.57\n",
            "[9031 | 4516.44] loss=2.83 avg=2.58\n",
            "[9032 | 4516.93] loss=2.78 avg=2.58\n",
            "[9033 | 4517.42] loss=2.46 avg=2.58\n",
            "[9034 | 4517.90] loss=2.64 avg=2.58\n",
            "[9035 | 4518.39] loss=2.63 avg=2.58\n",
            "[9036 | 4518.88] loss=2.49 avg=2.58\n",
            "[9037 | 4519.36] loss=2.68 avg=2.58\n",
            "[9038 | 4519.85] loss=2.91 avg=2.58\n",
            "[9039 | 4520.34] loss=2.64 avg=2.58\n",
            "[9040 | 4520.83] loss=2.62 avg=2.58\n",
            "[9041 | 4521.32] loss=2.46 avg=2.58\n",
            "[9042 | 4521.81] loss=2.83 avg=2.58\n",
            "[9043 | 4522.31] loss=2.52 avg=2.58\n",
            "[9044 | 4522.80] loss=2.71 avg=2.59\n",
            "[9045 | 4523.28] loss=2.69 avg=2.59\n",
            "[9046 | 4523.77] loss=2.94 avg=2.59\n",
            "[9047 | 4524.26] loss=2.75 avg=2.59\n",
            "[9048 | 4524.75] loss=2.46 avg=2.59\n",
            "[9049 | 4525.23] loss=2.63 avg=2.59\n",
            "[9050 | 4525.72] loss=2.66 avg=2.59\n",
            "[9051 | 4526.22] loss=2.78 avg=2.59\n",
            "[9052 | 4526.71] loss=2.91 avg=2.60\n",
            "[9053 | 4527.20] loss=2.55 avg=2.60\n",
            "[9054 | 4527.69] loss=2.83 avg=2.60\n",
            "[9055 | 4528.17] loss=2.49 avg=2.60\n",
            "[9056 | 4528.66] loss=2.65 avg=2.60\n",
            "[9057 | 4529.15] loss=2.50 avg=2.60\n",
            "[9058 | 4529.63] loss=2.77 avg=2.60\n",
            "[9059 | 4530.12] loss=2.57 avg=2.60\n",
            "[9060 | 4530.61] loss=2.92 avg=2.60\n",
            "[9061 | 4531.09] loss=2.47 avg=2.60\n",
            "[9062 | 4531.58] loss=2.51 avg=2.60\n",
            "[9063 | 4532.07] loss=2.15 avg=2.59\n",
            "[9064 | 4532.56] loss=2.32 avg=2.59\n",
            "[9065 | 4533.04] loss=2.82 avg=2.59\n",
            "[9066 | 4533.53] loss=2.56 avg=2.59\n",
            "[9067 | 4534.02] loss=2.84 avg=2.60\n",
            "[9068 | 4534.50] loss=2.60 avg=2.60\n",
            "[9069 | 4534.99] loss=2.74 avg=2.60\n",
            "[9070 | 4535.48] loss=2.94 avg=2.60\n",
            "[9071 | 4535.97] loss=2.30 avg=2.60\n",
            "[9072 | 4536.45] loss=2.24 avg=2.59\n",
            "[9073 | 4536.94] loss=2.43 avg=2.59\n",
            "[9074 | 4537.43] loss=2.43 avg=2.59\n",
            "[9075 | 4537.92] loss=2.73 avg=2.59\n",
            "[9076 | 4538.40] loss=2.78 avg=2.59\n",
            "[9077 | 4538.89] loss=2.32 avg=2.59\n",
            "[9078 | 4539.38] loss=2.37 avg=2.59\n",
            "[9079 | 4539.87] loss=2.99 avg=2.59\n",
            "[9080 | 4540.35] loss=2.64 avg=2.59\n",
            "[9081 | 4540.84] loss=2.56 avg=2.59\n",
            "[9082 | 4541.33] loss=2.65 avg=2.59\n",
            "[9083 | 4541.82] loss=2.64 avg=2.59\n",
            "[9084 | 4542.30] loss=2.64 avg=2.60\n",
            "[9085 | 4542.79] loss=2.69 avg=2.60\n",
            "[9086 | 4543.27] loss=2.71 avg=2.60\n",
            "[9087 | 4543.77] loss=2.58 avg=2.60\n",
            "[9088 | 4544.25] loss=2.45 avg=2.60\n",
            "[9089 | 4544.74] loss=2.54 avg=2.59\n",
            "[9090 | 4545.23] loss=2.27 avg=2.59\n",
            "[9091 | 4545.72] loss=2.85 avg=2.59\n",
            "[9092 | 4546.21] loss=1.28 avg=2.58\n",
            "[9093 | 4546.70] loss=2.46 avg=2.58\n",
            "[9094 | 4547.18] loss=2.61 avg=2.58\n",
            "[9095 | 4547.67] loss=2.53 avg=2.58\n",
            "[9096 | 4548.16] loss=2.63 avg=2.58\n",
            "[9097 | 4548.65] loss=2.98 avg=2.58\n",
            "[9098 | 4549.14] loss=2.45 avg=2.58\n",
            "[9099 | 4549.62] loss=2.61 avg=2.58\n",
            "[9100 | 4550.11] loss=2.74 avg=2.58\n",
            "[9101 | 4550.60] loss=2.54 avg=2.58\n",
            "[9102 | 4551.09] loss=2.80 avg=2.59\n",
            "[9103 | 4551.58] loss=2.81 avg=2.59\n",
            "[9104 | 4552.07] loss=2.38 avg=2.59\n",
            "[9105 | 4552.55] loss=2.96 avg=2.59\n",
            "[9106 | 4553.04] loss=2.53 avg=2.59\n",
            "[9107 | 4553.53] loss=2.27 avg=2.59\n",
            "[9108 | 4554.02] loss=2.62 avg=2.59\n",
            "[9109 | 4554.50] loss=2.51 avg=2.59\n",
            "[9110 | 4554.99] loss=2.60 avg=2.59\n",
            "[9111 | 4555.48] loss=2.58 avg=2.59\n",
            "[9112 | 4555.97] loss=2.52 avg=2.59\n",
            "[9113 | 4556.45] loss=2.67 avg=2.59\n",
            "[9114 | 4556.94] loss=2.92 avg=2.59\n",
            "[9115 | 4557.43] loss=2.76 avg=2.59\n",
            "[9116 | 4557.92] loss=2.15 avg=2.59\n",
            "[9117 | 4558.41] loss=2.52 avg=2.59\n",
            "[9118 | 4558.89] loss=2.24 avg=2.58\n",
            "[9119 | 4559.38] loss=2.55 avg=2.58\n",
            "[9120 | 4559.87] loss=2.67 avg=2.58\n",
            "[9121 | 4560.35] loss=2.43 avg=2.58\n",
            "[9122 | 4560.84] loss=2.35 avg=2.58\n",
            "[9123 | 4561.33] loss=2.82 avg=2.58\n",
            "[9124 | 4561.81] loss=2.79 avg=2.58\n",
            "[9125 | 4562.30] loss=2.47 avg=2.58\n",
            "[9126 | 4562.79] loss=2.68 avg=2.58\n",
            "[9127 | 4563.28] loss=2.14 avg=2.58\n",
            "[9128 | 4563.76] loss=2.68 avg=2.58\n",
            "[9129 | 4564.25] loss=2.34 avg=2.58\n",
            "[9130 | 4564.74] loss=2.36 avg=2.58\n",
            "[9131 | 4565.22] loss=2.37 avg=2.57\n",
            "[9132 | 4565.71] loss=2.63 avg=2.57\n",
            "[9133 | 4566.20] loss=2.57 avg=2.57\n",
            "[9134 | 4566.69] loss=2.78 avg=2.58\n",
            "[9135 | 4567.18] loss=2.21 avg=2.57\n",
            "[9136 | 4567.66] loss=2.65 avg=2.57\n",
            "[9137 | 4568.15] loss=2.19 avg=2.57\n",
            "[9138 | 4568.63] loss=2.58 avg=2.57\n",
            "[9139 | 4569.12] loss=2.32 avg=2.57\n",
            "[9140 | 4569.61] loss=2.79 avg=2.57\n",
            "[9141 | 4570.10] loss=2.44 avg=2.57\n",
            "[9142 | 4570.58] loss=2.89 avg=2.57\n",
            "[9143 | 4571.07] loss=1.61 avg=2.56\n",
            "[9144 | 4571.56] loss=2.55 avg=2.56\n",
            "[9145 | 4572.04] loss=2.81 avg=2.56\n",
            "[9146 | 4572.53] loss=2.29 avg=2.56\n",
            "[9147 | 4573.01] loss=2.94 avg=2.57\n",
            "[9148 | 4573.51] loss=2.67 avg=2.57\n",
            "[9149 | 4573.99] loss=2.54 avg=2.57\n",
            "[9150 | 4574.48] loss=2.48 avg=2.57\n",
            "[9151 | 4574.97] loss=2.69 avg=2.57\n",
            "[9152 | 4575.45] loss=2.61 avg=2.57\n",
            "[9153 | 4575.94] loss=2.67 avg=2.57\n",
            "[9154 | 4576.43] loss=2.60 avg=2.57\n",
            "[9155 | 4576.92] loss=2.66 avg=2.57\n",
            "[9156 | 4577.40] loss=2.92 avg=2.57\n",
            "[9157 | 4577.89] loss=2.47 avg=2.57\n",
            "[9158 | 4578.38] loss=2.80 avg=2.57\n",
            "[9159 | 4578.87] loss=2.34 avg=2.57\n",
            "[9160 | 4579.35] loss=2.46 avg=2.57\n",
            "[9161 | 4579.84] loss=2.34 avg=2.57\n",
            "[9162 | 4580.33] loss=2.60 avg=2.57\n",
            "[9163 | 4580.81] loss=2.93 avg=2.57\n",
            "[9164 | 4581.30] loss=2.48 avg=2.57\n",
            "[9165 | 4581.79] loss=2.23 avg=2.57\n",
            "[9166 | 4582.28] loss=3.10 avg=2.57\n",
            "[9167 | 4582.76] loss=2.56 avg=2.57\n",
            "[9168 | 4583.25] loss=2.68 avg=2.57\n",
            "[9169 | 4583.74] loss=2.36 avg=2.57\n",
            "[9170 | 4584.23] loss=3.00 avg=2.58\n",
            "[9171 | 4584.71] loss=2.41 avg=2.57\n",
            "[9172 | 4585.20] loss=2.59 avg=2.57\n",
            "[9173 | 4585.69] loss=2.35 avg=2.57\n",
            "[9174 | 4586.17] loss=2.70 avg=2.57\n",
            "[9175 | 4586.66] loss=2.44 avg=2.57\n",
            "[9176 | 4587.15] loss=2.67 avg=2.57\n",
            "[9177 | 4587.64] loss=2.37 avg=2.57\n",
            "[9178 | 4588.12] loss=2.49 avg=2.57\n",
            "[9179 | 4588.61] loss=2.55 avg=2.57\n",
            "[9180 | 4589.10] loss=2.49 avg=2.57\n",
            "[9181 | 4589.59] loss=2.77 avg=2.57\n",
            "[9182 | 4590.07] loss=2.45 avg=2.57\n",
            "[9183 | 4590.56] loss=2.56 avg=2.57\n",
            "[9184 | 4591.05] loss=2.55 avg=2.57\n",
            "[9185 | 4591.53] loss=2.72 avg=2.57\n",
            "[9186 | 4592.02] loss=2.71 avg=2.57\n",
            "[9187 | 4592.51] loss=3.04 avg=2.58\n",
            "[9188 | 4593.00] loss=2.70 avg=2.58\n",
            "[9189 | 4593.48] loss=2.39 avg=2.58\n",
            "[9190 | 4593.97] loss=2.62 avg=2.58\n",
            "[9191 | 4594.46] loss=2.73 avg=2.58\n",
            "[9192 | 4594.95] loss=2.67 avg=2.58\n",
            "[9193 | 4595.43] loss=2.73 avg=2.58\n",
            "[9194 | 4595.92] loss=2.53 avg=2.58\n",
            "[9195 | 4596.41] loss=2.71 avg=2.58\n",
            "[9196 | 4596.89] loss=2.69 avg=2.58\n",
            "[9197 | 4597.38] loss=2.18 avg=2.58\n",
            "[9198 | 4597.87] loss=2.47 avg=2.58\n",
            "[9199 | 4598.36] loss=2.46 avg=2.58\n",
            "[9200 | 4598.84] loss=2.41 avg=2.58\n",
            "[9201 | 4599.33] loss=2.86 avg=2.58\n",
            "[9202 | 4599.82] loss=2.55 avg=2.58\n",
            "[9203 | 4600.31] loss=2.50 avg=2.58\n",
            "[9204 | 4600.79] loss=2.61 avg=2.58\n",
            "[9205 | 4601.28] loss=2.48 avg=2.58\n",
            "[9206 | 4601.77] loss=2.59 avg=2.58\n",
            "[9207 | 4602.26] loss=2.86 avg=2.58\n",
            "[9208 | 4602.74] loss=2.53 avg=2.58\n",
            "[9209 | 4603.23] loss=2.71 avg=2.58\n",
            "[9210 | 4603.72] loss=2.36 avg=2.58\n",
            "[9211 | 4604.20] loss=2.46 avg=2.58\n",
            "[9212 | 4604.69] loss=2.64 avg=2.58\n",
            "[9213 | 4605.18] loss=2.29 avg=2.57\n",
            "[9214 | 4605.67] loss=2.41 avg=2.57\n",
            "[9215 | 4606.16] loss=2.91 avg=2.58\n",
            "[9216 | 4606.64] loss=2.93 avg=2.58\n",
            "[9217 | 4607.13] loss=2.60 avg=2.58\n",
            "[9218 | 4607.62] loss=2.79 avg=2.58\n",
            "[9219 | 4608.10] loss=2.48 avg=2.58\n",
            "[9220 | 4608.59] loss=2.63 avg=2.58\n",
            "[9221 | 4609.08] loss=2.48 avg=2.58\n",
            "[9222 | 4609.56] loss=2.61 avg=2.58\n",
            "[9223 | 4610.05] loss=2.74 avg=2.58\n",
            "[9224 | 4610.54] loss=2.66 avg=2.58\n",
            "[9225 | 4611.03] loss=2.58 avg=2.58\n",
            "[9226 | 4611.52] loss=2.62 avg=2.58\n",
            "[9227 | 4612.00] loss=2.59 avg=2.58\n",
            "[9228 | 4612.49] loss=2.39 avg=2.58\n",
            "[9229 | 4612.98] loss=2.72 avg=2.58\n",
            "[9230 | 4613.47] loss=2.45 avg=2.58\n",
            "[9231 | 4613.95] loss=2.28 avg=2.58\n",
            "[9232 | 4614.44] loss=2.58 avg=2.58\n",
            "[9233 | 4614.93] loss=2.55 avg=2.58\n",
            "[9234 | 4615.42] loss=2.95 avg=2.58\n",
            "[9235 | 4615.91] loss=2.54 avg=2.58\n",
            "[9236 | 4616.40] loss=2.55 avg=2.58\n",
            "[9237 | 4616.88] loss=2.56 avg=2.58\n",
            "[9238 | 4617.37] loss=2.33 avg=2.58\n",
            "[9239 | 4617.86] loss=2.61 avg=2.58\n",
            "[9240 | 4618.34] loss=2.34 avg=2.58\n",
            "[9241 | 4618.83] loss=2.57 avg=2.58\n",
            "[9242 | 4619.32] loss=2.28 avg=2.57\n",
            "[9243 | 4619.81] loss=2.30 avg=2.57\n",
            "[9244 | 4620.30] loss=2.69 avg=2.57\n",
            "[9245 | 4620.78] loss=2.44 avg=2.57\n",
            "[9246 | 4621.27] loss=2.70 avg=2.57\n",
            "[9247 | 4621.76] loss=2.64 avg=2.57\n",
            "[9248 | 4622.25] loss=2.81 avg=2.58\n",
            "[9249 | 4622.74] loss=2.87 avg=2.58\n",
            "[9250 | 4623.22] loss=1.43 avg=2.57\n",
            "[9251 | 4623.71] loss=2.58 avg=2.57\n",
            "[9252 | 4624.20] loss=2.45 avg=2.57\n",
            "[9253 | 4624.68] loss=1.41 avg=2.55\n",
            "[9254 | 4625.17] loss=2.85 avg=2.56\n",
            "[9255 | 4625.66] loss=2.75 avg=2.56\n",
            "[9256 | 4626.15] loss=2.76 avg=2.56\n",
            "[9257 | 4626.64] loss=3.15 avg=2.57\n",
            "[9258 | 4627.13] loss=2.68 avg=2.57\n",
            "[9259 | 4627.61] loss=2.44 avg=2.57\n",
            "[9260 | 4628.10] loss=2.75 avg=2.57\n",
            "[9261 | 4628.59] loss=2.77 avg=2.57\n",
            "[9262 | 4629.07] loss=2.62 avg=2.57\n",
            "[9263 | 4629.56] loss=2.88 avg=2.57\n",
            "[9264 | 4630.05] loss=2.53 avg=2.57\n",
            "[9265 | 4630.53] loss=2.68 avg=2.57\n",
            "[9266 | 4631.02] loss=2.84 avg=2.58\n",
            "[9267 | 4631.51] loss=2.02 avg=2.57\n",
            "[9268 | 4632.00] loss=2.55 avg=2.57\n",
            "[9269 | 4632.48] loss=2.45 avg=2.57\n",
            "[9270 | 4632.97] loss=2.38 avg=2.57\n",
            "[9271 | 4633.46] loss=2.64 avg=2.57\n",
            "[9272 | 4633.94] loss=2.72 avg=2.57\n",
            "[9273 | 4634.43] loss=2.66 avg=2.57\n",
            "[9274 | 4634.92] loss=2.57 avg=2.57\n",
            "[9275 | 4635.41] loss=2.44 avg=2.57\n",
            "[9276 | 4635.90] loss=2.44 avg=2.57\n",
            "[9277 | 4636.38] loss=2.60 avg=2.57\n",
            "[9278 | 4636.87] loss=2.35 avg=2.57\n",
            "[9279 | 4637.36] loss=2.46 avg=2.57\n",
            "[9280 | 4637.85] loss=2.82 avg=2.57\n",
            "[9281 | 4638.33] loss=2.35 avg=2.57\n",
            "[9282 | 4638.82] loss=2.42 avg=2.57\n",
            "[9283 | 4639.31] loss=2.28 avg=2.56\n",
            "[9284 | 4639.80] loss=2.43 avg=2.56\n",
            "[9285 | 4640.28] loss=2.71 avg=2.56\n",
            "[9286 | 4640.77] loss=2.30 avg=2.56\n",
            "[9287 | 4641.26] loss=3.02 avg=2.56\n",
            "[9288 | 4641.75] loss=2.69 avg=2.57\n",
            "[9289 | 4642.23] loss=2.57 avg=2.57\n",
            "[9290 | 4642.72] loss=2.62 avg=2.57\n",
            "[9291 | 4643.21] loss=2.51 avg=2.57\n",
            "[9292 | 4643.70] loss=2.52 avg=2.57\n",
            "[9293 | 4644.18] loss=2.53 avg=2.56\n",
            "[9294 | 4644.67] loss=2.50 avg=2.56\n",
            "[9295 | 4645.16] loss=2.50 avg=2.56\n",
            "[9296 | 4645.65] loss=2.86 avg=2.57\n",
            "[9297 | 4646.13] loss=2.37 avg=2.56\n",
            "[9298 | 4646.62] loss=2.70 avg=2.57\n",
            "[9299 | 4647.11] loss=2.49 avg=2.57\n",
            "[9300 | 4647.59] loss=2.68 avg=2.57\n",
            "[9301 | 4648.08] loss=2.65 avg=2.57\n",
            "[9302 | 4648.57] loss=2.50 avg=2.57\n",
            "[9303 | 4649.06] loss=2.52 avg=2.57\n",
            "[9304 | 4649.54] loss=2.49 avg=2.57\n",
            "[9305 | 4650.03] loss=2.34 avg=2.56\n",
            "[9306 | 4650.52] loss=2.27 avg=2.56\n",
            "[9307 | 4651.01] loss=2.40 avg=2.56\n",
            "[9308 | 4651.50] loss=2.70 avg=2.56\n",
            "[9309 | 4651.98] loss=2.54 avg=2.56\n",
            "[9310 | 4652.47] loss=2.70 avg=2.56\n",
            "[9311 | 4652.95] loss=2.49 avg=2.56\n",
            "[9312 | 4653.44] loss=2.64 avg=2.56\n",
            "[9313 | 4653.93] loss=2.70 avg=2.56\n",
            "[9314 | 4654.42] loss=2.26 avg=2.56\n",
            "[9315 | 4654.90] loss=2.89 avg=2.56\n",
            "[9316 | 4655.39] loss=2.09 avg=2.56\n",
            "[9317 | 4655.88] loss=2.39 avg=2.56\n",
            "[9318 | 4656.37] loss=2.41 avg=2.55\n",
            "[9319 | 4656.85] loss=2.55 avg=2.55\n",
            "[9320 | 4657.34] loss=2.73 avg=2.56\n",
            "[9321 | 4657.82] loss=2.19 avg=2.55\n",
            "[9322 | 4658.31] loss=2.70 avg=2.55\n",
            "[9323 | 4658.80] loss=2.74 avg=2.56\n",
            "[9324 | 4659.29] loss=2.61 avg=2.56\n",
            "[9325 | 4659.78] loss=2.57 avg=2.56\n",
            "[9326 | 4660.26] loss=2.31 avg=2.55\n",
            "[9327 | 4660.75] loss=2.60 avg=2.56\n",
            "[9328 | 4661.24] loss=2.44 avg=2.55\n",
            "[9329 | 4661.72] loss=2.75 avg=2.56\n",
            "[9330 | 4662.21] loss=2.72 avg=2.56\n",
            "[9331 | 4662.70] loss=2.76 avg=2.56\n",
            "[9332 | 4663.18] loss=2.29 avg=2.56\n",
            "[9333 | 4663.67] loss=2.58 avg=2.56\n",
            "[9334 | 4664.16] loss=2.48 avg=2.56\n",
            "[9335 | 4664.65] loss=2.33 avg=2.55\n",
            "[9336 | 4665.13] loss=2.62 avg=2.55\n",
            "[9337 | 4665.62] loss=2.73 avg=2.56\n",
            "[9338 | 4666.11] loss=2.24 avg=2.55\n",
            "[9339 | 4666.59] loss=2.47 avg=2.55\n",
            "[9340 | 4667.09] loss=2.75 avg=2.55\n",
            "[9341 | 4667.57] loss=2.85 avg=2.56\n",
            "[9342 | 4668.06] loss=2.79 avg=2.56\n",
            "[9343 | 4668.54] loss=2.61 avg=2.56\n",
            "[9344 | 4669.03] loss=2.12 avg=2.56\n",
            "[9345 | 4669.52] loss=2.33 avg=2.55\n",
            "[9346 | 4670.00] loss=2.57 avg=2.55\n",
            "[9347 | 4670.49] loss=2.62 avg=2.55\n",
            "[9348 | 4670.97] loss=2.77 avg=2.56\n",
            "[9349 | 4671.47] loss=2.54 avg=2.56\n",
            "[9350 | 4671.95] loss=2.47 avg=2.56\n",
            "[9351 | 4672.44] loss=2.89 avg=2.56\n",
            "[9352 | 4672.93] loss=2.63 avg=2.56\n",
            "[9353 | 4673.42] loss=2.70 avg=2.56\n",
            "[9354 | 4673.90] loss=2.75 avg=2.56\n",
            "[9355 | 4674.39] loss=2.36 avg=2.56\n",
            "[9356 | 4674.88] loss=2.71 avg=2.56\n",
            "[9357 | 4675.37] loss=2.83 avg=2.56\n",
            "[9358 | 4675.85] loss=2.70 avg=2.57\n",
            "[9359 | 4676.34] loss=2.83 avg=2.57\n",
            "[9360 | 4676.83] loss=2.21 avg=2.57\n",
            "[9361 | 4677.32] loss=2.63 avg=2.57\n",
            "[9362 | 4677.80] loss=2.54 avg=2.57\n",
            "[9363 | 4678.29] loss=2.48 avg=2.56\n",
            "[9364 | 4678.78] loss=2.75 avg=2.57\n",
            "[9365 | 4679.27] loss=2.30 avg=2.56\n",
            "[9366 | 4679.76] loss=3.10 avg=2.57\n",
            "[9367 | 4680.25] loss=2.80 avg=2.57\n",
            "[9368 | 4680.73] loss=2.29 avg=2.57\n",
            "[9369 | 4681.22] loss=2.56 avg=2.57\n",
            "[9370 | 4681.71] loss=2.52 avg=2.57\n",
            "[9371 | 4682.20] loss=2.60 avg=2.57\n",
            "[9372 | 4682.69] loss=2.54 avg=2.57\n",
            "[9373 | 4683.17] loss=2.97 avg=2.57\n",
            "[9374 | 4683.66] loss=2.92 avg=2.58\n",
            "[9375 | 4684.15] loss=2.60 avg=2.58\n",
            "[9376 | 4684.64] loss=2.23 avg=2.57\n",
            "[9377 | 4685.13] loss=2.34 avg=2.57\n",
            "[9378 | 4685.61] loss=2.16 avg=2.57\n",
            "[9379 | 4686.10] loss=2.35 avg=2.56\n",
            "[9380 | 4686.59] loss=2.74 avg=2.57\n",
            "[9381 | 4687.08] loss=2.54 avg=2.57\n",
            "[9382 | 4687.57] loss=2.55 avg=2.57\n",
            "[9383 | 4688.05] loss=2.58 avg=2.57\n",
            "[9384 | 4688.54] loss=2.35 avg=2.56\n",
            "[9385 | 4689.03] loss=2.45 avg=2.56\n",
            "[9386 | 4689.52] loss=2.84 avg=2.56\n",
            "[9387 | 4690.01] loss=2.72 avg=2.57\n",
            "[9388 | 4690.50] loss=2.48 avg=2.57\n",
            "[9389 | 4690.98] loss=2.75 avg=2.57\n",
            "[9390 | 4691.47] loss=2.38 avg=2.57\n",
            "[9391 | 4691.96] loss=2.38 avg=2.56\n",
            "[9392 | 4692.44] loss=2.63 avg=2.56\n",
            "[9393 | 4692.94] loss=2.65 avg=2.57\n",
            "[9394 | 4693.42] loss=2.76 avg=2.57\n",
            "[9395 | 4693.91] loss=2.54 avg=2.57\n",
            "[9396 | 4694.40] loss=2.47 avg=2.57\n",
            "[9397 | 4694.88] loss=2.54 avg=2.57\n",
            "[9398 | 4695.37] loss=2.77 avg=2.57\n",
            "[9399 | 4695.86] loss=3.21 avg=2.57\n",
            "[9400 | 4696.34] loss=2.78 avg=2.58\n",
            "[9401 | 4696.83] loss=2.42 avg=2.57\n",
            "[9402 | 4697.32] loss=2.59 avg=2.57\n",
            "[9403 | 4697.81] loss=2.75 avg=2.58\n",
            "[9404 | 4698.29] loss=2.46 avg=2.58\n",
            "[9405 | 4698.78] loss=2.77 avg=2.58\n",
            "[9406 | 4699.27] loss=2.51 avg=2.58\n",
            "[9407 | 4699.76] loss=2.58 avg=2.58\n",
            "[9408 | 4700.24] loss=2.47 avg=2.58\n",
            "[9409 | 4700.73] loss=2.74 avg=2.58\n",
            "[9410 | 4701.22] loss=2.26 avg=2.57\n",
            "[9411 | 4701.70] loss=2.62 avg=2.57\n",
            "[9412 | 4702.19] loss=2.58 avg=2.57\n",
            "[9413 | 4702.68] loss=2.50 avg=2.57\n",
            "[9414 | 4703.16] loss=2.85 avg=2.58\n",
            "[9415 | 4703.65] loss=2.57 avg=2.58\n",
            "[9416 | 4704.14] loss=2.72 avg=2.58\n",
            "[9417 | 4704.62] loss=2.57 avg=2.58\n",
            "[9418 | 4705.11] loss=2.52 avg=2.58\n",
            "[9419 | 4705.60] loss=2.70 avg=2.58\n",
            "[9420 | 4706.09] loss=2.89 avg=2.58\n",
            "[9421 | 4706.57] loss=2.76 avg=2.58\n",
            "[9422 | 4707.06] loss=2.75 avg=2.59\n",
            "[9423 | 4707.55] loss=2.64 avg=2.59\n",
            "[9424 | 4708.03] loss=2.39 avg=2.58\n",
            "[9425 | 4708.52] loss=2.57 avg=2.58\n",
            "[9426 | 4709.01] loss=2.48 avg=2.58\n",
            "[9427 | 4709.50] loss=2.55 avg=2.58\n",
            "[9428 | 4709.98] loss=2.45 avg=2.58\n",
            "[9429 | 4710.47] loss=2.94 avg=2.58\n",
            "[9430 | 4710.96] loss=2.73 avg=2.59\n",
            "[9431 | 4711.45] loss=2.91 avg=2.59\n",
            "[9432 | 4711.93] loss=2.73 avg=2.59\n",
            "[9433 | 4712.42] loss=2.53 avg=2.59\n",
            "[9434 | 4712.91] loss=2.79 avg=2.59\n",
            "[9435 | 4713.39] loss=2.51 avg=2.59\n",
            "[9436 | 4713.88] loss=2.70 avg=2.59\n",
            "[9437 | 4714.37] loss=2.65 avg=2.59\n",
            "[9438 | 4714.86] loss=2.32 avg=2.59\n",
            "[9439 | 4715.34] loss=2.66 avg=2.59\n",
            "[9440 | 4715.83] loss=2.40 avg=2.59\n",
            "[9441 | 4716.32] loss=2.56 avg=2.59\n",
            "[9442 | 4716.81] loss=2.67 avg=2.59\n",
            "[9443 | 4717.29] loss=2.78 avg=2.59\n",
            "[9444 | 4717.78] loss=2.49 avg=2.59\n",
            "[9445 | 4718.27] loss=2.20 avg=2.59\n",
            "[9446 | 4718.75] loss=2.99 avg=2.59\n",
            "[9447 | 4719.24] loss=1.58 avg=2.58\n",
            "[9448 | 4719.73] loss=2.85 avg=2.58\n",
            "[9449 | 4720.21] loss=2.58 avg=2.58\n",
            "[9450 | 4720.70] loss=2.53 avg=2.58\n",
            "[9451 | 4721.19] loss=2.35 avg=2.58\n",
            "[9452 | 4721.67] loss=1.93 avg=2.57\n",
            "[9453 | 4722.16] loss=2.61 avg=2.57\n",
            "[9454 | 4722.65] loss=2.68 avg=2.58\n",
            "[9455 | 4723.14] loss=2.90 avg=2.58\n",
            "[9456 | 4723.62] loss=2.52 avg=2.58\n",
            "[9457 | 4724.11] loss=2.47 avg=2.58\n",
            "[9458 | 4724.60] loss=2.58 avg=2.58\n",
            "[9459 | 4725.09] loss=1.60 avg=2.57\n",
            "[9460 | 4725.57] loss=2.50 avg=2.57\n",
            "[9461 | 4726.06] loss=2.42 avg=2.56\n",
            "[9462 | 4726.55] loss=2.44 avg=2.56\n",
            "[9463 | 4727.04] loss=2.53 avg=2.56\n",
            "[9464 | 4727.52] loss=2.58 avg=2.56\n",
            "[9465 | 4728.01] loss=2.66 avg=2.56\n",
            "[9466 | 4728.50] loss=2.71 avg=2.57\n",
            "[9467 | 4728.99] loss=2.53 avg=2.57\n",
            "[9468 | 4729.47] loss=2.40 avg=2.56\n",
            "[9469 | 4729.96] loss=2.32 avg=2.56\n",
            "[9470 | 4730.45] loss=2.61 avg=2.56\n",
            "[9471 | 4730.93] loss=2.13 avg=2.56\n",
            "[9472 | 4731.42] loss=2.62 avg=2.56\n",
            "[9473 | 4731.90] loss=2.52 avg=2.56\n",
            "[9474 | 4732.40] loss=2.60 avg=2.56\n",
            "[9475 | 4732.88] loss=2.49 avg=2.56\n",
            "[9476 | 4733.37] loss=2.65 avg=2.56\n",
            "[9477 | 4733.86] loss=2.70 avg=2.56\n",
            "[9478 | 4734.35] loss=2.53 avg=2.56\n",
            "[9479 | 4734.83] loss=2.69 avg=2.56\n",
            "[9480 | 4735.32] loss=2.64 avg=2.56\n",
            "[9481 | 4735.81] loss=2.38 avg=2.56\n",
            "[9482 | 4736.30] loss=2.78 avg=2.56\n",
            "[9483 | 4736.78] loss=3.03 avg=2.57\n",
            "[9484 | 4737.27] loss=2.14 avg=2.56\n",
            "[9485 | 4737.75] loss=2.37 avg=2.56\n",
            "[9486 | 4738.24] loss=2.29 avg=2.56\n",
            "[9487 | 4738.73] loss=2.56 avg=2.56\n",
            "[9488 | 4739.22] loss=2.52 avg=2.56\n",
            "[9489 | 4739.70] loss=2.80 avg=2.56\n",
            "[9490 | 4740.19] loss=2.54 avg=2.56\n",
            "[9491 | 4740.68] loss=2.98 avg=2.56\n",
            "[9492 | 4741.16] loss=2.76 avg=2.57\n",
            "[9493 | 4741.65] loss=2.76 avg=2.57\n",
            "[9494 | 4742.14] loss=2.11 avg=2.56\n",
            "[9495 | 4742.63] loss=2.48 avg=2.56\n",
            "[9496 | 4743.11] loss=2.40 avg=2.56\n",
            "[9497 | 4743.60] loss=2.37 avg=2.56\n",
            "[9498 | 4744.09] loss=2.97 avg=2.56\n",
            "[9499 | 4744.57] loss=2.47 avg=2.56\n",
            "[9500 | 4745.06] loss=2.56 avg=2.56\n",
            "[9501 | 4745.55] loss=2.18 avg=2.56\n",
            "[9502 | 4746.04] loss=2.83 avg=2.56\n",
            "[9503 | 4746.53] loss=2.53 avg=2.56\n",
            "[9504 | 4747.01] loss=2.77 avg=2.56\n",
            "[9505 | 4747.50] loss=2.80 avg=2.57\n",
            "[9506 | 4747.99] loss=2.46 avg=2.56\n",
            "[9507 | 4748.48] loss=2.39 avg=2.56\n",
            "[9508 | 4748.96] loss=2.64 avg=2.56\n",
            "[9509 | 4749.45] loss=2.48 avg=2.56\n",
            "[9510 | 4749.94] loss=2.41 avg=2.56\n",
            "[9511 | 4750.43] loss=2.54 avg=2.56\n",
            "[9512 | 4750.92] loss=2.56 avg=2.56\n",
            "[9513 | 4751.40] loss=2.96 avg=2.56\n",
            "[9514 | 4751.89] loss=2.78 avg=2.57\n",
            "[9515 | 4752.38] loss=2.52 avg=2.57\n",
            "[9516 | 4752.87] loss=2.49 avg=2.57\n",
            "[9517 | 4753.36] loss=2.63 avg=2.57\n",
            "[9518 | 4753.84] loss=2.44 avg=2.56\n",
            "[9519 | 4754.33] loss=2.48 avg=2.56\n",
            "[9520 | 4754.82] loss=2.86 avg=2.57\n",
            "[9521 | 4755.30] loss=2.32 avg=2.56\n",
            "[9522 | 4755.79] loss=2.77 avg=2.57\n",
            "[9523 | 4756.28] loss=2.87 avg=2.57\n",
            "[9524 | 4756.77] loss=2.70 avg=2.57\n",
            "[9525 | 4757.25] loss=2.55 avg=2.57\n",
            "[9526 | 4757.75] loss=2.25 avg=2.57\n",
            "[9527 | 4758.23] loss=2.53 avg=2.57\n",
            "[9528 | 4758.72] loss=2.76 avg=2.57\n",
            "[9529 | 4759.21] loss=2.24 avg=2.57\n",
            "[9530 | 4759.70] loss=2.88 avg=2.57\n",
            "[9531 | 4760.18] loss=2.45 avg=2.57\n",
            "[9532 | 4760.67] loss=2.53 avg=2.57\n",
            "[9533 | 4761.16] loss=2.12 avg=2.56\n",
            "[9534 | 4761.65] loss=2.36 avg=2.56\n",
            "[9535 | 4762.13] loss=2.57 avg=2.56\n",
            "[9536 | 4762.62] loss=2.59 avg=2.56\n",
            "[9537 | 4763.11] loss=2.37 avg=2.56\n",
            "[9538 | 4763.60] loss=2.04 avg=2.55\n",
            "[9539 | 4764.08] loss=2.54 avg=2.55\n",
            "[9540 | 4764.57] loss=2.92 avg=2.56\n",
            "[9541 | 4765.06] loss=2.53 avg=2.56\n",
            "[9542 | 4765.54] loss=2.88 avg=2.56\n",
            "[9543 | 4766.03] loss=2.77 avg=2.56\n",
            "[9544 | 4766.52] loss=2.54 avg=2.56\n",
            "[9545 | 4767.01] loss=2.73 avg=2.56\n",
            "[9546 | 4767.49] loss=2.47 avg=2.56\n",
            "[9547 | 4767.98] loss=2.65 avg=2.56\n",
            "[9548 | 4768.47] loss=2.72 avg=2.57\n",
            "[9549 | 4768.95] loss=2.59 avg=2.57\n",
            "[9550 | 4769.44] loss=2.64 avg=2.57\n",
            "[9551 | 4769.93] loss=2.91 avg=2.57\n",
            "[9552 | 4770.42] loss=2.53 avg=2.57\n",
            "[9553 | 4770.90] loss=2.65 avg=2.57\n",
            "[9554 | 4771.39] loss=2.46 avg=2.57\n",
            "[9555 | 4771.88] loss=2.35 avg=2.57\n",
            "[9556 | 4772.37] loss=2.40 avg=2.57\n",
            "[9557 | 4772.85] loss=2.50 avg=2.56\n",
            "[9558 | 4773.34] loss=2.23 avg=2.56\n",
            "[9559 | 4773.82] loss=2.62 avg=2.56\n",
            "[9560 | 4774.31] loss=2.72 avg=2.56\n",
            "[9561 | 4774.80] loss=2.29 avg=2.56\n",
            "[9562 | 4775.29] loss=2.56 avg=2.56\n",
            "[9563 | 4775.78] loss=2.98 avg=2.56\n",
            "[9564 | 4776.26] loss=2.64 avg=2.57\n",
            "[9565 | 4776.75] loss=2.97 avg=2.57\n",
            "[9566 | 4777.23] loss=2.49 avg=2.57\n",
            "[9567 | 4777.73] loss=2.46 avg=2.57\n",
            "[9568 | 4778.21] loss=2.53 avg=2.57\n",
            "[9569 | 4778.70] loss=2.56 avg=2.57\n",
            "[9570 | 4779.19] loss=2.66 avg=2.57\n",
            "[9571 | 4779.68] loss=2.50 avg=2.57\n",
            "[9572 | 4780.16] loss=2.86 avg=2.57\n",
            "[9573 | 4780.65] loss=2.64 avg=2.57\n",
            "[9574 | 4781.14] loss=2.44 avg=2.57\n",
            "[9575 | 4781.62] loss=2.51 avg=2.57\n",
            "[9576 | 4782.11] loss=2.58 avg=2.57\n",
            "[9577 | 4782.60] loss=1.98 avg=2.56\n",
            "[9578 | 4783.08] loss=2.68 avg=2.56\n",
            "[9579 | 4783.57] loss=3.09 avg=2.57\n",
            "[9580 | 4784.06] loss=2.65 avg=2.57\n",
            "[9581 | 4784.55] loss=2.48 avg=2.57\n",
            "[9582 | 4785.03] loss=2.51 avg=2.57\n",
            "[9583 | 4785.52] loss=2.75 avg=2.57\n",
            "[9584 | 4786.01] loss=2.77 avg=2.57\n",
            "[9585 | 4786.49] loss=2.52 avg=2.57\n",
            "[9586 | 4786.98] loss=2.56 avg=2.57\n",
            "[9587 | 4787.47] loss=2.44 avg=2.57\n",
            "[9588 | 4787.96] loss=2.68 avg=2.57\n",
            "[9589 | 4788.44] loss=2.57 avg=2.57\n",
            "[9590 | 4788.93] loss=2.23 avg=2.57\n",
            "[9591 | 4789.42] loss=1.50 avg=2.56\n",
            "[9592 | 4789.91] loss=2.52 avg=2.56\n",
            "[9593 | 4790.41] loss=2.51 avg=2.56\n",
            "[9594 | 4790.90] loss=2.73 avg=2.56\n",
            "[9595 | 4791.39] loss=2.57 avg=2.56\n",
            "[9596 | 4791.87] loss=2.53 avg=2.56\n",
            "[9597 | 4792.36] loss=2.73 avg=2.56\n",
            "[9598 | 4792.85] loss=2.98 avg=2.56\n",
            "[9599 | 4793.33] loss=2.67 avg=2.57\n",
            "[9600 | 4793.82] loss=2.81 avg=2.57\n",
            "[9601 | 4794.31] loss=2.56 avg=2.57\n",
            "[9602 | 4794.79] loss=2.62 avg=2.57\n",
            "[9603 | 4795.28] loss=2.74 avg=2.57\n",
            "[9604 | 4795.77] loss=2.81 avg=2.57\n",
            "[9605 | 4796.25] loss=2.68 avg=2.57\n",
            "[9606 | 4796.74] loss=2.57 avg=2.57\n",
            "[9607 | 4797.23] loss=2.30 avg=2.57\n",
            "[9608 | 4797.72] loss=2.92 avg=2.57\n",
            "[9609 | 4798.20] loss=2.31 avg=2.57\n",
            "[9610 | 4798.69] loss=2.50 avg=2.57\n",
            "[9611 | 4799.18] loss=2.69 avg=2.57\n",
            "[9612 | 4799.66] loss=2.45 avg=2.57\n",
            "[9613 | 4800.15] loss=2.42 avg=2.57\n",
            "[9614 | 4800.64] loss=2.24 avg=2.57\n",
            "[9615 | 4801.12] loss=2.94 avg=2.57\n",
            "[9616 | 4801.61] loss=2.63 avg=2.57\n",
            "[9617 | 4802.10] loss=2.58 avg=2.57\n",
            "[9618 | 4802.58] loss=2.26 avg=2.57\n",
            "[9619 | 4803.07] loss=2.30 avg=2.56\n",
            "[9620 | 4803.56] loss=2.48 avg=2.56\n",
            "[9621 | 4804.04] loss=2.47 avg=2.56\n",
            "[9622 | 4804.53] loss=2.80 avg=2.57\n",
            "[9623 | 4805.02] loss=2.80 avg=2.57\n",
            "[9624 | 4805.50] loss=2.14 avg=2.56\n",
            "[9625 | 4805.99] loss=2.47 avg=2.56\n",
            "[9626 | 4806.48] loss=2.56 avg=2.56\n",
            "[9627 | 4806.97] loss=2.61 avg=2.56\n",
            "[9628 | 4807.45] loss=2.18 avg=2.56\n",
            "[9629 | 4807.94] loss=2.51 avg=2.56\n",
            "[9630 | 4808.43] loss=2.94 avg=2.56\n",
            "[9631 | 4808.91] loss=2.81 avg=2.56\n",
            "[9632 | 4809.40] loss=2.53 avg=2.56\n",
            "[9633 | 4809.89] loss=3.00 avg=2.57\n",
            "[9634 | 4810.37] loss=2.82 avg=2.57\n",
            "[9635 | 4810.86] loss=2.49 avg=2.57\n",
            "[9636 | 4811.35] loss=1.56 avg=2.56\n",
            "[9637 | 4811.84] loss=2.65 avg=2.56\n",
            "[9638 | 4812.33] loss=2.52 avg=2.56\n",
            "[9639 | 4812.81] loss=2.75 avg=2.56\n",
            "[9640 | 4813.30] loss=2.55 avg=2.56\n",
            "[9641 | 4813.79] loss=2.54 avg=2.56\n",
            "[9642 | 4814.28] loss=2.56 avg=2.56\n",
            "[9643 | 4814.76] loss=2.25 avg=2.56\n",
            "[9644 | 4815.25] loss=2.72 avg=2.56\n",
            "[9645 | 4815.74] loss=2.64 avg=2.56\n",
            "[9646 | 4816.22] loss=2.18 avg=2.56\n",
            "[9647 | 4816.71] loss=2.36 avg=2.56\n",
            "[9648 | 4817.20] loss=2.28 avg=2.55\n",
            "[9649 | 4817.69] loss=2.30 avg=2.55\n",
            "[9650 | 4818.18] loss=2.74 avg=2.55\n",
            "[9651 | 4818.67] loss=2.59 avg=2.55\n",
            "[9652 | 4819.15] loss=2.74 avg=2.55\n",
            "[9653 | 4819.64] loss=2.49 avg=2.55\n",
            "[9654 | 4820.13] loss=2.65 avg=2.55\n",
            "[9655 | 4820.62] loss=2.94 avg=2.56\n",
            "[9656 | 4821.10] loss=2.80 avg=2.56\n",
            "[9657 | 4821.59] loss=2.92 avg=2.56\n",
            "[9658 | 4822.08] loss=2.54 avg=2.56\n",
            "[9659 | 4822.57] loss=2.55 avg=2.56\n",
            "[9660 | 4823.06] loss=2.37 avg=2.56\n",
            "[9661 | 4823.55] loss=2.75 avg=2.56\n",
            "[9662 | 4824.03] loss=2.74 avg=2.57\n",
            "[9663 | 4824.52] loss=2.75 avg=2.57\n",
            "[9664 | 4825.01] loss=2.66 avg=2.57\n",
            "[9665 | 4825.50] loss=2.61 avg=2.57\n",
            "[9666 | 4825.98] loss=2.78 avg=2.57\n",
            "[9667 | 4826.47] loss=2.53 avg=2.57\n",
            "[9668 | 4826.96] loss=2.23 avg=2.57\n",
            "[9669 | 4827.45] loss=2.83 avg=2.57\n",
            "[9670 | 4827.93] loss=2.33 avg=2.57\n",
            "[9671 | 4828.42] loss=2.44 avg=2.57\n",
            "[9672 | 4828.91] loss=2.26 avg=2.56\n",
            "[9673 | 4829.40] loss=2.29 avg=2.56\n",
            "[9674 | 4829.88] loss=2.55 avg=2.56\n",
            "[9675 | 4830.37] loss=2.35 avg=2.56\n",
            "[9676 | 4830.86] loss=2.74 avg=2.56\n",
            "[9677 | 4831.35] loss=2.41 avg=2.56\n",
            "[9678 | 4831.83] loss=2.21 avg=2.56\n",
            "[9679 | 4832.32] loss=2.45 avg=2.55\n",
            "[9680 | 4832.81] loss=2.52 avg=2.55\n",
            "[9681 | 4833.29] loss=2.52 avg=2.55\n",
            "[9682 | 4833.78] loss=2.59 avg=2.55\n",
            "[9683 | 4834.27] loss=2.67 avg=2.55\n",
            "[9684 | 4834.76] loss=2.29 avg=2.55\n",
            "[9685 | 4835.24] loss=2.45 avg=2.55\n",
            "[9686 | 4835.73] loss=2.54 avg=2.55\n",
            "[9687 | 4836.22] loss=2.38 avg=2.55\n",
            "[9688 | 4836.71] loss=2.78 avg=2.55\n",
            "[9689 | 4837.19] loss=2.64 avg=2.55\n",
            "[9690 | 4837.68] loss=2.30 avg=2.55\n",
            "[9691 | 4838.17] loss=2.67 avg=2.55\n",
            "[9692 | 4838.65] loss=2.38 avg=2.55\n",
            "[9693 | 4839.14] loss=2.24 avg=2.55\n",
            "[9694 | 4839.64] loss=2.41 avg=2.55\n",
            "[9695 | 4840.13] loss=2.57 avg=2.55\n",
            "[9696 | 4840.62] loss=2.65 avg=2.55\n",
            "[9697 | 4841.11] loss=2.39 avg=2.54\n",
            "[9698 | 4841.60] loss=2.18 avg=2.54\n",
            "[9699 | 4842.08] loss=2.57 avg=2.54\n",
            "[9700 | 4842.57] loss=2.39 avg=2.54\n",
            "[9701 | 4843.05] loss=2.80 avg=2.54\n",
            "[9702 | 4843.54] loss=2.33 avg=2.54\n",
            "[9703 | 4844.03] loss=2.79 avg=2.54\n",
            "[9704 | 4844.52] loss=2.49 avg=2.54\n",
            "[9705 | 4845.01] loss=2.42 avg=2.54\n",
            "[9706 | 4845.49] loss=2.63 avg=2.54\n",
            "[9707 | 4845.98] loss=2.86 avg=2.55\n",
            "[9708 | 4846.47] loss=2.68 avg=2.55\n",
            "[9709 | 4846.95] loss=2.46 avg=2.55\n",
            "[9710 | 4847.44] loss=2.58 avg=2.55\n",
            "[9711 | 4847.93] loss=2.53 avg=2.55\n",
            "[9712 | 4848.42] loss=2.59 avg=2.55\n",
            "[9713 | 4848.90] loss=2.81 avg=2.55\n",
            "[9714 | 4849.39] loss=2.36 avg=2.55\n",
            "[9715 | 4849.88] loss=2.69 avg=2.55\n",
            "[9716 | 4850.37] loss=2.58 avg=2.55\n",
            "[9717 | 4850.85] loss=2.57 avg=2.55\n",
            "[9718 | 4851.34] loss=1.68 avg=2.54\n",
            "[9719 | 4851.83] loss=2.61 avg=2.54\n",
            "[9720 | 4852.31] loss=2.24 avg=2.54\n",
            "[9721 | 4852.82] loss=2.72 avg=2.54\n",
            "[9722 | 4853.30] loss=2.11 avg=2.54\n",
            "[9723 | 4853.79] loss=2.60 avg=2.54\n",
            "[9724 | 4854.28] loss=2.62 avg=2.54\n",
            "[9725 | 4854.77] loss=2.78 avg=2.54\n",
            "[9726 | 4855.26] loss=2.60 avg=2.54\n",
            "[9727 | 4855.74] loss=2.78 avg=2.54\n",
            "[9728 | 4856.23] loss=2.69 avg=2.54\n",
            "[9729 | 4856.72] loss=2.77 avg=2.55\n",
            "[9730 | 4857.20] loss=2.61 avg=2.55\n",
            "[9731 | 4857.69] loss=2.12 avg=2.54\n",
            "[9732 | 4858.18] loss=2.48 avg=2.54\n",
            "[9733 | 4858.66] loss=2.83 avg=2.54\n",
            "[9734 | 4859.15] loss=2.94 avg=2.55\n",
            "[9735 | 4859.64] loss=2.64 avg=2.55\n",
            "[9736 | 4860.13] loss=2.40 avg=2.55\n",
            "[9737 | 4860.61] loss=2.60 avg=2.55\n",
            "[9738 | 4861.10] loss=2.76 avg=2.55\n",
            "[9739 | 4861.58] loss=2.49 avg=2.55\n",
            "[9740 | 4862.07] loss=2.33 avg=2.55\n",
            "[9741 | 4862.56] loss=2.69 avg=2.55\n",
            "[9742 | 4863.05] loss=2.38 avg=2.55\n",
            "[9743 | 4863.54] loss=2.63 avg=2.55\n",
            "[9744 | 4864.02] loss=2.61 avg=2.55\n",
            "[9745 | 4864.51] loss=2.95 avg=2.55\n",
            "[9746 | 4865.00] loss=2.82 avg=2.56\n",
            "[9747 | 4865.48] loss=2.48 avg=2.56\n",
            "[9748 | 4865.97] loss=2.48 avg=2.55\n",
            "[9749 | 4866.46] loss=2.51 avg=2.55\n",
            "[9750 | 4866.95] loss=2.63 avg=2.55\n",
            "[9751 | 4867.43] loss=2.48 avg=2.55\n",
            "[9752 | 4867.92] loss=2.05 avg=2.55\n",
            "[9753 | 4868.41] loss=2.59 avg=2.55\n",
            "[9754 | 4868.90] loss=2.40 avg=2.55\n",
            "[9755 | 4869.38] loss=2.73 avg=2.55\n",
            "[9756 | 4869.87] loss=2.07 avg=2.55\n",
            "[9757 | 4870.36] loss=2.48 avg=2.54\n",
            "[9758 | 4870.85] loss=2.64 avg=2.55\n",
            "[9759 | 4871.33] loss=2.55 avg=2.55\n",
            "[9760 | 4871.82] loss=2.53 avg=2.55\n",
            "[9761 | 4872.30] loss=2.43 avg=2.54\n",
            "[9762 | 4872.79] loss=2.62 avg=2.54\n",
            "[9763 | 4873.28] loss=2.71 avg=2.55\n",
            "[9764 | 4873.77] loss=3.05 avg=2.55\n",
            "[9765 | 4874.25] loss=2.68 avg=2.55\n",
            "[9766 | 4874.74] loss=2.71 avg=2.55\n",
            "[9767 | 4875.22] loss=2.55 avg=2.55\n",
            "[9768 | 4875.71] loss=2.54 avg=2.55\n",
            "[9769 | 4876.20] loss=2.87 avg=2.56\n",
            "[9770 | 4876.69] loss=2.54 avg=2.56\n",
            "[9771 | 4877.18] loss=2.59 avg=2.56\n",
            "[9772 | 4877.66] loss=2.40 avg=2.56\n",
            "[9773 | 4878.15] loss=2.52 avg=2.56\n",
            "[9774 | 4878.64] loss=2.73 avg=2.56\n",
            "[9775 | 4879.13] loss=2.74 avg=2.56\n",
            "[9776 | 4879.61] loss=2.68 avg=2.56\n",
            "[9777 | 4880.10] loss=2.87 avg=2.56\n",
            "[9778 | 4880.59] loss=2.52 avg=2.56\n",
            "[9779 | 4881.07] loss=2.45 avg=2.56\n",
            "[9780 | 4881.56] loss=2.77 avg=2.56\n",
            "[9781 | 4882.05] loss=2.68 avg=2.57\n",
            "[9782 | 4882.54] loss=2.73 avg=2.57\n",
            "[9783 | 4883.02] loss=2.90 avg=2.57\n",
            "[9784 | 4883.51] loss=2.22 avg=2.57\n",
            "[9785 | 4883.99] loss=2.11 avg=2.56\n",
            "[9786 | 4884.48] loss=2.50 avg=2.56\n",
            "[9787 | 4884.97] loss=2.46 avg=2.56\n",
            "[9788 | 4885.46] loss=2.82 avg=2.56\n",
            "[9789 | 4885.95] loss=2.24 avg=2.56\n",
            "[9790 | 4886.44] loss=1.67 avg=2.55\n",
            "[9791 | 4886.92] loss=2.24 avg=2.55\n",
            "[9792 | 4887.41] loss=2.46 avg=2.55\n",
            "[9793 | 4887.90] loss=2.47 avg=2.55\n",
            "[9794 | 4888.39] loss=2.70 avg=2.55\n",
            "[9795 | 4888.88] loss=2.68 avg=2.55\n",
            "[9796 | 4889.37] loss=2.70 avg=2.55\n",
            "[9797 | 4889.86] loss=2.69 avg=2.55\n",
            "[9798 | 4890.34] loss=2.66 avg=2.55\n",
            "[9799 | 4890.83] loss=2.49 avg=2.55\n",
            "[9800 | 4891.32] loss=2.54 avg=2.55\n",
            "[9801 | 4891.81] loss=2.71 avg=2.55\n",
            "[9802 | 4892.29] loss=2.44 avg=2.55\n",
            "[9803 | 4892.78] loss=2.67 avg=2.55\n",
            "[9804 | 4893.27] loss=2.72 avg=2.56\n",
            "[9805 | 4893.76] loss=2.77 avg=2.56\n",
            "[9806 | 4894.25] loss=2.03 avg=2.55\n",
            "[9807 | 4894.74] loss=2.86 avg=2.56\n",
            "[9808 | 4895.23] loss=2.52 avg=2.56\n",
            "[9809 | 4895.71] loss=2.49 avg=2.55\n",
            "[9810 | 4896.20] loss=2.49 avg=2.55\n",
            "[9811 | 4896.69] loss=2.36 avg=2.55\n",
            "[9812 | 4897.18] loss=2.82 avg=2.55\n",
            "[9813 | 4897.66] loss=2.68 avg=2.56\n",
            "[9814 | 4898.15] loss=2.40 avg=2.55\n",
            "[9815 | 4898.64] loss=2.44 avg=2.55\n",
            "[9816 | 4899.12] loss=2.82 avg=2.56\n",
            "[9817 | 4899.61] loss=2.44 avg=2.55\n",
            "[9818 | 4900.10] loss=2.54 avg=2.55\n",
            "[9819 | 4900.59] loss=2.34 avg=2.55\n",
            "[9820 | 4901.07] loss=2.63 avg=2.55\n",
            "[9821 | 4901.56] loss=2.76 avg=2.56\n",
            "[9822 | 4902.05] loss=2.30 avg=2.55\n",
            "[9823 | 4902.53] loss=1.91 avg=2.55\n",
            "[9824 | 4903.02] loss=2.56 avg=2.55\n",
            "[9825 | 4903.51] loss=2.94 avg=2.55\n",
            "[9826 | 4903.99] loss=2.09 avg=2.55\n",
            "[9827 | 4904.48] loss=2.85 avg=2.55\n",
            "[9828 | 4904.97] loss=2.38 avg=2.55\n",
            "[9829 | 4905.45] loss=2.19 avg=2.54\n",
            "[9830 | 4905.94] loss=2.40 avg=2.54\n",
            "[9831 | 4906.43] loss=2.55 avg=2.54\n",
            "[9832 | 4906.92] loss=2.67 avg=2.54\n",
            "[9833 | 4907.40] loss=2.21 avg=2.54\n",
            "[9834 | 4907.89] loss=2.42 avg=2.54\n",
            "[9835 | 4908.38] loss=2.49 avg=2.54\n",
            "[9836 | 4908.86] loss=2.27 avg=2.54\n",
            "[9837 | 4909.35] loss=2.63 avg=2.54\n",
            "[9838 | 4909.84] loss=2.42 avg=2.54\n",
            "[9839 | 4910.32] loss=2.54 avg=2.54\n",
            "[9840 | 4910.81] loss=2.60 avg=2.54\n",
            "[9841 | 4911.30] loss=2.61 avg=2.54\n",
            "[9842 | 4911.79] loss=2.45 avg=2.54\n",
            "[9843 | 4912.27] loss=2.75 avg=2.54\n",
            "[9844 | 4912.76] loss=2.73 avg=2.54\n",
            "[9845 | 4913.25] loss=2.73 avg=2.54\n",
            "[9846 | 4913.74] loss=2.51 avg=2.54\n",
            "[9847 | 4914.23] loss=2.83 avg=2.54\n",
            "[9848 | 4914.71] loss=2.58 avg=2.54\n",
            "[9849 | 4915.20] loss=2.35 avg=2.54\n",
            "[9850 | 4915.69] loss=2.52 avg=2.54\n",
            "[9851 | 4916.17] loss=2.74 avg=2.54\n",
            "[9852 | 4916.66] loss=2.94 avg=2.55\n",
            "[9853 | 4917.15] loss=2.49 avg=2.55\n",
            "[9854 | 4917.64] loss=2.34 avg=2.55\n",
            "[9855 | 4918.12] loss=2.50 avg=2.55\n",
            "[9856 | 4918.61] loss=2.68 avg=2.55\n",
            "[9857 | 4919.10] loss=2.30 avg=2.54\n",
            "[9858 | 4919.59] loss=2.28 avg=2.54\n",
            "[9859 | 4920.07] loss=2.67 avg=2.54\n",
            "[9860 | 4920.56] loss=2.54 avg=2.54\n",
            "[9861 | 4921.05] loss=2.28 avg=2.54\n",
            "[9862 | 4921.53] loss=2.34 avg=2.54\n",
            "[9863 | 4922.02] loss=2.91 avg=2.54\n",
            "[9864 | 4922.51] loss=2.85 avg=2.55\n",
            "[9865 | 4923.00] loss=2.83 avg=2.55\n",
            "[9866 | 4923.49] loss=2.18 avg=2.54\n",
            "[9867 | 4923.97] loss=2.35 avg=2.54\n",
            "[9868 | 4924.46] loss=2.77 avg=2.54\n",
            "[9869 | 4924.94] loss=2.53 avg=2.54\n",
            "[9870 | 4925.43] loss=2.75 avg=2.55\n",
            "[9871 | 4925.92] loss=2.92 avg=2.55\n",
            "[9872 | 4926.41] loss=2.16 avg=2.55\n",
            "[9873 | 4926.90] loss=2.28 avg=2.54\n",
            "[9874 | 4927.38] loss=2.46 avg=2.54\n",
            "[9875 | 4927.87] loss=2.72 avg=2.54\n",
            "[9876 | 4928.36] loss=2.81 avg=2.55\n",
            "[9877 | 4928.84] loss=2.56 avg=2.55\n",
            "[9878 | 4929.33] loss=2.72 avg=2.55\n",
            "[9879 | 4929.82] loss=2.30 avg=2.55\n",
            "[9880 | 4930.30] loss=2.83 avg=2.55\n",
            "[9881 | 4930.79] loss=2.30 avg=2.55\n",
            "[9882 | 4931.28] loss=2.37 avg=2.55\n",
            "[9883 | 4931.77] loss=2.35 avg=2.54\n",
            "[9884 | 4932.25] loss=2.59 avg=2.54\n",
            "[9885 | 4932.74] loss=2.73 avg=2.55\n",
            "[9886 | 4933.22] loss=2.36 avg=2.54\n",
            "[9887 | 4933.71] loss=2.43 avg=2.54\n",
            "[9888 | 4934.20] loss=2.53 avg=2.54\n",
            "[9889 | 4934.69] loss=2.43 avg=2.54\n",
            "[9890 | 4935.18] loss=2.39 avg=2.54\n",
            "[9891 | 4935.66] loss=2.24 avg=2.54\n",
            "[9892 | 4936.15] loss=2.60 avg=2.54\n",
            "[9893 | 4936.64] loss=2.59 avg=2.54\n",
            "[9894 | 4937.13] loss=1.57 avg=2.53\n",
            "[9895 | 4937.61] loss=2.71 avg=2.53\n",
            "[9896 | 4938.10] loss=2.61 avg=2.53\n",
            "[9897 | 4938.59] loss=2.23 avg=2.53\n",
            "[9898 | 4939.08] loss=2.71 avg=2.53\n",
            "[9899 | 4939.57] loss=2.65 avg=2.53\n",
            "[9900 | 4940.05] loss=2.33 avg=2.53\n",
            "[9901 | 4940.54] loss=2.38 avg=2.53\n",
            "[9902 | 4941.03] loss=3.06 avg=2.53\n",
            "[9903 | 4941.51] loss=2.59 avg=2.53\n",
            "[9904 | 4942.00] loss=2.50 avg=2.53\n",
            "[9905 | 4942.49] loss=2.39 avg=2.53\n",
            "[9906 | 4942.97] loss=2.60 avg=2.53\n",
            "[9907 | 4943.46] loss=2.76 avg=2.53\n",
            "[9908 | 4943.95] loss=2.28 avg=2.53\n",
            "[9909 | 4944.43] loss=2.47 avg=2.53\n",
            "[9910 | 4944.92] loss=2.73 avg=2.53\n",
            "[9911 | 4945.41] loss=2.57 avg=2.53\n",
            "[9912 | 4945.90] loss=2.08 avg=2.53\n",
            "[9913 | 4946.38] loss=2.81 avg=2.53\n",
            "[9914 | 4946.87] loss=2.12 avg=2.53\n",
            "[9915 | 4947.36] loss=2.92 avg=2.53\n",
            "[9916 | 4947.85] loss=2.86 avg=2.54\n",
            "[9917 | 4948.34] loss=2.45 avg=2.53\n",
            "[9918 | 4948.82] loss=2.45 avg=2.53\n",
            "[9919 | 4949.31] loss=2.62 avg=2.53\n",
            "[9920 | 4949.80] loss=2.36 avg=2.53\n",
            "[9921 | 4950.29] loss=2.56 avg=2.53\n",
            "[9922 | 4950.77] loss=2.75 avg=2.54\n",
            "[9923 | 4951.26] loss=2.84 avg=2.54\n",
            "[9924 | 4951.75] loss=2.36 avg=2.54\n",
            "[9925 | 4952.24] loss=2.67 avg=2.54\n",
            "[9926 | 4952.72] loss=2.35 avg=2.54\n",
            "[9927 | 4953.21] loss=2.84 avg=2.54\n",
            "[9928 | 4953.70] loss=2.14 avg=2.54\n",
            "[9929 | 4954.19] loss=2.80 avg=2.54\n",
            "[9930 | 4954.67] loss=2.36 avg=2.54\n",
            "[9931 | 4955.16] loss=2.81 avg=2.54\n",
            "[9932 | 4955.65] loss=2.62 avg=2.54\n",
            "[9933 | 4956.14] loss=2.66 avg=2.54\n",
            "[9934 | 4956.62] loss=2.51 avg=2.54\n",
            "[9935 | 4957.11] loss=2.36 avg=2.54\n",
            "[9936 | 4957.60] loss=2.54 avg=2.54\n",
            "[9937 | 4958.09] loss=2.59 avg=2.54\n",
            "[9938 | 4958.58] loss=2.46 avg=2.54\n",
            "[9939 | 4959.07] loss=2.91 avg=2.54\n",
            "[9940 | 4959.55] loss=2.92 avg=2.55\n",
            "[9941 | 4960.04] loss=2.43 avg=2.54\n",
            "[9942 | 4960.53] loss=2.41 avg=2.54\n",
            "[9943 | 4961.02] loss=2.88 avg=2.55\n",
            "[9944 | 4961.50] loss=2.51 avg=2.55\n",
            "[9945 | 4961.99] loss=2.87 avg=2.55\n",
            "[9946 | 4962.48] loss=2.69 avg=2.55\n",
            "[9947 | 4962.97] loss=2.56 avg=2.55\n",
            "[9948 | 4963.46] loss=2.41 avg=2.55\n",
            "[9949 | 4963.94] loss=2.38 avg=2.55\n",
            "[9950 | 4964.43] loss=2.70 avg=2.55\n",
            "[9951 | 4964.92] loss=2.58 avg=2.55\n",
            "[9952 | 4965.41] loss=2.62 avg=2.55\n",
            "[9953 | 4965.89] loss=2.64 avg=2.55\n",
            "[9954 | 4966.38] loss=2.54 avg=2.55\n",
            "[9955 | 4966.87] loss=2.48 avg=2.55\n",
            "[9956 | 4967.35] loss=2.08 avg=2.55\n",
            "[9957 | 4967.84] loss=2.79 avg=2.55\n",
            "[9958 | 4968.33] loss=2.71 avg=2.55\n",
            "[9959 | 4968.82] loss=2.62 avg=2.55\n",
            "[9960 | 4969.30] loss=2.37 avg=2.55\n",
            "[9961 | 4969.79] loss=2.79 avg=2.55\n",
            "[9962 | 4970.28] loss=2.65 avg=2.55\n",
            "[9963 | 4970.76] loss=2.49 avg=2.55\n",
            "[9964 | 4971.25] loss=2.73 avg=2.55\n",
            "[9965 | 4971.74] loss=2.60 avg=2.55\n",
            "[9966 | 4972.23] loss=2.72 avg=2.56\n",
            "[9967 | 4972.71] loss=2.77 avg=2.56\n",
            "[9968 | 4973.20] loss=2.58 avg=2.56\n",
            "[9969 | 4973.69] loss=2.55 avg=2.56\n",
            "[9970 | 4974.17] loss=2.52 avg=2.56\n",
            "[9971 | 4974.66] loss=2.35 avg=2.56\n",
            "[9972 | 4975.15] loss=2.54 avg=2.56\n",
            "[9973 | 4975.63] loss=2.79 avg=2.56\n",
            "[9974 | 4976.12] loss=2.91 avg=2.56\n",
            "[9975 | 4976.61] loss=2.57 avg=2.56\n",
            "[9976 | 4977.10] loss=2.83 avg=2.56\n",
            "[9977 | 4977.58] loss=2.68 avg=2.57\n",
            "[9978 | 4978.07] loss=2.63 avg=2.57\n",
            "[9979 | 4978.56] loss=2.63 avg=2.57\n",
            "[9980 | 4979.04] loss=2.91 avg=2.57\n",
            "[9981 | 4979.53] loss=2.37 avg=2.57\n",
            "[9982 | 4980.02] loss=2.38 avg=2.57\n",
            "[9983 | 4980.51] loss=2.44 avg=2.56\n",
            "[9984 | 4980.99] loss=2.47 avg=2.56\n",
            "[9985 | 4981.48] loss=1.56 avg=2.55\n",
            "[9986 | 4981.97] loss=2.50 avg=2.55\n",
            "[9987 | 4982.46] loss=2.13 avg=2.55\n",
            "[9988 | 4982.94] loss=2.59 avg=2.55\n",
            "[9989 | 4983.43] loss=2.52 avg=2.55\n",
            "[9990 | 4983.92] loss=2.63 avg=2.55\n",
            "[9991 | 4984.40] loss=2.53 avg=2.55\n",
            "[9992 | 4984.89] loss=2.81 avg=2.55\n",
            "[9993 | 4985.38] loss=2.80 avg=2.55\n",
            "[9994 | 4985.86] loss=2.60 avg=2.56\n",
            "[9995 | 4986.35] loss=2.31 avg=2.55\n",
            "[9996 | 4986.84] loss=2.58 avg=2.55\n",
            "[9997 | 4987.32] loss=2.68 avg=2.55\n",
            "[9998 | 4987.82] loss=2.51 avg=2.55\n",
            "[9999 | 4988.30] loss=2.48 avg=2.55\n",
            "Saving checkpoint/run1/model-10000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "’s \n",
            "own.” \n",
            "“I’m not.” \n",
            "“You’re not.” \n",
            "“I am not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not.” \n",
            "“You’re not\n",
            "\n",
            "[10000 | 5001.31] loss=2.68 avg=2.55\n",
            "[10001 | 5001.80] loss=2.63 avg=2.56\n",
            "[10002 | 5002.28] loss=2.59 avg=2.56\n",
            "[10003 | 5002.77] loss=2.60 avg=2.56\n",
            "[10004 | 5003.25] loss=2.44 avg=2.55\n",
            "[10005 | 5003.74] loss=2.79 avg=2.56\n",
            "[10006 | 5004.23] loss=1.82 avg=2.55\n",
            "[10007 | 5004.72] loss=2.57 avg=2.55\n",
            "[10008 | 5005.20] loss=2.70 avg=2.55\n",
            "[10009 | 5005.69] loss=2.72 avg=2.55\n",
            "[10010 | 5006.18] loss=2.71 avg=2.55\n",
            "[10011 | 5006.66] loss=2.47 avg=2.55\n",
            "[10012 | 5007.15] loss=2.37 avg=2.55\n",
            "[10013 | 5007.64] loss=2.60 avg=2.55\n",
            "[10014 | 5008.12] loss=2.38 avg=2.55\n",
            "[10015 | 5008.61] loss=1.92 avg=2.54\n",
            "[10016 | 5009.10] loss=2.38 avg=2.54\n",
            "[10017 | 5009.58] loss=2.46 avg=2.54\n",
            "[10018 | 5010.07] loss=2.31 avg=2.54\n",
            "[10019 | 5010.56] loss=2.81 avg=2.54\n",
            "[10020 | 5011.05] loss=2.32 avg=2.54\n",
            "[10021 | 5011.54] loss=2.54 avg=2.54\n",
            "[10022 | 5012.02] loss=2.67 avg=2.54\n",
            "[10023 | 5012.51] loss=2.43 avg=2.54\n",
            "[10024 | 5013.00] loss=2.81 avg=2.54\n",
            "[10025 | 5013.49] loss=2.28 avg=2.54\n",
            "[10026 | 5013.97] loss=2.64 avg=2.54\n",
            "[10027 | 5014.46] loss=2.30 avg=2.54\n",
            "[10028 | 5014.95] loss=2.55 avg=2.54\n",
            "[10029 | 5015.43] loss=2.62 avg=2.54\n",
            "[10030 | 5015.92] loss=2.41 avg=2.54\n",
            "[10031 | 5016.41] loss=2.46 avg=2.54\n",
            "[10032 | 5016.90] loss=2.64 avg=2.54\n",
            "[10033 | 5017.39] loss=2.67 avg=2.54\n",
            "[10034 | 5017.87] loss=2.35 avg=2.54\n",
            "[10035 | 5018.36] loss=2.12 avg=2.53\n",
            "[10036 | 5018.85] loss=2.13 avg=2.53\n",
            "[10037 | 5019.34] loss=2.61 avg=2.53\n",
            "[10038 | 5019.82] loss=2.46 avg=2.53\n",
            "[10039 | 5020.31] loss=2.26 avg=2.53\n",
            "[10040 | 5020.80] loss=2.23 avg=2.52\n",
            "[10041 | 5021.29] loss=2.53 avg=2.52\n",
            "[10042 | 5021.78] loss=2.71 avg=2.53\n",
            "[10043 | 5022.26] loss=2.79 avg=2.53\n",
            "[10044 | 5022.75] loss=2.83 avg=2.53\n",
            "[10045 | 5023.24] loss=2.08 avg=2.53\n",
            "[10046 | 5023.73] loss=2.46 avg=2.53\n",
            "[10047 | 5024.21] loss=2.12 avg=2.52\n",
            "[10048 | 5024.70] loss=2.87 avg=2.53\n",
            "[10049 | 5025.19] loss=2.70 avg=2.53\n",
            "[10050 | 5025.68] loss=2.80 avg=2.53\n",
            "[10051 | 5026.16] loss=2.56 avg=2.53\n",
            "[10052 | 5026.66] loss=2.43 avg=2.53\n",
            "[10053 | 5027.14] loss=2.47 avg=2.53\n",
            "[10054 | 5027.63] loss=2.37 avg=2.53\n",
            "[10055 | 5028.12] loss=2.54 avg=2.53\n",
            "[10056 | 5028.61] loss=2.84 avg=2.53\n",
            "[10057 | 5029.10] loss=2.36 avg=2.53\n",
            "[10058 | 5029.58] loss=2.61 avg=2.53\n",
            "[10059 | 5030.08] loss=2.15 avg=2.53\n",
            "[10060 | 5030.58] loss=2.40 avg=2.53\n",
            "[10061 | 5031.07] loss=2.67 avg=2.53\n",
            "[10062 | 5031.55] loss=2.46 avg=2.53\n",
            "[10063 | 5032.04] loss=2.38 avg=2.52\n",
            "[10064 | 5032.53] loss=2.82 avg=2.53\n",
            "[10065 | 5033.02] loss=2.28 avg=2.52\n",
            "[10066 | 5033.50] loss=2.57 avg=2.53\n",
            "[10067 | 5033.99] loss=2.53 avg=2.53\n",
            "[10068 | 5034.48] loss=2.62 avg=2.53\n",
            "[10069 | 5034.96] loss=2.71 avg=2.53\n",
            "[10070 | 5035.45] loss=2.41 avg=2.53\n",
            "[10071 | 5035.94] loss=3.08 avg=2.53\n",
            "[10072 | 5036.42] loss=2.97 avg=2.54\n",
            "[10073 | 5036.91] loss=2.48 avg=2.54\n",
            "[10074 | 5037.40] loss=2.67 avg=2.54\n",
            "[10075 | 5037.89] loss=2.78 avg=2.54\n",
            "[10076 | 5038.37] loss=2.67 avg=2.54\n",
            "[10077 | 5038.86] loss=2.56 avg=2.54\n",
            "[10078 | 5039.35] loss=2.40 avg=2.54\n",
            "[10079 | 5039.83] loss=2.83 avg=2.54\n",
            "[10080 | 5040.33] loss=2.23 avg=2.54\n",
            "[10081 | 5040.81] loss=3.06 avg=2.55\n",
            "[10082 | 5041.30] loss=2.72 avg=2.55\n",
            "[10083 | 5041.79] loss=2.58 avg=2.55\n",
            "[10084 | 5042.27] loss=2.28 avg=2.54\n",
            "[10085 | 5042.76] loss=2.41 avg=2.54\n",
            "[10086 | 5043.25] loss=2.31 avg=2.54\n",
            "[10087 | 5043.73] loss=2.68 avg=2.54\n",
            "[10088 | 5044.22] loss=2.92 avg=2.55\n",
            "[10089 | 5044.71] loss=2.77 avg=2.55\n",
            "[10090 | 5045.19] loss=2.43 avg=2.55\n",
            "[10091 | 5045.68] loss=2.58 avg=2.55\n",
            "[10092 | 5046.17] loss=2.60 avg=2.55\n",
            "[10093 | 5046.66] loss=2.50 avg=2.55\n",
            "[10094 | 5047.14] loss=2.55 avg=2.55\n",
            "[10095 | 5047.63] loss=2.58 avg=2.55\n",
            "[10096 | 5048.12] loss=2.44 avg=2.55\n",
            "[10097 | 5048.60] loss=2.62 avg=2.55\n",
            "[10098 | 5049.09] loss=2.56 avg=2.55\n",
            "[10099 | 5049.58] loss=2.38 avg=2.55\n",
            "[10100 | 5050.06] loss=2.33 avg=2.54\n",
            "[10101 | 5050.55] loss=2.23 avg=2.54\n",
            "[10102 | 5051.04] loss=2.49 avg=2.54\n",
            "[10103 | 5051.53] loss=2.53 avg=2.54\n",
            "[10104 | 5052.02] loss=2.47 avg=2.54\n",
            "[10105 | 5052.50] loss=2.22 avg=2.54\n",
            "[10106 | 5052.99] loss=2.45 avg=2.54\n",
            "[10107 | 5053.47] loss=2.36 avg=2.53\n",
            "[10108 | 5053.96] loss=2.84 avg=2.54\n",
            "[10109 | 5054.45] loss=2.84 avg=2.54\n",
            "[10110 | 5054.94] loss=2.29 avg=2.54\n",
            "[10111 | 5055.42] loss=2.25 avg=2.53\n",
            "[10112 | 5055.91] loss=2.78 avg=2.54\n",
            "[10113 | 5056.40] loss=2.63 avg=2.54\n",
            "[10114 | 5056.88] loss=2.76 avg=2.54\n",
            "[10115 | 5057.37] loss=2.70 avg=2.54\n",
            "[10116 | 5057.86] loss=2.48 avg=2.54\n",
            "[10117 | 5058.35] loss=3.04 avg=2.55\n",
            "[10118 | 5058.83] loss=2.81 avg=2.55\n",
            "[10119 | 5059.32] loss=2.58 avg=2.55\n",
            "[10120 | 5059.81] loss=2.53 avg=2.55\n",
            "[10121 | 5060.30] loss=2.66 avg=2.55\n",
            "[10122 | 5060.78] loss=2.46 avg=2.55\n",
            "[10123 | 5061.27] loss=2.68 avg=2.55\n",
            "[10124 | 5061.76] loss=2.61 avg=2.55\n",
            "[10125 | 5062.25] loss=2.39 avg=2.55\n",
            "[10126 | 5062.73] loss=2.41 avg=2.55\n",
            "[10127 | 5063.22] loss=2.24 avg=2.54\n",
            "[10128 | 5063.70] loss=2.22 avg=2.54\n",
            "[10129 | 5064.19] loss=3.00 avg=2.55\n",
            "[10130 | 5064.68] loss=2.15 avg=2.54\n",
            "[10131 | 5065.16] loss=2.31 avg=2.54\n",
            "[10132 | 5065.65] loss=2.76 avg=2.54\n",
            "[10133 | 5066.14] loss=2.98 avg=2.55\n",
            "[10134 | 5066.63] loss=2.49 avg=2.55\n",
            "[10135 | 5067.12] loss=2.66 avg=2.55\n",
            "[10136 | 5067.60] loss=2.50 avg=2.55\n",
            "[10137 | 5068.09] loss=2.49 avg=2.55\n",
            "[10138 | 5068.58] loss=2.32 avg=2.54\n",
            "[10139 | 5069.06] loss=2.89 avg=2.55\n",
            "[10140 | 5069.55] loss=2.59 avg=2.55\n",
            "[10141 | 5070.04] loss=2.53 avg=2.55\n",
            "[10142 | 5070.53] loss=2.42 avg=2.55\n",
            "[10143 | 5071.02] loss=2.57 avg=2.55\n",
            "[10144 | 5071.50] loss=2.74 avg=2.55\n",
            "[10145 | 5071.99] loss=2.62 avg=2.55\n",
            "[10146 | 5072.48] loss=2.76 avg=2.55\n",
            "[10147 | 5072.96] loss=2.61 avg=2.55\n",
            "[10148 | 5073.45] loss=2.83 avg=2.55\n",
            "[10149 | 5073.94] loss=2.26 avg=2.55\n",
            "[10150 | 5074.43] loss=2.68 avg=2.55\n",
            "[10151 | 5074.91] loss=2.25 avg=2.55\n",
            "[10152 | 5075.40] loss=2.70 avg=2.55\n",
            "[10153 | 5075.89] loss=2.21 avg=2.55\n",
            "[10154 | 5076.37] loss=2.43 avg=2.55\n",
            "[10155 | 5076.86] loss=2.45 avg=2.55\n",
            "[10156 | 5077.35] loss=2.38 avg=2.54\n",
            "[10157 | 5077.84] loss=2.39 avg=2.54\n",
            "[10158 | 5078.32] loss=2.45 avg=2.54\n",
            "[10159 | 5078.81] loss=2.52 avg=2.54\n",
            "[10160 | 5079.30] loss=2.99 avg=2.55\n",
            "[10161 | 5079.78] loss=2.78 avg=2.55\n",
            "[10162 | 5080.27] loss=2.74 avg=2.55\n",
            "[10163 | 5080.76] loss=2.64 avg=2.55\n",
            "[10164 | 5081.25] loss=2.32 avg=2.55\n",
            "[10165 | 5081.73] loss=2.35 avg=2.55\n",
            "[10166 | 5082.22] loss=2.92 avg=2.55\n",
            "[10167 | 5082.71] loss=2.69 avg=2.55\n",
            "[10168 | 5083.20] loss=2.56 avg=2.55\n",
            "[10169 | 5083.68] loss=2.35 avg=2.55\n",
            "[10170 | 5084.17] loss=2.83 avg=2.55\n",
            "[10171 | 5084.66] loss=2.65 avg=2.55\n",
            "[10172 | 5085.15] loss=2.34 avg=2.55\n",
            "[10173 | 5085.64] loss=2.34 avg=2.55\n",
            "[10174 | 5086.12] loss=2.50 avg=2.55\n",
            "[10175 | 5086.61] loss=2.24 avg=2.55\n",
            "[10176 | 5087.10] loss=2.58 avg=2.55\n",
            "[10177 | 5087.59] loss=2.49 avg=2.55\n",
            "[10178 | 5088.08] loss=2.13 avg=2.54\n",
            "[10179 | 5088.56] loss=2.80 avg=2.54\n",
            "[10180 | 5089.05] loss=2.65 avg=2.55\n",
            "[10181 | 5089.54] loss=2.67 avg=2.55\n",
            "[10182 | 5090.03] loss=2.61 avg=2.55\n",
            "[10183 | 5090.51] loss=2.55 avg=2.55\n",
            "[10184 | 5091.00] loss=2.46 avg=2.55\n",
            "[10185 | 5091.49] loss=2.59 avg=2.55\n",
            "[10186 | 5091.98] loss=2.38 avg=2.55\n",
            "[10187 | 5092.47] loss=2.53 avg=2.54\n",
            "[10188 | 5092.95] loss=2.65 avg=2.55\n",
            "[10189 | 5093.44] loss=2.62 avg=2.55\n",
            "[10190 | 5093.93] loss=2.61 avg=2.55\n",
            "[10191 | 5094.42] loss=1.65 avg=2.54\n",
            "[10192 | 5094.90] loss=2.33 avg=2.54\n",
            "[10193 | 5095.39] loss=2.57 avg=2.54\n",
            "[10194 | 5095.88] loss=2.62 avg=2.54\n",
            "[10195 | 5096.37] loss=2.87 avg=2.54\n",
            "[10196 | 5096.86] loss=2.87 avg=2.54\n",
            "[10197 | 5097.35] loss=2.29 avg=2.54\n",
            "[10198 | 5097.83] loss=2.26 avg=2.54\n",
            "[10199 | 5098.32] loss=2.51 avg=2.54\n",
            "[10200 | 5098.81] loss=1.77 avg=2.53\n",
            "[10201 | 5099.30] loss=2.74 avg=2.53\n",
            "[10202 | 5099.79] loss=2.61 avg=2.53\n",
            "[10203 | 5100.27] loss=2.47 avg=2.53\n",
            "[10204 | 5100.76] loss=2.20 avg=2.53\n",
            "[10205 | 5101.24] loss=2.34 avg=2.53\n",
            "[10206 | 5101.73] loss=2.63 avg=2.53\n",
            "[10207 | 5102.22] loss=2.39 avg=2.53\n",
            "[10208 | 5102.71] loss=2.42 avg=2.53\n",
            "[10209 | 5103.19] loss=2.15 avg=2.52\n",
            "[10210 | 5103.68] loss=3.10 avg=2.53\n",
            "[10211 | 5104.16] loss=2.63 avg=2.53\n",
            "[10212 | 5104.66] loss=2.29 avg=2.53\n",
            "[10213 | 5105.14] loss=2.24 avg=2.52\n",
            "[10214 | 5105.63] loss=2.14 avg=2.52\n",
            "[10215 | 5106.12] loss=2.71 avg=2.52\n",
            "[10216 | 5106.61] loss=2.57 avg=2.52\n",
            "[10217 | 5107.09] loss=2.63 avg=2.52\n",
            "[10218 | 5107.58] loss=2.48 avg=2.52\n",
            "[10219 | 5108.07] loss=2.54 avg=2.52\n",
            "[10220 | 5108.55] loss=2.52 avg=2.52\n",
            "[10221 | 5109.04] loss=2.74 avg=2.53\n",
            "[10222 | 5109.52] loss=2.14 avg=2.52\n",
            "[10223 | 5110.02] loss=3.05 avg=2.53\n",
            "[10224 | 5110.50] loss=2.76 avg=2.53\n",
            "[10225 | 5110.99] loss=2.53 avg=2.53\n",
            "[10226 | 5111.47] loss=2.32 avg=2.53\n",
            "[10227 | 5111.96] loss=2.51 avg=2.53\n",
            "[10228 | 5112.45] loss=2.44 avg=2.53\n",
            "[10229 | 5112.94] loss=2.55 avg=2.53\n",
            "[10230 | 5113.42] loss=2.48 avg=2.53\n",
            "[10231 | 5113.91] loss=2.44 avg=2.52\n",
            "[10232 | 5114.40] loss=2.34 avg=2.52\n",
            "[10233 | 5114.88] loss=2.49 avg=2.52\n",
            "[10234 | 5115.37] loss=2.68 avg=2.52\n",
            "[10235 | 5115.85] loss=2.43 avg=2.52\n",
            "[10236 | 5116.34] loss=2.42 avg=2.52\n",
            "[10237 | 5116.83] loss=2.70 avg=2.52\n",
            "[10238 | 5117.32] loss=2.58 avg=2.52\n",
            "[10239 | 5117.81] loss=2.21 avg=2.52\n",
            "[10240 | 5118.29] loss=2.52 avg=2.52\n",
            "[10241 | 5118.78] loss=2.44 avg=2.52\n",
            "[10242 | 5119.27] loss=2.13 avg=2.52\n",
            "[10243 | 5119.75] loss=2.57 avg=2.52\n",
            "[10244 | 5120.24] loss=2.45 avg=2.52\n",
            "[10245 | 5120.73] loss=2.67 avg=2.52\n",
            "[10246 | 5121.21] loss=2.68 avg=2.52\n",
            "[10247 | 5121.70] loss=2.89 avg=2.52\n",
            "[10248 | 5122.19] loss=2.25 avg=2.52\n",
            "[10249 | 5122.68] loss=2.41 avg=2.52\n",
            "[10250 | 5123.16] loss=2.61 avg=2.52\n",
            "[10251 | 5123.65] loss=2.67 avg=2.52\n",
            "[10252 | 5124.14] loss=2.31 avg=2.52\n",
            "[10253 | 5124.62] loss=2.50 avg=2.52\n",
            "[10254 | 5125.11] loss=2.71 avg=2.52\n",
            "[10255 | 5125.60] loss=1.67 avg=2.51\n",
            "[10256 | 5126.09] loss=2.45 avg=2.51\n",
            "[10257 | 5126.57] loss=2.44 avg=2.51\n",
            "[10258 | 5127.06] loss=2.45 avg=2.51\n",
            "[10259 | 5127.55] loss=1.14 avg=2.50\n",
            "[10260 | 5128.03] loss=2.36 avg=2.50\n",
            "[10261 | 5128.52] loss=2.27 avg=2.49\n",
            "[10262 | 5129.01] loss=2.49 avg=2.49\n",
            "[10263 | 5129.50] loss=2.47 avg=2.49\n",
            "[10264 | 5129.98] loss=2.73 avg=2.50\n",
            "[10265 | 5130.47] loss=2.71 avg=2.50\n",
            "[10266 | 5130.96] loss=2.75 avg=2.50\n",
            "[10267 | 5131.45] loss=2.68 avg=2.50\n",
            "[10268 | 5131.93] loss=2.67 avg=2.50\n",
            "[10269 | 5132.42] loss=2.74 avg=2.51\n",
            "[10270 | 5132.91] loss=2.34 avg=2.50\n",
            "[10271 | 5133.39] loss=2.26 avg=2.50\n",
            "[10272 | 5133.88] loss=2.62 avg=2.50\n",
            "[10273 | 5134.37] loss=2.81 avg=2.51\n",
            "[10274 | 5134.86] loss=2.75 avg=2.51\n",
            "[10275 | 5135.34] loss=2.61 avg=2.51\n",
            "[10276 | 5135.83] loss=2.76 avg=2.51\n",
            "[10277 | 5136.32] loss=2.61 avg=2.51\n",
            "[10278 | 5136.80] loss=2.44 avg=2.51\n",
            "[10279 | 5137.29] loss=2.80 avg=2.52\n",
            "[10280 | 5137.78] loss=2.10 avg=2.51\n",
            "[10281 | 5138.27] loss=2.58 avg=2.51\n",
            "[10282 | 5138.76] loss=2.66 avg=2.51\n",
            "[10283 | 5139.24] loss=2.49 avg=2.51\n",
            "[10284 | 5139.73] loss=2.58 avg=2.51\n",
            "[10285 | 5140.22] loss=2.39 avg=2.51\n",
            "[10286 | 5140.71] loss=2.78 avg=2.52\n",
            "[10287 | 5141.19] loss=2.66 avg=2.52\n",
            "[10288 | 5141.68] loss=2.48 avg=2.52\n",
            "[10289 | 5142.17] loss=2.14 avg=2.51\n",
            "[10290 | 5142.66] loss=2.26 avg=2.51\n",
            "[10291 | 5143.14] loss=2.56 avg=2.51\n",
            "[10292 | 5143.63] loss=2.36 avg=2.51\n",
            "[10293 | 5144.12] loss=2.67 avg=2.51\n",
            "[10294 | 5144.60] loss=2.16 avg=2.51\n",
            "[10295 | 5145.09] loss=2.26 avg=2.50\n",
            "[10296 | 5145.58] loss=2.43 avg=2.50\n",
            "[10297 | 5146.06] loss=2.34 avg=2.50\n",
            "[10298 | 5146.55] loss=2.41 avg=2.50\n",
            "[10299 | 5147.04] loss=2.94 avg=2.51\n",
            "[10300 | 5147.53] loss=2.63 avg=2.51\n",
            "[10301 | 5148.01] loss=2.66 avg=2.51\n",
            "[10302 | 5148.50] loss=2.53 avg=2.51\n",
            "[10303 | 5148.99] loss=1.77 avg=2.50\n",
            "[10304 | 5149.48] loss=2.30 avg=2.50\n",
            "[10305 | 5149.96] loss=2.49 avg=2.50\n",
            "[10306 | 5150.45] loss=2.59 avg=2.50\n",
            "[10307 | 5150.94] loss=2.53 avg=2.50\n",
            "[10308 | 5151.43] loss=2.89 avg=2.50\n",
            "[10309 | 5151.91] loss=2.74 avg=2.51\n",
            "[10310 | 5152.40] loss=2.84 avg=2.51\n",
            "[10311 | 5152.89] loss=2.42 avg=2.51\n",
            "[10312 | 5153.38] loss=2.63 avg=2.51\n",
            "[10313 | 5153.86] loss=2.49 avg=2.51\n",
            "[10314 | 5154.35] loss=2.72 avg=2.51\n",
            "[10315 | 5154.84] loss=2.60 avg=2.51\n",
            "[10316 | 5155.32] loss=2.24 avg=2.51\n",
            "[10317 | 5155.81] loss=3.17 avg=2.52\n",
            "[10318 | 5156.30] loss=2.62 avg=2.52\n",
            "[10319 | 5156.79] loss=2.17 avg=2.51\n",
            "[10320 | 5157.27] loss=2.80 avg=2.52\n",
            "[10321 | 5157.76] loss=2.77 avg=2.52\n",
            "[10322 | 5158.25] loss=2.52 avg=2.52\n",
            "[10323 | 5158.74] loss=2.45 avg=2.52\n",
            "[10324 | 5159.23] loss=2.60 avg=2.52\n",
            "[10325 | 5159.72] loss=2.33 avg=2.52\n",
            "[10326 | 5160.20] loss=2.64 avg=2.52\n",
            "[10327 | 5160.69] loss=2.72 avg=2.52\n",
            "[10328 | 5161.18] loss=2.80 avg=2.52\n",
            "[10329 | 5161.66] loss=2.36 avg=2.52\n",
            "[10330 | 5162.15] loss=2.48 avg=2.52\n",
            "[10331 | 5162.64] loss=2.51 avg=2.52\n",
            "[10332 | 5163.13] loss=1.29 avg=2.51\n",
            "[10333 | 5163.62] loss=1.99 avg=2.50\n",
            "[10334 | 5164.10] loss=2.64 avg=2.51\n",
            "interrupted\n",
            "Saving checkpoint/run1/model-10335\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsO4uiPe2jqu"
      },
      "source": [
        "#Creating a Training Model directory named 'tgmodel'\n",
        "#import os\n",
        "run_dir = '/content/gpt-2/models/tgmodel'\n",
        "if not os.path.exists(run_dir):\n",
        "  os.makedirs(run_dir)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6LFMyyI289s"
      },
      "source": [
        "#Copying training Files\n",
        "!cp /content/gpt-2/src/checkpoint/run1/model-10000.data-00000-of-00001 /content/gpt-2/models/tgmodel\n",
        "!cp /content/gpt-2/src/checkpoint/run1/checkpoint /content/gpt-2/models/tgmodel\n",
        "!cp /content/gpt-2/src/checkpoint/run1/model-10000.index /content/gpt-2/models/tgmodel\n",
        "!cp /content/gpt-2/src/checkpoint/run1/model-10000.meta /content/gpt-2/models/tgmodel"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsHEid7u3HrS"
      },
      "source": [
        "#Copying the OpenAI GPT-2 117M Model files\n",
        "!cp /content/gpt-2/models/117M/encoder.json /content/gpt-2/models/tgmodel\n",
        "!cp /content/gpt-2/models/117M/hparams.json /content/gpt-2/models/tgmodel\n",
        "!cp /content/gpt-2/models/117M/vocab.bpe /content/gpt-2/models/tgmodel"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7EFuTcV3ZGs"
      },
      "source": [
        "#Renaming the model directories\n",
        "#import os\n",
        "!mv /content/gpt-2/models/117M  /content/gpt-2/models/117M_OpenAI\n",
        "!mv /content/gpt-2/models/tgmodel  /content/gpt-2/models/117M"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1qMKcqh3gNt",
        "outputId": "c13690a6-3284-49a2-a69b-fdaf986a65bf"
      },
      "source": [
        "#Generating Unconditional Samples\n",
        "#import os # import after runtime is restarted\n",
        "os.chdir(\"/content/gpt-2/src\")\n",
        "!python generate_unconditional_samples.py --model_name '117M'"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From generate_unconditional_samples.py:54: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2021-07-20 05:10:40.778710: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2021-07-20 05:10:40.812985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-20 05:10:40.813570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-20 05:10:40.813928: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-20 05:10:40.815764: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-20 05:10:40.817431: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-20 05:10:40.818002: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-20 05:10:40.819785: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-20 05:10:40.820611: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-20 05:10:40.823898: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-20 05:10:40.824013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-20 05:10:40.824595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-20 05:10:40.825096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-20 05:10:40.832078: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2021-07-20 05:10:40.832308: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55908e564d80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2021-07-20 05:10:40.832337: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2021-07-20 05:10:41.026791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-20 05:10:41.027520: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55908e564bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2021-07-20 05:10:41.027552: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2021-07-20 05:10:41.027725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-20 05:10:41.028258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-20 05:10:41.028331: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-20 05:10:41.028358: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-20 05:10:41.028380: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-20 05:10:41.028401: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-20 05:10:41.028424: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-20 05:10:41.028443: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-20 05:10:41.028463: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-20 05:10:41.028532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-20 05:10:41.029071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-20 05:10:41.029581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-20 05:10:41.029646: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-20 05:10:41.030962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-20 05:10:41.031000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-20 05:10:41.031013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-20 05:10:41.031139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-20 05:10:41.031955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-20 05:10:41.032743: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-07-20 05:10:41.032809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From generate_unconditional_samples.py:56: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:39: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:67: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From generate_unconditional_samples.py:65: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "2021-07-20 05:10:58.409048: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "======================================== SAMPLE 1 ========================================\n",
            "RBC received a $116,500 grant from the World Bank Group to help rehabilitate a 2006 National Hospital of Lebanon- later adopted as a girl's hospital.\n",
            "\n",
            "The project attempted to expand the roles of crossing Howard football fields to other communications and care centres.\n",
            "\n",
            "Discussions between parts of the ministry were ordered to find an agreement to help distribute payments to needy families where available.<|endoftext|>Ethnocentric moralism Dr. Division Paul K Raven Lee of sino- academic forced us all to just throw aside a newborn newborn when then - 2 weeks after birth - we took him at arm's length to cross out after every big. Ducks were gobbling up his trunk, pierced brother's face & brain. We concluded that this person was actually a different type of play RELEASE INDEX, COLLECTION & DONATIOROUND BY THE EARTH\n",
            "\n",
            "Table of Contents ------- ----------------------- OVERALL\n",
            "\n",
            "This page contains 40 charts of data and ideas from our search for Dr. Paul K. Raven Lee and his Catholic colleagues.\n",
            "\n",
            "Most recently Dr. Richard Daehn commented on this note: Like many of the other Word Jewish designations here in the \"All Your Redeemer's Sacrifice\" of Temple in Freemasonry in ME CL leading theme, many do not understand why little time has passed since the Unification of the Byway and Hospital saw the Great Decree bearing down on the Gentile Jews. Ah, a time when a matter that is of profound historical importance aimlessly neglected could possibly be discussed. In order to contribute to this healthful healing we had to create a page for each source of inspiration that comes from Freemasonry...\n",
            "\n",
            "###<|endoftext|>CaneJayne, in a new book, 'Wicked Weed' the actor has made his first transgender film – and while it's mainly about treating cannabis users with care – it's a chance at the \"greenest\" to see a transgender character as squeamish about what came first.\n",
            "\n",
            "She divides her set into three tactical landscapes, write, play and then water!\n",
            "\n",
            "In each one she presents a player with the help of a few special characters, so you won't feel left alone over the African Australian Fawfaye Hardy.\n",
            "\n",
            "The stage indicates Idoot? The sandy black cactysaver is the new face of Cabali Dax. The Mandarin Monster, chubby, eyebrows straight and ash dark as da Vinci? The blonde-haired and purring King?).\n",
            "\n",
            "Each campaign concludes with a rather fascinating set that ends successfully with Bam.\n",
            "\n",
            "Chicky and boastful, depraved cocky fatter plays don't overlook her homosexuality? No one quite loves her, but she genuinely does in the film. But why not still love act in scene when she is bathing and passionately bonding with Muhammad the beachgoer?\n",
            "\n",
            "Despite the trio's choice of character (I am Harry Potter?) some viewers try to create sides with Beatrix Black but somehow the supposed hu'gh cast doesn't want any sort of compromise or cut-throat thinking above and beyond.\n",
            "\n",
            "Down the hot beans, gapero smiles? No one does better.\n",
            "\n",
            "In Thief Stations All-Brightly Dancing! Brittney channels her freakish beretly prop with this not-so-quiet trick:\n",
            "\n",
            "1. Lawley Dick – Always white hair and scruffy-looking cut with a frown onto her cheek\n",
            "\n",
            "2. Babum the Deathbomber – For fun, shoots with unlimited reels of ice\n",
            "\n",
            "3. Virginia Stoneland – With lowered collarbone; a curvy gait and a wink to fellow ballet dancer's body\n",
            "\n",
            "4. Dr. Louis Legendone – You can't believe though your unborn elephant looks like a woman; sprays her with over 50 other vibrating tropical kisses\n",
            "\n",
            "5. Lorenzo Graefe – Every fight may end up killing you, but body culture informs the makeup and trim of your skin.\n",
            "\n",
            "6. Caitlin McKenna-Winters – 4in. Dorothy reminds you that when you're offsetting the progressive blond wig this stage witch will raise green-suited minded diddy skirts and women guides and next Nov publicize that typical Victorian haircut when I have the audacity to call her #MOSHWAKE CRAZY PICTURE. You will laugh because she has cold clumsiness\n",
            "\n",
            "7. Condolences Delirite Deacoustic Singers Supreme Six – She can do more and more with it\n",
            "\n",
            "8. Michael Cavallo – Neither will you miss potbellied movies, so consider your fear that he may be blameless; an open letter to his girlfriend starring old Miriam Be Arthur\n",
            "\n",
            "9. Matryoshna Proscina\n",
            "\n",
            "10. Sophia Barker – Talk about a family with nearly superficial appearance act nipping on the heels of the snotre Mel Gibson impression, but instead end up assaulting your nephew girl's sake\n",
            "\n",
            "10. Sofia Anthony Alvarez\n",
            "\n",
            "11.\n",
            "======================================== SAMPLE 2 ========================================\n",
            ": The Choice of Edy Fork, Authors Garcia Consombiete and Robert Gosling (Kindle Edition) http://www.maoulinabrown.isner.gov/    Dear Robert, > pursuant toPRov.US policy, DO NOT show at least two separate forms of montages. Via email, I expected to see them like clicky plus or slapdash minus as many things as I could fit into a typical afternoon publication, so just to bring with me the case for my audience. Lots of things, lots of references. We will talk more about these once the story has topped rates and We wonder if you would like a second interview (I am well aware Eduardo Cain's Daily Show had also reached ad revenue for the year.) Jonathan Slobodnicl , MD , Coauthors <-- http://www.mundo.org/talk/kefoldsord. Bee requesting celebrity yours truly: nice, well done. Thoughts of additional technical projects you're working on. Emails from Robin Zimmerman <---- https://mail.maoulinabrown.edu/~marsteinappts> ^ on adaptation bans Disney. CNY August or more, especially for programming that conjures up WitchManga. ______________________________________________________________________ Don't forget to buy Latest Allowation tiers on Adwords , or MAIL been Capital: https://giveurs.com/bonau/premium Federal: https://www.affinity.com/get-free--- Told list fun: http://www.bigauges.com In-game winners and developers who \"do Like Us\" to me: How I wrote the promo for and with Bravely Default: Yamato Shihou walkthrough, by Charminglyad (http://gyazo.com/33bd22b547ffe9dcaes0566088f1334) [+110] I'll get around to talking to you again _____________________________________ |CAP SWEAT series PG-13: Touichi-kun wa Chō Sakai (Japan); Yuudou shíhiden koshi, GO☆る确! End of story: Yasukuni Nishipa (USA) | PR: 0580354 Facebook | Twitter + SA<|endoftext|>Yes it's an absolute truth in this game, but with the meta and deckbuilding meta fast moving, making it clear that the deck doesn't care about juke or how many you have. Having a non meta support deck can take some mental pressure on a Fugate who telegraphs +1 on tapping, then curses of war. Having such a viable Mage-heavy deck battle against even the most ~less efficient meta with a rolling Jackson & Smite combo might seem near useless. However there is still plenty of value below that they could make over enough experience to raffle. I'm choosing though to ignore Jakod's Box and gain some practice time without the troublesome side Snow patients, after seeing a bit of review in which they just looked for +4/+4.\n",
            "\n",
            "The best Blessing of Chant was given in the EN which on impact made a huge difference in these matters with 4 toughness (turmoil and juke wheel effect. Fantasy opposed it!) and was used again with 2 however really encouraging. With this set it slowly becomes easier to ban Jackson to drop, more important when your opponent is fighting Flanders monsters. After ignoring the +3/+3 Shout go anti the need by slapping the opposing chant on Jakod's Royal Guard and riding its blockers beginning his current fight against the opposing Ichor. This version ended up with 7 toughness and +3 health, unlike Edge and Dizzamani Hunter, but builds around it. The only other thing to note we also had to peel as the BGG has either decided not to live Alenko in the Rings or to obliterate the Winnipeg Thunder arguably the most unique Risk Based format out there. It's surprising then, that 9+ toughness wasn't a top deck problem.\n",
            "\n",
            "The good thing is that this 'new class' has provided me a getting out of face to face a mix of cards that one can't bother with in a metagame that doesn't lack creature removal. This is a good 'We live and die with you' moment, however it will feel wrong in determining spells that made us much less satisfied. Take Jakod's Royal Guard, for example. As mentioned once a day a target card like this is game breaking given that it's very common. Therefore chasing it will negate the value while keeping us guessing on how to avoid so bad eventuality. After I caught up to most other fighting N'Quan Blue's Patches in force side almost all of those interactions got easily eliminated. Houseon Greystone, Cirque du Soleil, Zealot of the Damned were knocked to flanks again, causing many of us to pick up on their slower speed, reduced toughness, and massive fatigue values. This Orgain Abliteration is going to be vital and provides a card like this to push\n",
            "======================================== SAMPLE 3 ========================================\n",
            "1. 2 oz. florets\n",
            "\n",
            "Spread out 1 inch. Always use top heavy. Fold leaves in half. Review by: Greg @ 1:45 pm, 4/25/2018\n",
            "\n",
            "Car pieced ciennas with fine ciennas leaves the design is easy. If you double peeled them they can tolerate or bite easily.\n",
            "\n",
            "Calories: 175 grams (55) mg\n",
            "\n",
            "Dairy ratio: Mash 1.5 lbs.\n",
            "\n",
            "This is a mixture for my goal of using extra soft and dense loaves of ciennas lightly so they don't become \"flabby and clumpy\". I did mine in the same style of mojito - 6-8 tbsp. (5 tablespoons for a best end results)\n",
            "\n",
            "Ingredients:\n",
            "\n",
            "6 cyl. potatoes - leftovers from 4 oz (220 ml) bánica flakes\n",
            "\n",
            "Cook ciocta leaves\n",
            "\n",
            "4 cups Whole Foods give - all natural, 7% Clovemary\n",
            "\n",
            "Grill abattoir\n",
            "\n",
            "2 tablespoons Soak leaves in soapy water (until the leaves are milky and opaque) Rinse leaves with cold water in a tea bag and bring to a boil over high heat, add flowers and spices. Rinse leaves in cold water again and strain to a clove and season with salt and pepper. Taste ... But head still cream, russetts and other strainer until creamed, oval or more pro se (turn it in half). Potatoes have a temper in them with natural flavor, so all the vegetables leave a big and permeable sweet kick in the season. 1. Ounce o quick fraction bottle ...conventional supermarket frozen Creamy, machet flavors Pickles, M&M's Every order would already get you a tasteful, a very fresh light pie. Pickle is a weak common scratch flavor for most decoupled eating tastes called \"Carton\" flavor. Pack Size ... 16 units\n",
            "\n",
            "Hummus: Agave Almond Malt & Celery Blend with Toasted or Extra Light Capsicum II berries\n",
            "\n",
            "Spices or decoquered herbs\n",
            "\n",
            "Roasted Cabbage\n",
            "\n",
            "Sauce, Fed. - Discard Old Flour, rib of pea or dried fruit from draining Spread his feet a little over threshold so that the cieng rushes are still what they should be. Part of view Less than 8 4.1 lb.\n",
            "\n",
            "Rating on FlavoredKneesBy L.N. the 4th of June 2018\n",
            "\n",
            "That sugar content reflects only slightly EVRIxx format, not mtn ie cayenne used for pepper. There isn't much pungency in French Pepper. Regardless, from try pot and expect it to run a little SPY. pic.twitter.com/NQ1TcfZ3MEM — Lucy Yaumann (@LucyYumann) June 12, 2018\n",
            "\n",
            "So... Can I order Vanilla Cod Chowder from the blender ? Venomenta spicy porridge for those yearsous cookies that have just double sprinkled mushroom chips or meat/sour dips suddenly clink together and there is just the sense of SO LIFE Hellopic.twitter.com/ORVTgj3Kwqf — Lily Kellogg & Major Lawson (@lilykellogg) May 07, 2017\n",
            "\n",
            "tips / research interactions pages? science & science smart as nails would\n",
            "\n",
            "author comments [-!] ★★★☆ A mouth full of reviewing FDA regulations. Trusted by many, but having a good shop there. I can't wait to roll back. You need to visit today. — Modila Vaye (@modila.vaye/) February 24, 2017\n",
            "\n",
            "I've been using mine in the kitchen a long time now. Huge, flavorful and easy to serve. Light weight, easy to mini, beneficial to children. Again this new blended product doesn't sugar, but it comes full circle with xerox blocking and a two-layer side salad dressing added. I worked it up timely to out Insane Magazine. Thanks so much Lucy! Bacon Madness & Cilantro Rum Hard to comment, but the ingredients are delicious. Very tasty. Overall my favorite pic. DON TOUCH AWAY! — Danldocoppinity (@danldocoppinity) November 12, 2015\n",
            "\n",
            "Original AMA review: Having tried Maguire turkey with poached egg, it was a total happy birthday. This will be the best sweet meal party ever!! <4 Chevalier Cavendish Mushroom Bacon Madness / Mug Cake Cake Great bacon variations using Van Amers' NATOHazard free house bouquet, especially for diabetic resistant dogs in North America. Served w/3 oz head and the Spicy Onion Cilantro & Share Rice Pork Peanut Butter Toppings Low carb, friendly, and also low calorie. — Puy N Kat (@PuyNKat) January 27, 2015\n",
            "\n",
            "4 oz. Macaroni & Cheese to\n",
            "Traceback (most recent call last):\n",
            "  File \"generate_unconditional_samples.py\", line 79, in <module>\n",
            "    fire.Fire(sample_model)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fire/core.py\", line 471, in _Fire\n",
            "    target=component.__name__)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fire/core.py\", line 681, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"generate_unconditional_samples.py\", line 71, in sample_model\n",
            "    out = sess.run(output)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 956, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1180, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1359, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1365, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\n",
            "    target_list, run_metadata)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-KYUHSD4Atf",
        "outputId": "f64ae9eb-ea9a-43c8-e1cf-e8ad8889ef92"
      },
      "source": [
        "#Interactive Context and Completion Examples\n",
        "import os # import after runtime is restarted\n",
        "os.chdir(\"/content/gpt-2/src\")\n",
        "!python interactive_conditional_samples.py --temperature 0.8 --top_k 40 --model_name '117M'"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From interactive_conditional_samples.py:57: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2021-07-20 05:11:50.141993: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2021-07-20 05:11:50.170412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-20 05:11:50.171076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-20 05:11:50.171420: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-20 05:11:50.173014: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-20 05:11:50.174753: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-20 05:11:50.175109: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-20 05:11:50.176800: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-20 05:11:50.177564: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-20 05:11:50.180841: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-20 05:11:50.180979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-20 05:11:50.182051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-20 05:11:50.182733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-20 05:11:50.187718: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2021-07-20 05:11:50.187926: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555e4516ed80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2021-07-20 05:11:50.187956: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2021-07-20 05:11:50.379783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-20 05:11:50.380609: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555e4516ebc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2021-07-20 05:11:50.380643: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2021-07-20 05:11:50.380829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-20 05:11:50.381466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-20 05:11:50.381535: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-20 05:11:50.381562: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-20 05:11:50.381587: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-20 05:11:50.381610: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-20 05:11:50.381638: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-20 05:11:50.381663: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-20 05:11:50.381688: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-20 05:11:50.381770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-20 05:11:50.382711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-20 05:11:50.386214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-20 05:11:50.386353: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-20 05:11:50.388115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-20 05:11:50.388169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-20 05:11:50.388188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-20 05:11:50.388449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-20 05:11:50.389462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-20 05:11:50.390077: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-07-20 05:11:50.390124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From interactive_conditional_samples.py:58: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From interactive_conditional_samples.py:60: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:67: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From interactive_conditional_samples.py:68: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Model prompt >>> The watchers moved forward together, as if some signal had been given. Swords rose and fell, all in a  deathly silence. It was cold butchery. The pale blades sliced through ringmail as if it were silk. Will closed  his eyes. Far beneath him, he heard their voices and laughter sharp as icicles. \n",
            "2021-07-20 05:12:52.947007: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "======================================== SAMPLE 1 ========================================\n",
            " As he opened his eyes, he saw a pair of eyes. They were dark. It was cold, but a cold one. He stood up, and the wind blew in. He opened his eyes wide, and they widened into a narrow, blank stare. The thin blade of the blade cut through his left eye.  It was as though he had been torn from his body by the blade.\n",
            "The watchers moved forward together, as if some signal had been given. Swords rose and fell, all in a   deathly silence. It was cold butchery. The pale blades sliced through ringmail as if it were silk. Will closed  his eyes. Far beneath him, he heard their voices and laughter sharp as icicles. Will closed his eyes and they widened into a narrow, blank stare. The thin blade of the blade cut through his left eye.  It was as though he had been torn from his body by the blade.  It was as though he had been torn from his body by the blade. The watchers moved forward together, as if some signal had been given. Swords rose and fell, all in a   deathly silence. It was cold butchery. The pale blades sliced through ringmail as if it were silk. Will closed  his eyes. Far beneath him, he heard their voices and laughter sharp as icicles. Will closed his eyes and they widened into a narrow, blank stare. The thin blade of the blade cut through his left eye.  It was as though he had been torn from his body by the blade. The watchers moved forward together, as if some signal had been given. Swords rose and fell, all in a   deathly silence. It was cold butchery. The pale blades sliced through ringmail as if it were silk. Will closed  his eyes. Far beneath him, he heard their voices and laughter sharp as icicles. Will closed  his eyes and he widened into a narrow, blank stare. The thin blade of the blade cut through his left eye.  It was as though he had been torn from his body by the blade. The watchers moved forward together, as if some signal had been given. Swords rose and fell, all in a   deathly silence. It was cold butchery. The pale blades sliced through ringmail as if it were silk. Will closed  his eyes. Far beneath him, he heard their voices and laughter sharp as icicles. Will closed  his\n",
            "================================================================================\n",
            "Model prompt >>> The breath of man and horse mingled, steaming, in the cold morning air as his lord father had the man cut  down from the wall and  dragged before them. Robb and Jon sat tall and still on their horses, with Bran between them on his  pony, trying to seem older than seven, trying to pretend that he'd seen all this before. A faint wind blew  through the holdfast gate. Over their heads flapped the banner of the Starks of Winterfell: a grey direwolf  racing across an ice-white field.  Bran's father sat solemnly on his horse, long brown hair stirring in the wind. His closely trimmed beard  was shot with white, making him look older than his thirty-five years. He had a grim cast to his grey eyes  this day, and he seemed not at all the man who would sit before the fire in the evening and talk softly of  the age of heroes and the children of the forest. He had taken off Father's face, Bran thought, and  donned the face of Lord Stark of Winterfell.  There were questions asked and answers given there in the chill of morning, but afterward Bran could  not recall much of what had been said. Finally his lord father gave a command, and two of his guardsmen  dragged the ragged man to the ironwood stump in the center of the square. They forced his head down  onto the hard black wood. Lord Eddard Stark dismounted and his ward Theon Greyjoy brought forth  the sword. \"Ice,\" that sword was called. It was as wide across as a man's hand, and taller even than  Robb. The blade was Valyrian steel, spell-forged and dark as smoke. Nothing held an edge like Valyrian  steel. \n",
            "======================================== SAMPLE 1 ========================================\n",
            " \"Ice-iron!\" roared one of the guardsmen. It rained  all day long. \"Ice-fire!\" The wight called again, and so he had it . It was only at night that he heard his own cry . The fire of the ironwood ran cold. The ironwood was burned by the flames of the wight . The wight was angry and angry , and he was angry with his lord. He had been at war  and had died  with his lord  for over a thousand years . With the wight  his own lord  stood as the last bastion  of the Targaryens .  With his lord  he would die , and the Targaryens would be dead . In a dark corner of the burning hall , a boy had said that it was the wight's turn  to die . \"My lord, my lord,\" Theon cried. \"I will die with you . You are the last bastion  of the Targaryens . I will die with my lord . And you with me .\" The boy smiled as his lord . He was dressed in his Lord's clothes . He had black hair that was tied tightly behind his ears . His father would be  here and he would be there. In his hand was a long chain of long red hair wagging on the branches . It was a black raven on the side of his head .  The boy had his own . He had red mittens . He had black boots . The boy had white tattered gloves . He had a sword . He had a mace . He had a mace  near his hand . He had a heavy, thick, long mace he had on his foot . He had a mace in his hand. It was a black mace . He had a mace  with a long iron rod. He had a mace  with a long iron rod. He had a mace  in his hand. He had a long, dark gray mace  between his hands. He had a long, dark gray mace in his hand. He had a very long, light gray mace . It was a light gray mace  on his right hand . It was black . The boy had a long, dark gray mace  on his left hand . It was black . He had a dagger on his left hand . The boy\n",
            "================================================================================\n",
            "Model prompt >>> Jory's sword was already out. \"Robb, get away from it!\" he called as his horse reared under him.  Robb grinned and looked up from the bundle in his arms. \"She can't hurt you,\" he said. \"She's dead,  Jory.\"  Bran was afire with curiosity by then. He would have spurred the pony faster, but his father made them  dismount beside the bridge and approach on foot. Bran jumped off and ran.  By then Jon, Jory, and Theon Greyjoy had all dismounted as well. \"What in the seven hells is it?\"  Greyjoy was saying.  \"A wolf,\" Robb told him.  \"A freak,\" Greyjoy said. \"Look at the size of it.\"  Bran's heart was thumping in his chest as he pushed through a waist-high drift to his brothers' side.  Half-buried in bloodstained snow, a huge dark shape slumped in death. Ice had formed in its shaggy  grey fur, and the faint smell of corruption clung to it like a woman's perfume. Bran glimpsed blind eyes  crawling with maggots, a wide mouth full of yellowed teeth. But it was the size of it that made him gasp. It  was bigger than his pony, twice the size of the largest hound in his father's kennel. \n",
            "======================================== SAMPLE 1 ========================================\n",
            " \"My God,\" his mother said. \n",
            "Lion and Greyjoy's battle with the Night's Watch. \"The Great Hunt !\" \n",
            "Cullen snickered at them as he stood over the massive hounds in the river.  \"They've eaten him... \" he said.  \"They've destroyed his life!  They didn't even know his name!\"  The black haired hounds all cheered and cheered even more loudly when he realized how much more they had to kill to get them all to abandon the Night's Watch.  Bran looked up and shook his head. \"They aren't my people. They're still here. They came from the Blackwater. They're still there. It shouldn't have come to this! There's no way they can take them away!\"  He pointed his horn at the Hound and cried out. Bran felt the blood in his nose move across his cheeks.  \"What the hell is going on?\" Bran cried.  His brothers gasped, blood rushing from their mouths at the sight.  \"I'm sorry, I didn't mean to go in there,\" Jon said as he picked up a large hound and looked at it before they came to his side.  \"They're not my people. They're still here! They're still waiting for us, but I told them that when they came, they're here, and now they're here!\"  The Hound yelled at Bran as they came to his side. \"Get some food,  Robb.  You've got a lot of blood. You've got to feed them.\" \"That's what I tell you,\" the Hound said.  \"You know, like what's on the TV? Get some food,  Robb.  And some more food, and some more food, and a little more food.\"   \"But no,\" the Hound said, turning round to go to Bran.  \"No, no, no. I don't think you'd say that. They're still here, they still fight. They've been there for a long time.  They've fought long enough, to know there's someone who can do it for them.  But I don't know if they'll even fight, and I don't know if they're going to be able to do it for me.  If they're going to fight, then they have to survive.  If they haven't survived for too long, then it's time for them to be\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"interactive_conditional_samples.py\", line 91, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fire/core.py\", line 471, in _Fire\n",
            "    target=component.__name__)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fire/core.py\", line 681, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"interactive_conditional_samples.py\", line 73, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X67GLqxbU8b_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}