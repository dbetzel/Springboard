{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQHHBOUu8XLo"
   },
   "source": [
    "# Story Gerneration\n",
    "In this project, you'll generate your own story using RNNs.  You'll be using the Stephen King Books dataset from kaggle.  The author of this dataset web scraped sixteen Stephen King books and created text files from them.  You can see this data at https://www.kaggle.com/ttalbitt/stephen-king-books.\n",
    "## Get the Data\n",
    "The first thing I did was to combine all sixteen separate text files into one large text file which I have called StephenKingAll.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pi8Ra_Xh8XLx"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "data_dir = 'StephenKingAll.txt'\n",
    "with open(data_dir, \"r\") as f:\n",
    "  text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7POwR038XLy"
   },
   "source": [
    "## Explore the Data\n",
    "I did just a few print statements to explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FWn16Mb4-W5M",
    "outputId": "67da7742-7113-46a2-8aad-95838b0082d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 73419\n",
      "Average number of sentences in each scene: 17.0\n",
      "Number of lines: 18\n",
      "Length of text: 14659521 characters\n",
      "per dispensers my wife fell down Her pursestrap stayed over her shoulder but her prescription bag slipped from her hand and the sinus inhaler slid halfway out The other item stayed put No one noticed her lying there by the newspaper dispensers everyone was focused on the tangled vehicles the screaming women the spreading puddle of water and antifreeze from the Public Works trucks ruptured radiator Thats gas the clerk from Fast Foto shouted to anyone who would listen Thats gas watch out she dont blow fellas I suppose one or two of the wouldbe rescuers might have jumped right over her perhaps thinking she had fainted To assume such a thing on a day when the temperature was pushing ninetyfive degrees would not have been unreasonable Roughly two dozen people from the shopping center clustered around the accident another four dozen or so came running over from Strawford Park where a baseball game had been going on I imagine that all the things you would expect to hear in such situations were said many of them more than once Milling around Someone reaching through the misshapen hole which had been the driversside window to pat Esthers trembling old hand People immediately giving way for Joe Wyzer at such moments anyone in a white coat automatically becomes the belle of the ball In the distance the warble of an ambulance siren rising like shaky air over an incinerator All during this lying unnoticed in the parking lot was my wife with her purse still over her shoulder inside still w\n",
      "54 unique characters\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (9500, 10500)\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "scenes = text.split('\\n\\n')\n",
    "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')\n",
    "\n",
    "# Take a look at the first 250 characters in text\n",
    "print(text[9500:11000])\n",
    "\n",
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x18gw54r8XLz"
   },
   "source": [
    "## Implement Preprocessing Functions\n",
    "The first thing to do to any dataset is preprocessing.  Implement the following preprocessing functions below:\n",
    "- Lookup Table\n",
    "- Tokenize Punctuation\n",
    "\n",
    "### Lookup Table\n",
    "To create a word embedding, you first need to transform the words to ids.  In this function, create two dictionaries:\n",
    "- Dictionary to go from the words to an id, we'll call `vocab_to_int`\n",
    "- Dictionary to go from the id to word, we'll call `int_to_vocab`\n",
    "\n",
    "Return these dictionaries in the following tuple `(vocab_to_int, int_to_vocab)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VYWNSCrO8XL0",
    "outputId": "571aea41-47fd-4a69-8b32-a05ecc34ee80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, 'A': 2, 'B': 3, 'C': 4, 'D': 5, 'E': 6, 'F': 7, 'G': 8, 'H': 9, 'I': 10, 'J': 11, 'K': 12, 'L': 13, 'M': 14, 'N': 15, 'O': 16, 'P': 17, 'Q': 18, 'R': 19, 'S': 20, 'T': 21, 'U': 22, 'V': 23, 'W': 24, 'X': 25, 'Y': 26, 'Z': 27, 'a': 28, 'b': 29, 'c': 30, 'd': 31, 'e': 32, 'f': 33, 'g': 34, 'h': 35, 'i': 36, 'j': 37, 'k': 38, 'l': 39, 'm': 40, 'n': 41, 'o': 42, 'p': 43, 'q': 44, 'r': 45, 's': 46, 't': 47, 'u': 48, 'v': 49, 'w': 50, 'x': 51, 'y': 52, 'z': 53}\n",
      "{0: '\\n', 1: ' ', 2: 'A', 3: 'B', 4: 'C', 5: 'D', 6: 'E', 7: 'F', 8: 'G', 9: 'H', 10: 'I', 11: 'J', 12: 'K', 13: 'L', 14: 'M', 15: 'N', 16: 'O', 17: 'P', 18: 'Q', 19: 'R', 20: 'S', 21: 'T', 22: 'U', 23: 'V', 24: 'W', 25: 'X', 26: 'Y', 27: 'Z', 28: 'a', 29: 'b', 30: 'c', 31: 'd', 32: 'e', 33: 'f', 34: 'g', 35: 'h', 36: 'i', 37: 'j', 38: 'k', 39: 'l', 40: 'm', 41: 'n', 42: 'o', 43: 'p', 44: 'q', 45: 'r', 46: 's', 47: 't', 48: 'u', 49: 'v', 50: 'w', 51: 'x', 52: 'y', 53: 'z'}\n",
      "['\\n', ' ', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "words_no_duplicates = sorted(list(set(text)))\n",
    "vocab_to_int = dict((word,i) for i, word in enumerate(words_no_duplicates))\n",
    "int_to_vocab = dict((i,word) for i, word in enumerate(words_no_duplicates))\n",
    "print (vocab_to_int)\n",
    "print (int_to_vocab)\n",
    "print(words_no_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5CDeK1YIJ8RP",
    "outputId": "cfa92aa4-e812-4e51-a51c-b5d5e1c6e611"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'accident', 'across', 'all', 'and', 'antifreeze', 'around', 'at', 'bag', 'be', 'been', 'both', 'but', 'by', 'cluster', 'couldnt', 'dispensers', 'doing', 'down', 'dunbarry', 'easterling', 'everyone', 'fell', 'focused', 'friend', 'from', 'had', 'halfway', 'hand', 'help', 'her', 'hurt', 'in', 'inhaler', 'irene', 'item', 'jill', 'johanna', 'least', 'little', 'lot', 'lying', 'miss', 'mrs', 'my', 'near', 'newspaper', 'no', 'noticed', 'occurred', 'of', 'on', 'one', 'other', 'others', 'out', 'over', 'parking', 'past', 'prescription', 'pretty', 'public', 'puddle', 'pursestrap', 'put', 'radiator', 'radio', 'remembered', 'running', 'ruptured', 'said', 'same', 'screaming', 'shack', 'she', 'shoulder', 'sinus', 'slacks', 'slid', 'slipped', 'somebody', 'someone', 'spreading', 'stayed', 'sure', 'tangled', 'that', 'the', 'them', 'then', 'there', 'they', 'thought', 'trucks', 'vehicles', 'was', 'water', 'were', 'when', 'wife', 'windowshopping', 'women', 'works', 'wouldnt', 'yellow']\n",
      "{'a': 0, 'accident': 1, 'across': 2, 'all': 3, 'and': 4, 'antifreeze': 5, 'around': 6, 'at': 7, 'bag': 8, 'be': 9, 'been': 10, 'both': 11, 'but': 12, 'by': 13, 'cluster': 14, 'couldnt': 15, 'dispensers': 16, 'doing': 17, 'down': 18, 'dunbarry': 19, 'easterling': 20, 'everyone': 21, 'fell': 22, 'focused': 23, 'friend': 24, 'from': 25, 'had': 26, 'halfway': 27, 'hand': 28, 'help': 29, 'her': 30, 'hurt': 31, 'in': 32, 'inhaler': 33, 'irene': 34, 'item': 35, 'jill': 36, 'johanna': 37, 'least': 38, 'little': 39, 'lot': 40, 'lying': 41, 'miss': 42, 'mrs': 43, 'my': 44, 'near': 45, 'newspaper': 46, 'no': 47, 'noticed': 48, 'occurred': 49, 'of': 50, 'on': 51, 'one': 52, 'other': 53, 'others': 54, 'out': 55, 'over': 56, 'parking': 57, 'past': 58, 'prescription': 59, 'pretty': 60, 'public': 61, 'puddle': 62, 'pursestrap': 63, 'put': 64, 'radiator': 65, 'radio': 66, 'remembered': 67, 'running': 68, 'ruptured': 69, 'said': 70, 'same': 71, 'screaming': 72, 'shack': 73, 'she': 74, 'shoulder': 75, 'sinus': 76, 'slacks': 77, 'slid': 78, 'slipped': 79, 'somebody': 80, 'someone': 81, 'spreading': 82, 'stayed': 83, 'sure': 84, 'tangled': 85, 'that': 86, 'the': 87, 'them': 88, 'then': 89, 'there': 90, 'they': 91, 'thought': 92, 'trucks': 93, 'vehicles': 94, 'was': 95, 'water': 96, 'were': 97, 'when': 98, 'wife': 99, 'windowshopping': 100, 'women': 101, 'works': 102, 'wouldnt': 103, 'yellow': 104}\n",
      "{0: 'a', 1: 'accident', 2: 'across', 3: 'all', 4: 'and', 5: 'antifreeze', 6: 'around', 7: 'at', 8: 'bag', 9: 'be', 10: 'been', 11: 'both', 12: 'but', 13: 'by', 14: 'cluster', 15: 'couldnt', 16: 'dispensers', 17: 'doing', 18: 'down', 19: 'dunbarry', 20: 'easterling', 21: 'everyone', 22: 'fell', 23: 'focused', 24: 'friend', 25: 'from', 26: 'had', 27: 'halfway', 28: 'hand', 29: 'help', 30: 'her', 31: 'hurt', 32: 'in', 33: 'inhaler', 34: 'irene', 35: 'item', 36: 'jill', 37: 'johanna', 38: 'least', 39: 'little', 40: 'lot', 41: 'lying', 42: 'miss', 43: 'mrs', 44: 'my', 45: 'near', 46: 'newspaper', 47: 'no', 48: 'noticed', 49: 'occurred', 50: 'of', 51: 'on', 52: 'one', 53: 'other', 54: 'others', 55: 'out', 56: 'over', 57: 'parking', 58: 'past', 59: 'prescription', 60: 'pretty', 61: 'public', 62: 'puddle', 63: 'pursestrap', 64: 'put', 65: 'radiator', 66: 'radio', 67: 'remembered', 68: 'running', 69: 'ruptured', 70: 'said', 71: 'same', 72: 'screaming', 73: 'shack', 74: 'she', 75: 'shoulder', 76: 'sinus', 77: 'slacks', 78: 'slid', 79: 'slipped', 80: 'somebody', 81: 'someone', 82: 'spreading', 83: 'stayed', 84: 'sure', 85: 'tangled', 86: 'that', 87: 'the', 88: 'them', 89: 'then', 90: 'there', 91: 'they', 92: 'thought', 93: 'trucks', 94: 'vehicles', 95: 'was', 96: 'water', 97: 'were', 98: 'when', 99: 'wife', 100: 'windowshopping', 101: 'women', 102: 'works', 103: 'wouldnt', 104: 'yellow'}\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import problem_unittests as tests\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #return None, None\n",
    "    words_no_duplicates = sorted(list(set(text)))\n",
    "    vocab_to_int = dict((word,i) for i, word in enumerate(words_no_duplicates))\n",
    "    int_to_vocab = dict((i,word) for i, word in enumerate(words_no_duplicates))\n",
    "    print(words_no_duplicates)\n",
    "    print (vocab_to_int)\n",
    "    print (int_to_vocab)\n",
    "    return(vocab_to_int,int_to_vocab)\n",
    "\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFuMep_G8XL0"
   },
   "source": [
    "### Tokenize Punctuation\n",
    "We'll be splitting the script into a word array using spaces as delimiters.  However, punctuations like periods and exclamation marks make it hard for the neural network to distinguish between the word \"bye\" and \"bye!\".\n",
    "\n",
    "Implement the function `token_lookup` to return a dict that will be used to tokenize symbols like \"!\" into \"||Exclamation_Mark||\".  Create a dictionary for the following symbols where the symbol is the key and value is the token:\n",
    "- Period ( . )\n",
    "- Comma ( , )\n",
    "- Quotation Mark ( \" )\n",
    "- Semicolon ( ; )\n",
    "- Exclamation mark ( ! )\n",
    "- Question mark ( ? )\n",
    "- Left Parentheses ( ( )\n",
    "- Right Parentheses ( ) )\n",
    "- Dash ( -- )\n",
    "- Return ( \\n )\n",
    "\n",
    "This dictionary will be used to token the symbols and add the delimiter (space) around it.  This separates the symbols as it's own word, making it easier for the neural network to predict on the next word. Make sure you don't use a token that could be confused as a word. Instead of using the token \"dash\", try using something like \"||dash||\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LIf5koce8XL1",
    "outputId": "a9dddbd5-99ca-4dbb-d8d2-4ca030645c66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    return {'.': '||vPeriod||', \n",
    "            ',': '||vComma||',\n",
    "            '\"': '||vQuote||',\n",
    "            ';': '||vSemicolon||',\n",
    "            '!': '||vExclamation||',\n",
    "            '?': '||vQuestion||',\n",
    "            '(': '||vLeftParen||',\n",
    "            ')': '||vRightParen||',\n",
    "            '--': '||vDash||',\n",
    "            '\\n': '||vNewline||'\n",
    "            }\n",
    "\n",
    "tests.test_tokenize(token_lookup)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "dlnd_tv_script_generation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
